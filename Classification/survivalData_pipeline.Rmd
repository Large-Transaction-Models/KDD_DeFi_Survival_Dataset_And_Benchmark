---
title: "DeFi Survival Data Pipeline"
author: "Hanzhen Qin(qinh2)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: true
    number_sections: true
    df_print: paged
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
# Check and install required R packages
if (!require("conflicted")) {
  install.packages("conflicted", dependencies = TRUE)
  library(conflicted)
}

# Set default CRAN repository
local({
  r <- getOption("repos")
  r["CRAN"] <- "http://cran.r-project.org"
  options(repos = r)
})

# Define the list of required packages
required_packages <- c(
  "rmarkdown", "tidyverse", "stringr", "ggbiplot", "pheatmap", 
  "caret", "survival", "survminer", "ggplot2", 
  "kableExtra", "rpart", "glmnet", "data.table", "reshape2", "pROC", 
  "pander", "readr", "dplyr", "e1071", "ROSE", "xgboost", "parallel", "reticulate"
)

# Loop through the package list and install missing packages
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}

# Handle function name conflicts
conflict_prefer("slice", "dplyr")
conflict_prefer("filter", "dplyr")

# Set knitr options for R Markdown
knitr::opts_chunk$set(echo = TRUE)

# Rename dplyr functions to avoid conflicts with other packages
select <- dplyr::select
rename <- dplyr::rename
summarize <- dplyr::summarize
group_by <- dplyr::group_by
```

# Survival Data Pipeline

## Pipeline Summary

* Summary of work
  
  * Data Preprocessing: `data_processing`, `get_train_test_data`, `smote_data`, 
  `get_classification_cutoff`
  
  * Model Performance Evaluation And Visualization : `calculate_model_metrics`, `get_dataframe`,
  `combine_classification_results`, `accuracy_comparison_plot`, `get_percentage`, 
  `specific_accuracy_statistics`, `combine_accuracy_dataframes`
  
  * Classification Model Development And Optimization: `logistic_regression`, `decision_tree`, 
  `naive_bayes`, `XG_Boost`, `elastic_net`, `logistic_regression_op`, `decision_tree_op`, 
  `naive_bayes_op`, `elastic_net_op`

  * Deep Learning Classification Model: `deephit_model`, `deephit_model.py`, `transformation_surv_model`, 
  `transformation_surv_model.py`
  
## Data Preprocessing

### def get_train_test_data

The `get_train_test_data` function is designed to get the training and test data based on the `indexEvent` and `outcomeEvent` parameters, but the specific logic has not yet been implemented. The function references the external script `dataLoader.r` through `source()` to load the latest processed survival data, and returns the train and test data sets directly by default.

```{r}
get_train_test_data <- function(indexEvent, outcomeEvent) {
  source("~/KDD_DeFi_Survival_Dataset_And_Benchmark/dataLoader.R")
}
```

### def data_processing

The `data_processing` function is designed to transform a survival analysis dataset into a format suitable for classification tasks by performing a series of preprocessing steps. It begins by filtering out invalid records where `timeDiff` is less than or equal to 0, ensuring only valid observations are retained. Then, it removes records where `timeDiff` falls within a specified threshold (`set_timeDiff`) but no event occurred (`status == 0`). A new binary column, `event`, is created to indicate whether an event occurred within the threshold ("yes" or "no"). The function drops unnecessary columns as defined in a predefined list, ensuring only relevant features remain, and removes columns entirely populated with missing values. To handle missing data, numeric columns are filled with `-999` to mark missing values clearly without distorting numericul distributions, and categorical columns are filled with `"missing"` to create a distinct category for absent data. Finally, character columns are converted to factors, making them ready for classification models.

```{r}
data_processing <- function(survivalData, set_timeDiff) {
  # filter out invalid records where `timeDiff` is <= 0 early
  survivalData <- survivalData %>% filter(timeDiff > 0)
  
  # filter out records based on the `set_timeDiff` threshold and `status`
  survivalData <- survivalData %>% filter(!(timeDiff / 86400 <= set_timeDiff & status == 0))
  
  # create a new binary column `event` based on `timeDiff`
  survivalDataForClassification <- survivalData %>%
    mutate(event = case_when(
      timeDiff / 86400 <= set_timeDiff ~ "yes",
      timeDiff / 86400 > set_timeDiff ~ "no"
    ))
  
  # define features to drop
  # featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
  #                     "timeDiff", "deployment", "version", "indexID", "user", "status", 
  #                     "quarter", "liquidator", "userFlashloanAvgAmount", "userReserveMode",
  #                     "reserve", "pool", "timestamp", "type", "datetime", "quarter_start_date",
  #                     "userCoinTypeMode", "coinType", "userIsNew", "userDepositSum", 
  #                     "userDepositSumUSD", "userDepositAvgAmountUSD", "userDepositSumETH",
  #                     "userDepositAvgAmountETH", "userWithdrawSum", "userWithdrawSumUSD",
  #                     "userWithdrawAvgAmountUSD", "userWithdrawSumETH", 
  #                     "userWithdrawAvgAmountETH", "userBorrowSum", "userBorrowSumUSD", 
  #                     "userBorrowAvgAmountUSD", "userBorrowSumETH", 
  #                     "userBorrowAvgAmountETH", "userRepaySum", "userRepaySumUSD", 
  #                     "userRepayAvgAmountUSD", "userRepaySumETH", "userRepayAvgAmountETH", 
  #                     "userFlashloanSum", "userLiquidationSum", "userLiquidationSumUSD",
  #                     "userLiquidationAvgAmountUSD", "userLiquidationSumETH", 
  #                     "userLiquidationAvgAmountETH", "userFlashloanCount", "priceInUSD")
  
  featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
                      "timeDiff", "status", "deployment", "version", "indexID", "user", 
                      "liquidator", "pool", "timestamp", "type", "datetime", "quarter_start_date")
  
  # remove only columns that actually exist in the dataset
  featuresToDrop <- intersect(featuresToDrop, colnames(survivalDataForClassification))
  
  survivalDataForClassification <- survivalDataForClassification %>%
    # drop unnecessary columns
    select(-any_of(featuresToDrop)) %>%
    # remove columns with only NA values
    select(where(~ !all(is.na(.)))) %>%
    # replace NA in numeric columns with -999
    mutate(across(where(is.numeric), ~ replace_na(., -999))) %>%
    # replace NA in character columns with "missing"
    mutate(across(where(is.character), ~ replace_na(., "missing"))) %>%
    # convert character columns to factors
    mutate(across(where(is.character), as.factor))
  
  # return the processed dataset
  return(survivalDataForClassification)
}
```

### def get_classification_cutoff (@Author: Aaron Green)

The `get_classification_cutoff` function retrieves the `ConvergedRMST_5` value from a predefined dataset based on the specified `indexEvent` and `outcomeEvent`. The dataset contains information about various index events (e.g., "Borrow", "Deposit") and their corresponding outcome events (e.g., "Full Repay", "Withdraw"), along with convergence metrics like `ConvergedTau` and `ConvergedRMST` at different time horizons. The function filters the dataset by matching the input events with the respective `IndexEvent` and `OutcomeEvent` columns and returns the relevant `ConvergedRMST_5` value, which represents a specific metric of convergence over a 5-unit time period. Return the optimal cutoff value for the `data_processing` function.

```{r}
get_classification_cutoff <- function(indexEvent, outcomeEvent){
  # library(stringr)
  # Create the dataframe
  data <- data.frame(
    IndexEvent = c("Borrow", "Borrow", "Borrow", "Borrow", "Borrow", "Borrow", 
                   "Deposit", "Deposit", "Deposit", "Deposit", "Deposit",
                   "Repay", "Repay", "Repay", "Repay", "Repay",
                   "Withdraw", "Withdraw", "Withdraw", "Withdraw", "Withdraw"),
    OutcomeEvent = c("Account Liquidated", "Deposit", "Full Repay", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Repay"),
    ConvergedTau_1 = c(91, 99, 95, 101, 69, 99, 
                       90, 100, 101, 99, 93, 
                       93, 95, 100, 101, 100, 
                       94, 101, 99, 101, 100),
    ConvergedRMST_1 = c(69.36, 90.98, 74.12, 100.88, 28.71, 92.59, 
                        68.64, 91.58, 100.95, 89.70, 64.84, 
                        72.56, 63.23, 93.72, 100.86, 93.93,
                        75.10, 96.01, 82.13, 100.88, 93.82),
    ConvergedTau_5 = c(20, 21, 20, 21, 17, 21, 
                       20, 21, 21, 21, 20, 
                       20, 20, 21, 21, 21,
                       20, 21, 21, 21, 21),
    ConvergedRMST_5 = c(17.43, 19.88, 17.15, 20.99, 10.24, 20.21, 
                        17.52, 19.73, 20.99, 19.69, 15.67, 
                        17.43, 14.99, 20.13, 20.99, 20.15,
                        17.72, 20.26, 18.14, 20.98, 20.04),
    ConvergedTau_10 = c(11, 11, 11, 11, 10, 11, 
                        11, 11, 11, 11, 11, 
                        11, 11, 11, 11, 11,
                        11, 11, 11, 11, 11),
    ConvergedRMST_10 = c(9.94, 10.51, 9.72, 11.00, 6.66, 10.68, 
                         10.00, 10.43, 11.00, 10.44, 8.95, 
                         9.90, 8.64, 10.62, 11.00, 10.62,
                         10.05, 10.68, 9.68, 10.99, 10.57)
  )
  
  
  return(data %>% filter(IndexEvent == str_to_title(indexEvent), OutcomeEvent == str_to_title(outcomeEvent)) 
         %>% pull(ConvergedRMST_5))
}
```

### def smote_data

The `smote_data` function leverages the `ROSE` package to address class imbalance in datasets by dynamically generating a balanced dataset based on oversampling and undersampling techniques. The function is designed to be flexible, allowing the user to specify the target variable (`target_var`) and an optional random seed (`seed`) for reproducibility. It validates the input dataset to ensure the target variable exists and dynamically constructs the formula for the `ROSE` function, making it adaptable to different datasets and classification tasks. By generating a balanced dataset where the minority and majority classes are better represented, this function helps mitigate bias in machine learning models and improves their classification accuracy. 

```{r}
smote_data <- function(train_data, target_var = "event", seed = 123) {
  # library(ROSE)
  # check if the input data contains the target variable
  if (!target_var %in% colnames(train_data)) {
    stop(paste("Target variable", target_var, "not found in the dataset"))
  }
  
  # set the random seed (if provided)
  if (!is.null(seed)) {
    set.seed(seed)
  }
  
  # dynamic formula creation to adapt to different target variables
  formula <- as.formula(paste(target_var, "~ ."))
  
  # applying ROSE Balance Data
  train_data_balanced <- ROSE(formula, data = train_data, seed = seed)$data
  
  # return the balanced dataset
  return(train_data_balanced)
}
```

## Model Performance Evaluation And Visualization

### def calculate_model_metrics

The `calculate_model_metrics` function evaluates a classification model's performance based on a given confusion matrix by computing key metrics, including **class accuracy (specificity)**, **negative-1 accuracy (sensitivity/recall)**, **balanced accuracy**, **precision**, and the **F1-score**. It extracts True Negatives (TN), False Positives (FP), False Negatives (FN), and True Positives (TP) from the matrix and calculates specificity as \( TN / (TN + FP) \) and sensitivity as \( TP / (TP + FN) \), then derives balanced accuracy as their average. Precision is determined as \( TP / (TP + FP) \), and the F1-score is computed as the harmonic mean of precision and recall. The function prints the balanced accuracy and F1-score in percentage format before returning them in a list.

```{r}
calculate_model_metrics <- function(confusion_matrix, binary_predictions, model_name) {
  TN <- confusion_matrix[1, 1] # True Negatives
  FP <- confusion_matrix[1, 2] # False Positives
  FN <- confusion_matrix[2, 1] # False Negatives
  TP <- confusion_matrix[2, 2] # True Positives
  
  # positive Class accuracy (Specificity): TN / (TN + FP)
  class_accuracy <- TN / (TN + FP)
  
  # negative 1 accuracy (Sensitivity/Recall): TP / (TP + FN)
  negative_1_accuracy <- TP / (TP + FN)
  
  # balanced accuracy: Average of Sensitivity and Specificity
  balanced_accuracy <- (class_accuracy + negative_1_accuracy) / 2
  
  # precision
  precision <- TP / (TP + FP)
  
  # f1 score
  f1_score <- 2 * (precision * negative_1_accuracy) / (precision + negative_1_accuracy)
  
  if (is.nan(balanced_accuracy)) {
    balanced_accuracy <- 0.50
  }
  
  if (is.nan(f1_score)) {
    f1_score <- 0.50
  }
  
  # print out all the accuracy records
  print(paste(model_name, "model prediction accuracy:"))
  cat("Balanced accuracy:", sprintf("%.2f%%", balanced_accuracy * 100), "\n")
  cat("F1 score:", sprintf("%.2f%%", f1_score * 100), "\n")
  
  return (list(balanced_accuracy = balanced_accuracy, 
               f1_score = f1_score))
}
```

### def get_dataframe

The `get_dataframe` function creates a formatted data frame containing accuracy metrics for a given model. It takes two inputs: `model_name`, a string representing the model's name, and `metrics`, a list containing the model's accuracy metrics. The function constructs a data frame with three columns: "Model," "Balanced_Accuracy," and "F1_Score." The accuracy values are converted to percentages with two decimal places using `sprintf("%.2f%%", value * 100)`. Finally, the formatted data frame is returned.

```{r}
get_dataframe <- function(model_name, metrics) {
  metrics_dataframe <- data.frame(
    Model = model_name, 
    Balanced_Accuracy = sprintf("%.2f%%", metrics$balanced_accuracy * 100), 
    F1_Score = sprintf("%.2f%%", metrics$f1_score * 100)
  )
  return (metrics_dataframe)
}
```

### def combine_classification_results

The `combine_classification_results` function is designed to unify performance metrics from multiple classification models into a single, cohesive dataframe. It accepts two parameters: a list of dataframes (`accuracy_dataframe_list`), each containing accuracy metrics for a different model, and a string (`data_combination`) describing the dataset combination (e.g., "Withdraw + Deposit"). The function uses `lapply` to iterate over each dataframe in the list, adding a `Data_Combination` column that records the dataset combination description for that model's metrics. Once each dataframe is labeled, `do.call(rbind, accuracy_dataframe_list)` concatenates the dataframes by rows, resulting in a single combined dataframe with all models’ metrics and an additional column indicating the dataset combination.

```{r}
combine_classification_results <- function(accuracy_dataframe_list, data_combination) {
  # apply the data combination description to each dataframe in the list
  accuracy_dataframe_list <- lapply(accuracy_dataframe_list, function(df) {
    # add a new column `Data_Combination` to store the combination description
    # this allows each dataframe to retain information about the specific data combination it
    # represents
    df$Data_Combination <- data_combination
    # return the modified dataframe with the new column added
    return(df)
  })
  
  # combine all the modified dataframes into one large dataframe
  # `do.call` applies `rbind` to all dataframes in the list, effectively stacking them by rows
  combined_dataframe <- do.call(rbind, accuracy_dataframe_list)
  
  # return the combined dataframe
  return(combined_dataframe)
}
```

### def get_percentage

The `get_percentage` function takes a dataset (`survivalDataForClassification`), along with two event labels (`indexEvent` and `outcomeEvent`), and calculates the percentage distribution of different event types within the dataset. It groups the data by the event variable, counts occurrences for each event type, and then calculates the total and corresponding percentage for each event. The function then creates a bar plot to visually display the percentage of each event type, labeling the bars with the percentage values and showing the y-axis as a percentage. The plot is titled according to the provided `indexEvent` and `outcomeEvent` labels.

```{r}
get_percentage <- function(survivalDataForClassification, indexEvent, outcomeEvent) {
  # indexEvent and outcomeEvent is a string type
  pctPerEvent <- survivalDataForClassification %>%
  group_by(event) %>%
  dplyr::summarize(numPerEvent = n()) %>%
  mutate(total = sum(numPerEvent)) %>%
  mutate(percentage = numPerEvent / total) %>%
  dplyr::select(event, percentage)
  # create a bar plot for event percentages
  # stat = "identity": percentages used directly to draw the bar chart
  print(ggplot(pctPerEvent, aes(x = event, y = percentage, fill = event)) +
    geom_bar(stat = "identity") +
    scale_y_continuous(labels = scales::percent_format()) +  # show y-axis in percentage
    labs(title = "Percentage of Events: 'Yes' event vs 'No' event",
         x = paste(indexEvent, "and", outcomeEvent),
         y = "Percentage") +
    geom_text(aes(label = scales::percent(percentage)), 
              vjust = -0.5, size = 3.5) +  # show percentages on top of bars
    theme_minimal())
}
```

### def accuracy_comparison_plot

The `accuracy_comparison_plot` function visualizes the accuracy performance of different models by creating a faceted bar plot. It takes `metrics_list` as input, where each element consists of a model’s accuracy metrics and its corresponding name. The function first initializes an empty data frame and iterates through `metrics_list`, extracting `balanced_accuracy` and `f1_score` for each model, and appending them to the data frame. The data is then transformed into a long format using `reshape2::melt()` to facilitate plotting. A `ggplot2` bar chart is generated, where models are represented on the x-axis and accuracy values on the y-axis. The plot is faceted by metric type, ensuring a separate visualization for `balanced_accuracy` and `f1_score`. Labels displaying percentage values are added on top of each bar for clarity. The final visualization is styled using a minimal theme, with x-axis text rotated for better readability.

```{r}
accuracy_comparison_plot <- function(metrics_list) {
  # initialize an empty data frame to store the metrics for all models
  accuracy_table <- data.frame()
  
  # loop over each element in metrics_list (each element is a list containing metrics and model name)
  for (metrics in metrics_list) {
    # Extract metrics and model name from each "tuple"
    model_metrics <- metrics[[1]]
    model_name <- metrics[[2]]
    
    # create a temporary dataframe for this model
    temp_df <- data.frame(
      Model = model_name, 
      BalancedA = model_metrics$balanced_accuracy, 
      F1_score = model_metrics$f1_score
    )
    
    # append the temporary dataframe to the main accuracy_table
    accuracy_table <- rbind(accuracy_table, temp_df)
  }
  
  # melt the dataframe into long format for plotting
  accuracy_results_melted <- reshape2::melt(accuracy_table, id.vars = "Model")
  
  # generate the plot with faceted bars
  ggplot(accuracy_results_melted, aes(x = Model, y = value, fill = Model)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ variable, scales = "free_y") +  # Facet by each metric
    labs(title = "Comparison of Accuracy Metrics Across Models",
         x = "Model",
         y = "Value") +
    # add percentage labels on top of each bar
    geom_text(aes(label = scales::percent(value, accuracy = 0.1)),
              position = position_dodge(width = 0.9),
              vjust = 0.5, size = 2.0) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

### def specific_accuracy_statistics

The `specific_accuracy_statistics` function calculates and organizes accuracy-related metrics for different models based on a given accuracy type. It takes three arguments: `event_pair`, which represents a specific event combination, `accuracy_type`, which specifies the type of accuracy metric to extract (either "balanced_accuracy" or "f1_score"), and `metrics_list`, a list containing accuracy data and corresponding model names. The function initializes a result list, placing `accuracy_type` in the top-left corner and `event_pair` as the first column. It then iterates through `metrics_list`, extracting and rounding the selected accuracy metric for each model. If an invalid `accuracy_type` is provided, it raises an error. Finally, the results are converted into a DataFrame and returned with row names set to `NULL`.

```{r}
specific_accuracy_statistics <- function(event_pair, accuracy_type, metrics_list) {
  # initialize the result list
  results <- list()
  
  # add accuracy_type as the left front corner and event_pair is the first column of the data
  results[[accuracy_type]] <- event_pair
  
  # traverse metrics_list and extract the specified accuracy type for each model
  for (metric_item in metrics_list) {
    # get accuracy data
    accuracy_data <- metric_item[[1]]
    # get the model name
    model_name <- metric_item[[2]]
    if (accuracy_type == "balanced_accuracy") {
      results[[model_name]] <- round(accuracy_data$balanced_accuracy * 100, 1)
    }
    else if (accuracy_type == "f1_score") {
      results[[model_name]] <- round(accuracy_data$f1_score * 100, 1)
    }
    else {
      # An error message is displayed if the specified accuracy_type does not exist.
      stop(paste("Invalid accuracy type:", accuracy_type))
    }
  }
  
  # convert the result to a DataFrame and set row.names = NULL
  df <- as.data.frame(results, row.names = NULL)
  return(df)
}
```

### def combine_accuracy_dataframes

The `combine_accuracy_dataframes` function takes a list of data frames as input and combines them into a single data frame by stacking the rows. It first checks if the input is a valid list and throws an error if not. Then, using `do.call` and `rbind`, it merges all the data frames in the list row-wise. The function is designed to handle multiple data frames efficiently and returns a consolidated data frame for further analysis.

```{r}
combine_accuracy_dataframes <- function(df_list) {
  # check if the input is a list
  if (!is.list(df_list)) {
    stop("Input must be a list of data.frames.")
  }
  
  # Use do.call and rbind to combine all data.frames in a list.
  combined_df <- do.call(rbind, df_list)
  
  # returns the merged data.frame
  return(combined_df)
}
```

## Classification Model Development And Optimization

### def logistic_regression

This **logistic regression model** applies **Lasso regularization (L1 penalty)** for **binary classification** using the `glmnet` package. The model follows a structured workflow, starting with **data preprocessing**, where numeric features in the `train_set`, `validation_set`, and `test_data` are **standardized** (mean = 0, standard deviation = 1) to ensure stable optimization. The categorical variables are handled automatically by converting the dataset into **matrix format** using `model.matrix()`, which excludes the intercept and creates a design matrix suitable for `glmnet`. To **train the logistic regression model**, **Lasso regularization** (`alpha = 1`) is applied, which helps in **feature selection by shrinking less important coefficients to zero**, improving interpretability and reducing overfitting. The model first undergoes **cross-validation (`cv.glmnet()`)**, which automatically determines a range of `lambda` values (the regularization parameter) and selects the `lambda.min` that minimizes the cross-validation error. This approach helps **balance model complexity and generalization**, preventing overfitting while ensuring predictive performance. After selecting the **best lambda**, the model predicts **event probabilities** on both the `validation_set` and `test_data`, applying a **0.5 threshold** to classify instances as `"yes"` or `"no"`. Performance evaluation is conducted using **confusion matrices** and key classification metrics computed through the `calculate_model_metrics()` function.

```{r}
logistic_regression <- function(train_data, test_data) {
  # library(glmnet)  # load glmnet package for logistic regression with regularization
  # library(data.table)  # load data.table for efficient data handling
  # ensure train_data and test_data are in the data.table format for fast operations
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  set.seed(123)  # ensure reproducibility
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset, and scale them to have mean = 0 and standard deviation = 1
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Scale numeric columns in train set
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # Scale numeric columns in validation set
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # Scale numeric columns in test set
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert the training, validation, and testing datasets to matrix format as required by the glmnet package
  # model matrix function excludes the intercept (-1) and converts data for glmnet
  x_train <- model.matrix(event ~ . - 1, data = train_set)  
  y_train <- train_set$event  # extract the target variable from the training data
  
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)  
  x_test <- model.matrix(event ~ . - 1, data = test_data)  
  
  # Apply logistic regression with Lasso regularization (alpha = 1 means Lasso)
  # 'family = binomial' specifies logistic regression for binary classification
  logistic_regression_classifier <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
  
  # Use validation set to select the best lambda value
  lambda_values <- logistic_regression_classifier$lambda  # Get available lambda values
  cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)  # Cross-validation
  best_lambda <- cv_model$lambda.min  # Select the best lambda based on cross-validation
  
  # Predict the probability of the event (outcome) on the validation set
  predict_probabilities_val <- predict(logistic_regression_classifier, s = best_lambda, newx = x_validation, type = "response")
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation set performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, "Logistic Regression (Validation)")
  
  # Predict the probability of the event (outcome) on the test set using the best lambda
  predict_probabilities_test <- predict(logistic_regression_classifier, s = best_lambda, newx = x_test, type = "response")
  binary_prediction_test <- ifelse(predict_probabilities_test > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  test_conf_matrix <- table(Predicted = binary_prediction_test, Actual = test_data$event)
  
  # Evaluate model performance by calculating metrics such as accuracy, precision, recall, etc.
  metrics_lr <- calculate_model_metrics(test_conf_matrix, predict_probabilities_test, "Logistic Regression")
  
  # Create a dataframe with the desired structure
  metrics_lr_dataframe = get_dataframe("Logistic Regression", metrics_lr)
  return (list(metrics_lr_dataframe = metrics_lr_dataframe, metrics_lr = metrics_lr))
}
```

### def decision_tree

This **decision tree model** is designed for **binary classification**, using the `rpart` package to construct and optimize a tree-based classifier. The model starts with **data partitioning**, where the training data is split into **80% training set and 20% validation set** to allow hyperparameter tuning. The decision tree is then trained on the `train_set` using the **CART (Classification and Regression Trees) algorithm**, with the **"class"** method to handle categorical target variables.Key hyperparameters are set through `rpart.control()`, including: **`cp = 0.01`**: The complexity parameter, which controls **pruning** by preventing overly complex trees. **`maxdepth = 30`**: The maximum depth of the tree, balancing complexity and generalization. **`minsplit = 20`**: The minimum number of observations required to split a node, ensuring stability in decision-making. After training, the model **predicts on the validation set**, and its performance is evaluated using a **confusion matrix** and classification metrics (`calculate_model_metrics()`). The final decision tree is then applied to the **test dataset**, producing **predicted labels**, which are compared against actual outcomes to compute key performance metrics.

```{r}
decision_tree <- function(train_data, test_data) {
  # library(rpart)
  # library(caret) # Load caret for data partitioning
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Train the decision tree model with hyperparameter tuning
  decision_tree_classifier <- rpart(
    event ~ .,
    data = train_set,
    method = "class",
    control = rpart.control(
      # Complexity parameter for pruning
      cp = 0.01,
      # Maximum depth of the tree
      maxdepth = 30,
      # Minimum number of observations needed to split a node
      minsplit = 20
    )
  )
  # Predict on the validation dataset
  predict_probabilities_val <- predict(decision_tree_classifier, validation_set, type = "class")
  validation_conf_matrix <- table(Predicted = predict_probabilities_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Decision Tree (Validation)")
  
  # Predict on the testing dataset
  predict_probabilities_dt <- predict(decision_tree_classifier, test_data, type = "class")
  test_conf_matrix <- table(Predicted = predict_probabilities_dt, Actual = test_data$event)
  
  # Evaluate model performance
  metrics_dt <- calculate_model_metrics(test_conf_matrix, predict_probabilities_dt, "Decision Tree")
  
  # Create a dataframe with the desired structure
  metrics_dt_dataframe = get_dataframe("Decision Tree", metrics_dt)
  return (list(metrics_dt_dataframe = metrics_dt_dataframe, metrics_dt = metrics_dt))
}
```

### def naive_bayes

This **Naive Bayes model** is designed for **binary classification**, leveraging the `e1071` package to implement a probabilistic classifier based on **Bayes' theorem** with the assumption of **feature independence**. The model begins with **data preprocessing**, where the target variable `event` is explicitly converted into a **factor** to ensure compatibility with Naive Bayes. The dataset is then **split into an 80% training set and a 20% validation set**, allowing for hyperparameter tuning and performance evaluation. Feature matrices for **validation and test sets** are prepared by removing the target column to ensure that the model only relies on predictors for classification. During training, the **Naive Bayes classifier** is built using the `naiveBayes()` function, which computes **prior probabilities** for each class and estimates conditional probabilities for each feature. Predictions are then made on the **validation set**, with both **class labels** and **probability scores** being generated. The model's performance is evaluated using a **confusion matrix** and various classification metrics, calculated through the `calculate_model_metrics()` function.

```{r}
naive_bayes <- function(train_data, test_data) {
  # library(e1071)
  # library(caret) # Load caret for data partitioning
  
  target_column = "event"
  
  # Convert the target column to a factor if it's not already
  train_data[[target_column]] <- as.factor(train_data[[target_column]])
  test_labels <- as.factor(test_data[[target_column]])
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data[[target_column]], p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Remove the target column from the validation and test sets for prediction
  validation_features <- validation_set %>% select(-all_of(target_column))
  test_features <- test_data %>% select(-all_of(target_column))
  
  # Train Naive Bayes model on train_set
  nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set)
  
  # Make predictions on the validation set
  validation_predictions <- predict(nb_model, validation_features)
  
  # Get prediction probabilities for validation set
  validation_probabilities <- predict(nb_model, validation_features, type = "raw")
  
  # Evaluate model performance with a confusion matrix for validation set
  validation_conf_matrix <- table(Predicted = validation_predictions, Actual = validation_set[[target_column]])
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, validation_probabilities, 
                                                "Naive Bayes (Validation)")
  
  # Make predictions on the test set
  predictions <- predict(nb_model, test_features)
  
  # Get prediction probabilities for test set
  prediction_probabilities <- predict(nb_model, test_features, type = "raw")
  
  # Ensure both predicted and actual labels are factors with the same levels
  predictions <- factor(predictions, levels = levels(test_labels))
  
  # Evaluate model performance with a confusion matrix for test set
  conf_matrix <- table(Predicted = predictions, Actual = test_labels)
  
  metrics_nb <- calculate_model_metrics(conf_matrix, prediction_probabilities, "Naive Bayes")
  
  # Create a dataframe with the desired structure
  metrics_nb_dataframe = get_dataframe("Naive Bayes", metrics_nb)
  
  # Each classification model needs to return these two variables
  return (list(metrics_nb_dataframe = metrics_nb_dataframe, metrics_nb = metrics_nb))
}
```

### def XG_Boost

This **XGBoost model** is a powerful gradient boosting framework designed for **binary classification**, leveraging **parallel computation** and **tree-based learning** to achieve high predictive performance. The model starts with **data preprocessing**, where numeric features in the training, validation, and test sets are **standardized** to ensure stable optimization. The target variable is converted into **binary labels (0/1)** to align with the **logistic objective function** used in XGBoost. The dataset is then transformed into **DMatrix format**, a highly optimized data structure that accelerates computation and improves memory efficiency. The model employs **hyperparameters** such as `max_depth = 6`, `eta = 0.1`, `subsample = 0.8`, and `colsample_bytree = 0.8` to **balance complexity and generalization**. A **learning rate (`eta`)** of 0.1 is chosen to ensure stable convergence, while `max_depth` controls the depth of each decision tree, preventing overfitting. Additionally, `subsample` and `colsample_bytree` introduce **randomness in data selection**, enhancing model robustness. The model is trained using **200 boosting rounds**, with **early stopping** (`early_stopping_rounds = 10`) applied to prevent unnecessary computation when performance no longer improves on the validation set. After training, the model **predicts event probabilities** on both the validation and test sets. Predictions are converted into binary class labels using a **0.5 probability threshold**, and classification performance is assessed using **confusion matrices** and key evaluation metrics through `calculate_model_metrics()`.

```{r}
XG_Boost <- function(train_data, test_data) {
  # Convert train_data and test_data to data.table format
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features and scale them
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert data to matrix format required by XGBoost
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Convert event labels to 0/1
  y_train <- as.numeric(train_set$event == "yes")
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  y_validation <- as.numeric(validation_set$event == "yes")
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Convert to DMatrix format, which is optimized for XGBoost
  dtrain <- xgb.DMatrix(data = x_train, label = y_train)
  dvalidation <- xgb.DMatrix(data = x_validation, label = y_validation)
  dtest <- xgb.DMatrix(data = x_test)
  
  # Detect available CPU cores for parallel computation
  num_cores <- detectCores()
  
  # Define XGBoost hyperparameters
  params <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = 6,
    eta = 0.1,
    subsample = 0.8,
    colsample_bytree = 0.8,
    nthread = num_cores
  )
  
  # Train XGBoost model with early stopping using validation set
  xgb_model <- xgb.train(params = params,
                         data = dtrain,
                         nrounds = 200,
                         early_stopping_rounds = 10,
                         watchlist = list(train = dtrain, validation = dvalidation),
                         verbose = 0)
  
  # Predict probabilities on the validation dataset
  predict_probabilities_val <- predict(xgb_model, dvalidation)
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(factor(binary_prediction_val, levels = c("yes", "no")),
                                  factor(validation_set$event, levels = c("yes", "no")))
  
  # Evaluate validation set performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "XGBoost (Validation)")
  
  # Predict probabilities on the test dataset
  predict_probabilities_xgb <- predict(xgb_model, dtest)
  # Convert predicted probabilities into binary class labels (yes/no) using a threshold of 0.5
  binary_prediction_xgb <- ifelse(predict_probabilities_xgb > 0.5, "yes", "no")
  
  # Ensure both predicted and actual labels are factors with the same levels
  binary_prediction_xgb <- factor(binary_prediction_xgb, levels = c("yes", "no"))
  test_data$event <- factor(test_data$event, levels = c("yes", "no"))
  
  # Ensure confusion matrix includes all levels
  confusion_matrix_xgb <- table(factor(binary_prediction_xgb, levels = c("yes", "no")), 
                                factor(test_data$event, levels = c("yes", "no")))
  
  # Ensure matrix is complete to avoid subscript out of bounds error
  if (!all(c("yes", "no") %in% rownames(confusion_matrix_xgb))) {
    confusion_matrix_xgb <- matrix(0, nrow = 2, ncol = 2, dimnames = list(c("yes", "no"), c("yes", "no")))
  }
  
  # Evaluate model performance using accuracy, precision, recall, and F1-score
  metrics_xgb <- calculate_model_metrics(confusion_matrix_xgb, predict_probabilities_xgb, "XGBoost")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_xgb_dataframe <- get_dataframe("XGBoost", metrics_xgb)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_xgb_dataframe = metrics_xgb_dataframe, metrics_xgb = metrics_xgb))
}
```

### def elastic_net

This **Elastic Net model** is a **regularized logistic regression classifier** that combines **Lasso (L1) and Ridge (L2) penalties**, offering a balance between **feature selection** and **coefficient shrinkage**. The model starts with **data preprocessing**, where numeric features are standardized to ensure stable optimization, and categorical features are transformed into a matrix format using `model.matrix()`, making them compatible with `glmnet`. The dataset is then split into **80% training set and 20% validation set**, allowing for hyperparameter tuning to find the optimal regularization strength. Elastic Net is trained with an **alpha value of 0.5**, meaning it applies **equal weight to both L1 and L2 regularization**, ensuring that it not only selects important features (like Lasso) but also maintains **feature correlations** (like Ridge). The model then undergoes **cross-validation (`cv.glmnet()`)**, which evaluates multiple values of `lambda`, the regularization parameter. The **best lambda (`lambda.min`)** is selected based on the lowest cross-validation error, ensuring an optimal trade-off between **bias and variance**. Once the **best lambda** is determined, the model makes **probabilistic predictions** on both the validation and test sets. These probabilities are **converted into binary classifications** using a **0.5 decision threshold**, and performance is assessed using a **confusion matrix and key classification metrics**, calculated through `calculate_model_metrics()`.

```{r}
elastic_net <- function(train_data, test_data) {
  # Load required libraries
  # library(glmnet) # Required for Elastic Net (Lasso + Ridge regularization)
  # library(data.table) # For efficient data handling using data.table
  
  # Convert train_data and test_data to data.table format for optimized processing
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset for standardization
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert the dataset into a matrix format, as required by glmnet
  # Feature matrix for training
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Target variable
  y_train <- train_set$event
  # Feature matrix for validation
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  # Feature matrix for testing
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Train the Elastic Net model with a combination of Lasso (L1) and Ridge (L2) regularization
  # alpha = 0.5 sets an equal mix of Lasso and Ridge penalties
  elastic_net_model <- glmnet(x_train, y_train, family = "binomial", alpha = 0.5)
  
  # Use cross-validation to determine the best lambda value
  cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0.5)
  best_lambda <- cv_model$lambda.min
  
  # Predict event probabilities for the validation dataset
  predict_probabilities_val <- predict(elastic_net_model, s = best_lambda, newx = x_validation, type = "response")
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Elastic Net (Validation)")
  
  # Predict event probabilities for the test dataset using the best lambda
  predict_probabilities_en <- predict(elastic_net_model, s = best_lambda, newx = x_test, type = "response")
  binary_prediction_en <- ifelse(predict_probabilities_en > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  confusion_matrix_en <- table(Predicted = binary_prediction_en, Actual = test_data$event)
  
  # Evaluate model performance using key classification metrics (accuracy, precision, recall, F1-score)
  metrics_en <- calculate_model_metrics(confusion_matrix_en, predict_probabilities_en, "Elastic Net")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_en_dataframe <- get_dataframe("Elastic Net", metrics_en)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_en_dataframe = metrics_en_dataframe, metrics_en = metrics_en))
}
```

### def logistic_regression_op

This **logistic regression model** implements **Lasso regularization (L1 penalty)** to enhance feature selection and prevent overfitting in binary classification tasks. The process begins with **data preprocessing**, where numeric features in the training, validation, and test sets are **standardized** (mean = 0, standard deviation = 1) to improve optimization stability. The data is then converted into **matrix format** using `model.matrix()`, ensuring compatibility with the `glmnet` package. To determine the **optimal regularization parameter (`lambda`)**, the model first **performs cross-validation (`cv.glmnet()`)** on the training set, generating a range of candidate `lambda` values. These values are then evaluated using the **validation set**, where for each `lambda`, the model predicts event probabilities and computes the **AUC (Area Under the Curve)**. The `lambda` yielding the highest **AUC** is selected as the best parameter, ensuring that the model maximizes predictive performance. Once the **best lambda** is determined, the model is retrained on the full training set with this optimal parameter. Predictions are made on both the **validation and test sets**, with event probabilities converted into **binary classifications** using a **0.5 threshold**. Model performance is assessed through **confusion matrices** and various classification metrics, calculated via `calculate_model_metrics()`.

```{r}
logistic_regression_op <- function(train_data, test_data) {
  # Load required libraries
  # library(glmnet) # Logistic regression with regularization
  # library(data.table) # Efficient data handling
  # library(pROC) # Compute AUC for validation
  
  # Convert train_data and test_data to data.table format
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features and scale them
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Standardize numeric features
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert datasets to matrix format as required by glmnet
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Convert event labels to 0/1
  y_train <- as.numeric(train_set$event == "yes")
  
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  # Convert event labels to 0/1
  y_validation <- as.numeric(validation_set$event == "yes")
  
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Apply logistic regression with Lasso regularization (alpha = 1)
  logistic_regression_classifier <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
  
  # Perform cross-validation to get a range of lambda values
  cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
  # Extract lambda values from cross-validation
  lambda_candidates <- cv_model$lambda
  
  # Select best lambda based on AUC from validation set
  best_lambda <- NULL
  # Initialize best AUC score
  best_auc <- -Inf
  
  for (lambda in lambda_candidates) {
    # Predict probabilities on validation set
    predict_probabilities_val <- predict(logistic_regression_classifier, s = lambda, newx = x_validation, 
                                         type = "response")
    # Convert matrix to numeric vector
    predict_probabilities_val <- as.vector(predict_probabilities_val)
    
    # Compute AUC for validation set
    roc_obj <- roc(y_validation, predict_probabilities_val)
    auc_val <- auc(roc_obj)
    
    # Update best lambda if current AUC is better
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_lambda <- lambda
    }
  }
  
  # Train final model with the best lambda
  final_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = best_lambda)
  
  # Predict on validation dataset using the best lambda
  predict_probabilities_val <- predict(final_model, newx = x_validation, type = "response")
  predict_probabilities_val <- as.vector(predict_probabilities_val)
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation set performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Logistic Regression (Validation)")
  
  # Predict on test dataset using the best lambda
  predict_probabilities_test <- predict(final_model, newx = x_test, type = "response")
  predict_probabilities_test <- as.vector(predict_probabilities_test)
  binary_prediction_test <- ifelse(predict_probabilities_test > 0.5, "yes", "no")
  
  # Create confusion matrix for the test set
  test_conf_matrix <- table(Predicted = binary_prediction_test, Actual = test_data$event)
  
  # Evaluate model performance on the test set
  metrics_lr <- calculate_model_metrics(test_conf_matrix, predict_probabilities_test, "Logistic Regression")
  
  # Store calculated metrics in a structured dataframe
  metrics_lr_dataframe <- get_dataframe("Logistic Regression", metrics_lr)
  
  return (list(metrics_lr_dataframe = metrics_lr_dataframe, metrics_lr = metrics_lr))
}
```

### def decision_tree_op

This **Decision Tree model** is a classification algorithm designed to iteratively split the data based on feature values, forming a tree-like structure for decision-making. The process begins with **data partitioning**, where the dataset is split into **80% training set and 20% validation set**, ensuring a separate dataset for hyperparameter tuning. The model undergoes **hyperparameter optimization** by selecting the best **complexity parameter (cp)**, which controls **pruning** and helps prevent overfitting. A range of `cp` values is tested, and for each candidate value, a **decision tree is trained on the training set**, followed by **prediction on the validation set**. The model's **AUC (Area Under the Curve)** is computed for each `cp` value, and the one that **yields the highest AUC** is selected as the **optimal pruning parameter**. Once the **best `cp`** is determined, the final **decision tree** is trained using this optimal parameter and is applied to both the **validation and test sets**. Predictions are generated, and classification performance is evaluated using a **confusion matrix and key metrics** computed through `calculate_model_metrics()`.

```{r}
decision_tree_op <- function(train_data, test_data) {
  # Load required libraries
  # library(rpart)
  # library(caret) # Load caret for data partitioning
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Define candidate complexity parameters (cp) for pruning
  # Range of possible cp values
  cp_candidates <- seq(0.0001, 0.05, by = 0.002)
  best_cp <- NULL
  # Initialize the best AUC score
  best_auc <- -Inf
  
  # Loop over different cp values to find the best one using the validation set
  for (cp in cp_candidates) {
    # Train the decision tree model with the current cp value
    decision_tree_model <- rpart(
      event ~ .,
      data = train_set,
      method = "class",
      control = rpart.control(
        cp = cp,        # Adjusting cp for pruning
        maxdepth = 30,  # Keeping max depth fixed
        minsplit = 20   # Keeping minsplit fixed
      )
    )
    
    # Predict probabilities on validation set
    # Get probability for "yes"
    predict_probabilities_val <- predict(decision_tree_model, validation_set, type = "prob")[, 2]
    
    # Compute AUC for validation set
    auc_val <- auc(roc(validation_set$event, predict_probabilities_val))
    
    # Update the best cp if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_cp <- cp
    }
  }
  
  # Train the final model with the best cp value
  decision_tree_classifier <- rpart(
    event ~ .,
    data = train_set,
    method = "class",
    control = rpart.control(
      cp = best_cp,  # Using the best cp found from validation set
      maxdepth = 30,
      minsplit = 20
    )
  )
  
  # Predict on the validation dataset using the selected cp
  predict_probabilities_val <- predict(decision_tree_classifier, validation_set, type = "class")
  validation_conf_matrix <- table(Predicted = predict_probabilities_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Decision Tree (Validation)")
  
  # Predict on the testing dataset using the final model
  predict_probabilities_dt <- predict(decision_tree_classifier, test_data, type = "class")
  test_conf_matrix <- table(Predicted = predict_probabilities_dt, Actual = test_data$event)
  
  # Evaluate model performance
  metrics_dt <- calculate_model_metrics(test_conf_matrix, predict_probabilities_dt, "Decision Tree")
  
  # Create a dataframe with the desired structure
  metrics_dt_dataframe = get_dataframe("Decision Tree", metrics_dt)
  
  return (list(metrics_dt_dataframe = metrics_dt_dataframe, metrics_dt = metrics_dt))
}
```

### def naive_bayes_op

This **Naïve Bayes model** is a probabilistic classifier based on **Bayes’ theorem**, assuming **feature independence** and leveraging **Laplace smoothing** to enhance performance. The model begins with **data preprocessing**, where the target variable is converted into a **factor**, ensuring compatibility with the `naiveBayes()` function. The dataset is then split into **80% training set and 20% validation set**, allowing for hyperparameter tuning. To optimize classification performance, the model tests multiple **Laplace smoothing values** ranging from 0 to 1, which help handle cases where certain feature-class combinations do not appear in the training data. For each candidate Laplace value, a **Naïve Bayes classifier** is trained and evaluated on the validation set, with **AUC (Area Under the Curve)** computed as the selection criterion. The Laplace value that yields the **highest AUC** is chosen as the optimal parameter, ensuring robust probability estimation. Once the best Laplace parameter is determined, the final **Naïve Bayes model** is trained on the full training set and applied to **both the validation and test datasets**. Predictions are made in **both categorical and probabilistic forms**, with **probabilities for the positive class ("yes")** used to evaluate model performance. The results are assessed using a **confusion matrix and classification metrics**, computed through `calculate_model_metrics()`.

```{r}
naive_bayes_op <- function(train_data, test_data) {
  # Load required libraries
  # library(e1071)
  # library(caret) # Load caret for data partitioning
  
  target_column = "event"
  
  # Convert the target column to a factor if it's not already
  train_data[[target_column]] <- as.factor(train_data[[target_column]])
  test_labels <- as.factor(test_data[[target_column]])
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data[[target_column]], p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Remove the target column from the validation and test sets for prediction
  validation_features <- validation_set %>% select(-all_of(target_column))
  test_features <- test_data %>% select(-all_of(target_column))
  
  # Define candidate Laplace smoothing values
  # Range of laplace values to test
  laplace_candidates <- seq(0, 1, by = 0.1)
  best_laplace <- NULL
  # Initialize the best AUC score
  best_auc <- -Inf
  
  # Hyperparameter tuning: Find the best laplace value using validation set
  for (laplace in laplace_candidates) {
    # Train Naïve Bayes model with current laplace value
    nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set, laplace = laplace)
    
    # Get probability predictions on the validation set
    # Probability for "yes"
    validation_probabilities <- predict(nb_model, validation_features, type = "raw")[, 2]
    
    # Compute AUC for validation set
    auc_val <- auc(roc(validation_set[[target_column]], validation_probabilities))
    
    # Update best laplace if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_laplace <- laplace
    }
  }
  
  # Train the final Naïve Bayes model with the best laplace value
  nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set, laplace = best_laplace)
  
  # Make predictions on the validation set
  validation_predictions <- predict(nb_model, validation_features)
  
  # Get prediction probabilities for validation set
  validation_probabilities <- predict(nb_model, validation_features, type = "raw")
  
  # Evaluate model performance with a confusion matrix for validation set
  validation_conf_matrix <- table(Predicted = validation_predictions, Actual = validation_set[[target_column]])
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, validation_probabilities, 
                                                "Naive Bayes (Validation)")
  
  # Make predictions on the test set using the final model
  predictions <- predict(nb_model, test_features)
  
  # Get prediction probabilities for test set
  prediction_probabilities <- predict(nb_model, test_features, type = "raw")
  
  # Ensure both predicted and actual labels are factors with the same levels
  predictions <- factor(predictions, levels = levels(test_labels))
  
  # Evaluate model performance with a confusion matrix for test set
  conf_matrix <- table(Predicted = predictions, Actual = test_labels)
  
  metrics_nb <- calculate_model_metrics(conf_matrix, prediction_probabilities, "Naive Bayes")
  
  # Create a dataframe with the desired structure
  metrics_nb_dataframe = get_dataframe("Naive Bayes", metrics_nb)
  
  # Each classification model needs to return these two variables
  return (list(metrics_nb_dataframe = metrics_nb_dataframe, metrics_nb = metrics_nb))
}
```

### def elastic_net_op

This **Elastic Net model** is an advanced **regularized logistic regression classifier** that combines both **Lasso (L1) and Ridge (L2) penalties**, optimizing for **feature selection and coefficient stability**. The model begins with **data preprocessing**, where numeric features are standardized for improved optimization, and categorical features are transformed into a matrix format using `model.matrix()` to ensure compatibility with `glmnet`. The dataset is split into **80% training and 20% validation**, allowing for hyperparameter tuning. Unlike standard models that rely on a predefined **alpha** value, this implementation systematically searches for the **best combination of alpha (L1-L2 mixing parameter) and lambda (regularization strength)** using the validation set. The model iterates through **alpha values ranging from 0 to 1**, with each step performing **cross-validation (`cv.glmnet()`)** to find the corresponding **optimal lambda**. The combination that achieves the **highest AUC (Area Under the Curve) score** on the validation set is selected as the best set of parameters. After determining the optimal hyperparameters, the final model is retrained on the full training set and used to **predict probabilities for both the validation and test sets**. Predictions are thresholded at **0.5** to produce binary classifications, which are evaluated using a **confusion matrix and key classification metrics** via `calculate_model_metrics()`.

```{r}
elastic_net_op <- function(train_data, test_data) {
  # Load required libraries
  # library(glmnet) # Required for Elastic Net (Lasso + Ridge regularization)
  # library(data.table) # For efficient data handling using data.table
  # library(pROC) # Compute AUC for validation
  
  # Convert train_data and test_data to data.table format for optimized processing
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset for standardization
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert the dataset into a matrix format, as required by glmnet
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Convert event labels to 0/1
  y_train <- as.numeric(train_set$event == "yes")
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  # Convert event labels to 0/1
  y_validation <- as.numeric(validation_set$event == "yes")
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Define candidate alpha values for Elastic Net optimization
  alpha_candidates <- seq(0, 1, by = 0.1)  # Range from 0 (Ridge) to 1 (Lasso)
  best_alpha <- NULL
  best_lambda <- NULL
  # Initialize best AUC score
  best_auc <- -Inf
  
  # Iterate through different alpha values to find the best one using validation set
  for (alpha in alpha_candidates) {
    # Perform cross-validation to find the best lambda for the current alpha
    cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = alpha)
    # Get best lambda from cross-validation
    lambda_min <- cv_model$lambda.min
    
    # Predict on validation set
    predict_probabilities_val <- predict(cv_model$glmnet.fit, s = lambda_min, newx = x_validation, 
                                         type = "response")
    # Convert matrix to numeric vector
    predict_probabilities_val <- as.vector(predict_probabilities_val)
    
    # Compute AUC for validation set
    roc_obj <- roc(y_validation, predict_probabilities_val)
    auc_val <- auc(roc_obj)
    
    # Update best alpha and lambda if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_alpha <- alpha
      best_lambda <- lambda_min
    }
  }
  
  # Train the final Elastic Net model with the best alpha and lambda values
  elastic_net_model <- glmnet(x_train, y_train, family = "binomial", alpha = best_alpha, lambda = best_lambda)
  
  # Predict on the validation dataset using the selected best parameters
  predict_probabilities_val <- predict(elastic_net_model, newx = x_validation, type = "response")
  # Convert matrix to numeric vector
  predict_probabilities_val <- as.vector(predict_probabilities_val)
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Elastic Net (Validation)")
  
  # Predict event probabilities for the test dataset using the best parameters
  predict_probabilities_en <- predict(elastic_net_model, newx = x_test, type = "response")
  # Convert matrix to numeric vector
  predict_probabilities_en <- as.vector(predict_probabilities_en)
  binary_prediction_en <- ifelse(predict_probabilities_en > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  confusion_matrix_en <- table(Predicted = binary_prediction_en, Actual = test_data$event)
  
  # Evaluate model performance using key classification metrics (accuracy, precision, recall, F1-score)
  metrics_en <- calculate_model_metrics(confusion_matrix_en, predict_probabilities_en, "Elastic Net")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_en_dataframe <- get_dataframe("Elastic Net", metrics_en)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_en_dataframe = metrics_en_dataframe, metrics_en = metrics_en))
}
```

## Deep Learning Classification Model

### def deephit_model

The `deephit_model` function is used to train the DeepHit model and predict and evaluate the test data. The function first performs data validation to ensure that `train_data` and `test_data` are not empty and contain the necessary `event` column. Next, the training data is divided into 80% training set and 20% validation set to ensure the generalization ability of model training. Subsequently, the data is preprocessed, including deleting the possible `id` column (because this column is meaningless for model training) and extracting the feature matrix `X` (removing the `event` column) and label `y` (retaining only the `event` column). To ensure the numerical processing of the data, the function defines the `convert_to_numeric` auxiliary function to convert all columns to numerical types and fill missing values with the median (if the median is not available, fill it with 0). After the conversion is completed, the feature data of the training set, validation set, and test set are converted into matrices to adapt to the model input format on the Python side, and the `event` label is converted to binary (0/1) form to ensure the correctness of the classification task. Next, the function uses `r_to_py` to convert the processed data into a format recognizable by Python and loads the `deephit_model.py` Python script, which contains the implementation of the DeepHit model. Subsequently, `train_deephit` is called to train the model. If `class_weight` is provided, it is used to adjust the class imbalance problem. After training, the trained model is used to predict the test set, and the prediction results are converted back to R for analysis. Subsequently, the function calculates the confusion matrix to compare the prediction results with the true labels, and calculates the model performance indicators (such as accuracy, sensitivity, etc.) through `calculate_model_metrics`. Finally, the function uses `get_dataframe` to convert the DeepHit related evaluation indicators into a data frame and returns a list containing the performance data frame and the original evaluation indicators.

```{r}
# Function to train a DeepHit model using the given training and test datasets
deephit_model <- function(train_data, test_data, epochs = 10, num_nodes = c(64L, 64L), 
                          dropout = 0, batch_size = 256L, lr = 0.001, class_weight = NULL) {
  
  # library(reticulate) # Enables integration with Python, allowing R to call Python functions
  # library(dplyr) # Provides functions for data manipulation and transformation
  # Data Validation: Ensure both training and testing datasets are provided, not empty, and contain 
  # the required 'event' column.
  if (is.null(train_data) || is.null(test_data)) {
    stop("Error: train_data or test_data is NULL!")
  }
  if (nrow(train_data) == 0 || nrow(test_data) == 0) {
    stop("Error: train_data or test_data is empty!")
  }
  if (!("event" %in% colnames(train_data)) || !("event" %in% colnames(test_data))) {
    stop("Error: 'event' column is missing in train_data or test_data!")
  }
  
  # Split the training data into training (80%) and validation (20%) subsets
  set.seed(123) # Setting seed for reproducibility
  train_idx <- sample(seq_len(nrow(train_data)), size = 0.8 * nrow(train_data))
  val_idx <- setdiff(seq_len(nrow(train_data)), train_idx)
  train_subset <- train_data[train_idx, ]
  val_subset <- train_data[val_idx, ]
  
  # Preprocess: Remove the "id" column if it exists in any of the datasets, as it is not needed 
  # for model training.
  if ("id" %in% names(train_subset)) {
    train_subset <- train_subset %>% select(-id)
  }
  if ("id" %in% names(val_subset)) {
    val_subset <- val_subset %>% select(-id)
  }
  if ("id" %in% names(test_data)) {
    test_data <- test_data %>% select(-id)
  }
  
  # Feature and Label Extraction:
  # For each dataset, separate the features (all columns except 'event') and the labels (the 'event' column).
  X_train <- train_subset %>% select(-event)
  y_train <- train_subset$event
  X_val <- val_subset %>% select(-event)
  y_val <- val_subset$event
  X_test <- test_data %>% select(-event)
  y_test <- test_data$event
  
  # Define a helper function to convert input data to numeric type and handle missing values
  # This function checks the type of the input:
  # - If already numeric, it is returned as-is
  # - If a factor or character, it converts to numeric (with warnings suppressed)
  # - Missing values (NA) are replaced with the median of the column; if median is unavailable, 0 is used
  convert_to_numeric <- function(x) {
    if (is.numeric(x)) {
      res <- x
    } else if (is.factor(x)) {
      res <- suppressWarnings(as.numeric(as.character(x)))
    } else if (is.character(x)) {
      res <- suppressWarnings(as.numeric(x))
    } else {
      res <- suppressWarnings(as.numeric(x))
    }
    if (any(is.na(res))) {
      med <- median(res, na.rm = TRUE)
      if (is.na(med)) med <- 0
      res[is.na(res)] <- med
    }
    return(res)
  }
  
  # Apply the conversion to numeric for all features in the training, validation, and testing sets
  X_train <- X_train %>% mutate(across(everything(), convert_to_numeric))
  X_val <- X_val %>% mutate(across(everything(), convert_to_numeric))
  X_test <- X_test %>% mutate(across(everything(), convert_to_numeric))
  
  # Convert data frames to matrices because the Python model expects matrix inputs
  X_train <- as.matrix(X_train)
  X_val <- as.matrix(X_val)
  X_test <- as.matrix(X_test)
  
  # Convert labels to binary numeric values (0/1)
  # Assumes the original labels start at 1, so subtract 1 to get 0-based labels
  y_train <- as.numeric(y_train) - 1
  y_val <- as.numeric(y_val) - 1
  y_test <- as.numeric(y_test) - 1
  
  # Prepare Data for Python:
  # Package training data along with validation data and model parameters into a list
  # Then convert the list to a Python object using r_to_py
  train_data_py <- r_to_py(list(
    X = X_train,
    y = y_train,
    val_X = X_val,
    val_y = y_val,
    epochs = as.integer(epochs),
    num_nodes = num_nodes,
    dropout = dropout,
    batch_size = as.integer(batch_size),
    lr = lr
  ))
  
  # Similarly, prepare the test dataset for prediction
  test_data_py <- r_to_py(list(X = X_test, y = y_test))
  
  # Convert class_weight to a Python dictionary if provided
  class_weight_py <- NULL
  if (!is.null(class_weight)) {
    class_weight_py <- r_to_py(as.list(class_weight))
  }
  
  # Load the Python script that contains the implementation of the DeepHit model
  source_python("deephit_model.py")
  
  # Train the DeepHit model using the training data prepared in Python
  # Pass class_weight_py to handle class imbalance in Python
  deep_model <- train_deephit(train_data_py, class_weight = class_weight_py)
  
  # Use the trained model to make predictions on the test data
  predictions <- deep_model$predict(test_data_py[["X"]])
  
  # Convert the predictions from Python back to R
  predictions <- py_to_r(predictions)
  
  # Create a confusion matrix comparing the predicted labels to the actual test labels
  test_conf_matrix <- table(
    Predicted = factor(predictions, levels = c(0, 1)),
    Actual = factor(y_test, levels = c(0, 1))
  )
  
  print(test_conf_matrix)
  
  # Calculate model performance metrics (such as accuracy, sensitivity, etc.) using the confusion matrix
  # The function calculate_model_metrics is assumed to be defined elsewhere
  metrics_dh <- calculate_model_metrics(test_conf_matrix, predictions, "DeepHit")
  
  # Convert the metrics into a data frame for reporting or further analysis
  metrics_dh_dataframe <- get_dataframe("DeepHit", metrics_dh)
  
  # Return a list containing both the metrics data frame and the raw metrics
  return(list(metrics_dh_dataframe = metrics_dh_dataframe, metrics_dh = metrics_dh))
}
```

### def deephit_model.py

This Python code implements the training and prediction process of the DeepHit model, including core parts such as custom loss functions, model construction, training and prediction. First, the `ranking_loss` function is defined, which calculates the ranking loss and encourages the model to predict that the mean probability of the positive class (event occurrence) is higher than the mean probability of the negative class (event not occurring) by a fixed margin. During the loss calculation process, `K.clip` is used to limit the range of predicted probabilities to prevent numerical anomalies, and `K.argmax` is used to create positive and negative sample masks, calculate their mean probabilities respectively, and finally calculate the ranking loss. Subsequently, `combined_loss` combines the standard `categorical_crossentropy` with `ranking_loss`, where `alpha=0.2` is used as a weight to balance the effects of the two to improve the model's predictive ability. Next, the `DeephitModel` class defines the architecture and training method of the DeepHit model. The constructor `__init__` accepts the input feature dimension `input_dim` and the number of hidden layer units `hidden_units`, and constructs a three-layer neural network. The network contains two fully connected hidden layers with ReLU activation, and a `BatchNormalization` layer is added after each hidden layer to improve training stability, and `Dropout(0.2)` is added to reduce overfitting. Finally, the output layer uses the `softmax` activation function to output class probabilities for binary classification tasks. The model uses the `Adam` optimizer, the learning rate is set to `0.0005` and `clipnorm=1.0` is used for gradient clipping to prevent gradient explosion. In addition, the loss function is set to `combined_loss`, and `accuracy` is used as the evaluation metric. The class method `fit` is responsible for training the model, allowing you to pass in training data `(X, y)`, the maximum number of training rounds `epochs`, the batch size `batch_size`, and the optional validation set `(val_X, val_y)` and class weight `class_weight`. During training, `EarlyStopping` is used to monitor `val_loss`. If there is no improvement for 3 consecutive rounds, the training is terminated early. At the same time, `ReduceLROnPlateau` is used to dynamically adjust the learning rate when the loss does not decrease to improve the optimization effect. The class method `predict` is responsible for using the trained model to predict the input data `X` and return the most likely category index (0 or 1). Finally, the `train_deephit` function is used to train the DeepHit model. It receives a dictionary `train_data` containing training data (including `X` and `y`, and optional `val_X` and `val_y`), and supports `class_weight` to handle class imbalance problems. The function first converts `X` and `y` to `numpy` arrays and checks the `X` dimension to ensure that it contains at least one feature. Then, `y` is converted to `one-hot` encoding to adapt to the `categorical_crossentropy` loss function. Depending on whether `train_data` contains a validation set, the `fit` method of `DeephitModel` is called for training. Finally, the function returns the trained DeepHit model instance, which can be used for subsequent prediction analysis.

```{python}
import numpy as np # For numerical operations
import tensorflow.keras.backend as K # For backend tensor operations in TensorFlow
from tensorflow.keras.models import Sequential # To create a sequential neural network model
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization  # *** New comment: Added BatchNormalization for training stability
from tensorflow.keras.optimizers import Adam # To use the Adam optimizer during training
from tensorflow.keras.utils import to_categorical # To convert labels into one-hot encoded format
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # To control training with callbacks

# Define A Function To Compute Ranking Loss Based On The Positive And Negative Classes
def ranking_loss(y_true, y_pred):
    """
    Computes a simple ranking loss that encourages the positive class's 
    mean predicted probability to exceed that of the negative class by a margin
    """
    # Extract The Predicted Probability For Class 1 and clip it to avoid extreme values
    p = K.clip(y_pred[:, 1], 1e-7, 1-1e-7)  # Clip predicted probabilities to prevent NaN issues
    # Create A Mask For Positive Samples Where The True Label Is Class 1
    pos_mask = K.cast(K.equal(K.argmax(y_true, axis=-1), 1), K.floatx())
    # Create A Mask For Negative Samples As The Complement Of The Positive Mask
    neg_mask = 1 - pos_mask
    # Compute The Mean Predicted Probability For Positive Samples
    pos_mean = K.sum(p * pos_mask) / (K.sum(pos_mask) + K.epsilon())
    # Compute The Mean Predicted Probability For Negative Samples
    neg_mean = K.sum(p * neg_mask) / (K.sum(neg_mask) + K.epsilon())
    # Define A Fixed Margin Value
    margin = 0.1
    # Return The Maximum Of Zero And The Difference Between The Margin And The Difference Of Means
    return K.maximum(0.0, margin - (pos_mean - neg_mean))

# Define A Combined Loss Function That Adds Categorical Crossentropy And A Weighted Ranking Loss
def combined_loss(y_true, y_pred):
    """
    Combined loss: standard categorical crossentropy plus a weighted ranking loss
    """
    # Compute The Standard Categorical Crossentropy Loss
    ce = K.categorical_crossentropy(y_true, y_pred)
    # Compute The Ranking Loss Using The Previously Defined Function
    rl = ranking_loss(y_true, y_pred)
    # Define A Weight For The Ranking Loss Component
    alpha = 0.2 # Weight For The Ranking Loss
    # Return The Sum Of The Categorical Crossentropy And The Weighted Ranking Loss
    return ce + alpha * rl

# Define A Class For The DeepHit Model Architecture And Training Procedures
class DeephitModel:
    # Initialize The DeepHit Model With The Provided Input Dimension And Hidden Units
    def __init__(self, input_dim, hidden_units=64):
        """
        Initializes a deep neural network model for binary classification following the DeepHit approach
        The model uses a combined loss (categorical crossentropy + ranking loss)
        
        Parameters:
        - input_dim: Number of input features
        - hidden_units: Number of neurons in hidden layers
        """
        # Build A Sequential Model With An Input Layer, Two Dense Layers With ReLU Activation, 
        # Dropout Layers, and BatchNormalization, And An Output Layer With Softmax Activation
        self.model = Sequential([
            Input(shape=(input_dim,)),
            Dense(hidden_units, activation='relu'),
            BatchNormalization(), # Added BatchNormalization for stability
            Dropout(0.2),
            Dense(hidden_units, activation='relu'),
            BatchNormalization(), # Added BatchNormalization for stability
            Dropout(0.2),
            Dense(2, activation='softmax') # Output Layer For Binary Classification
        ])
        # Compile The Model Using The Adam Optimizer, Combined Loss Function, And Accuracy As A Metric
        # Set learning rate to 0.0005 and added clipnorm for gradient clipping
        self.model.compile(optimizer=Adam(learning_rate=0.0005, clipnorm=1.0),
                           loss=combined_loss,
                           metrics=['accuracy'])
    
    # Define A Method To Train The Model With Optional Validation Data
    def fit(self, X, y, epochs=10, batch_size=32, validation_data=None, class_weight=None):
        """
        Trains the DeepHit model using early stopping and learning rate reduction
        
        Parameters:
        - X: Training feature matrix
        - y: One-hot encoded labels
        - epochs: Maximum number of training epochs
        - batch_size: Batch size
        - validation_data: Optional tuple (val_X, val_y) for validation
        - class_weight: Optional dictionary to handle imbalanced data, e.g. {0: 1.0, 1: 5.0}
        """
        # Define Callbacks For Early Stopping And Learning Rate Reduction 
        # Monitor 'val_loss' if validation data is provided, otherwise monitor 'loss'
        monitor_metric = 'val_loss' if validation_data is not None else 'loss'
        callbacks = [
            EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True),
            ReduceLROnPlateau(monitor=monitor_metric, factor=0.5, patience=2, min_lr=1e-6)
        ]
        
        # Pass 'class_weight' to handle class imbalance
        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0,
                       validation_data=validation_data, callbacks=callbacks,
                       class_weight=class_weight)
    
    # Define A Method To Predict Class Labels From Input Data
    def predict(self, X):
        """
        Predicts class labels for the given input
        
        Parameters:
        - X: Input feature matrix
        
        Returns:
        - Predicted class labels (0 or 1)
        """
        # Generate Predictions Using The Model And Select The Class With The Highest Predicted 
        # Probability For Each Sample
        preds = self.model.predict(X)
        return np.argmax(preds, axis=1)

# Define A Function To Train The DeepHit Model Using Provided Training Data
def train_deephit(train_data, class_weight=None):
    """
    Trains a DeepHit model using the provided training data
    
    Parameters:
    - train_data: A dictionary with keys:
         "X": Features
         "y": Labels (assumed to be 0/1)
         Optional keys "val_X" and "val_y" for validation data
    - class_weight: Optional dictionary to handle imbalanced data
    
    Returns:
    - A trained DeephitModel instance
    """
    # Convert Training Features And Labels To Numpy Arrays
    X = np.array(train_data["X"])
    y = np.array(train_data["y"])
    # Reshape X If It Is A One-Dimensional Array To Ensure It Has Two Dimensions
    if (X.ndim == 1):
        X = X.reshape(-1, 1)
    # Raise An Error If The Input Data Has No Features
    if (X.shape[1] == 0):
        raise ValueError("Invalid input: X has no features")
    # Determine The Number Of Input Features
    input_dim = X.shape[1]
    # Convert The Labels To A One-Hot Encoded Format For Binary Classification
    y_onehot = to_categorical(y, num_classes=2)
    
    # Initialize The DeepHit Model With The Determined Input Dimension And A Fixed Number Of Hidden Units
    model = DeephitModel(input_dim=input_dim, hidden_units=64)
    
    # Check If Validation Data Is Provided In The Training Data Dictionary
    if ("val_X" in train_data and "val_y" in train_data):
        val_X = np.array(train_data["val_X"])
        val_y = np.array(train_data["val_y"])
        if (val_X.ndim == 1):
            val_X = val_X.reshape(-1, 1)
        val_y_onehot = to_categorical(val_y, num_classes=2)
        
        # Train The Model Using Both The Training And Validation Data
        # Pass class_weight here to mitigate imbalanced data issues
        model.fit(X, y_onehot, epochs=10, batch_size=32, 
                  validation_data=(val_X, val_y_onehot), 
                  class_weight=class_weight)
    else:
        # Train The Model Using Only The Training Data If No Validation Data Is Provided
        model.fit(X, y_onehot, epochs=10, batch_size=32, class_weight=class_weight)
    
    return model
```

### def transformation_surv_model

The main function of the `transformation_surv_model` function is to train the **Transformation Survival Model** and use the test data for prediction and evaluation. The function first performs data validation to ensure that `train_data` and `test_data` are neither empty nor contain the required `event` column, otherwise an error message will be thrown. Subsequently, the training data is divided into **80% training set** and **20% validation set** to improve the generalization ability of the model, and the random seed is set to ensure the reproducibility of data partitioning. During the data preprocessing stage, the function automatically removes the `id` column (if it exists) because this column usually does not directly contribute to model training. During the feature and label extraction process, all data sets (training set, validation set, and test set) are split into feature matrices `X` (removing the `event` column) and labels `y` (retaining only the `event` column). To ensure consistent data format, the function defines a `convert_to_numeric` auxiliary function to convert all feature columns to numerical types and fill missing values with the median (if the median is not available, it is filled with 0), and then convert the data to a matrix format to adapt to the deep learning model on the Python side. In addition, the `event` label is converted to **binary (0/1) format** to ensure that the classification model can handle it correctly. After the data is processed, the function converts the data to a format recognizable by Python through `r_to_py` and creates a list containing training data, hyperparameters such as `epochs`, `num_nodes`, `dropout`, `batch_size`, and `lr`. Similarly, the test data is also converted to a Python-compatible format for subsequent prediction tasks. If the user provides `class_weight`, the information is converted to a Python dictionary to solve the problem of class imbalance. The function then calls `source_python("transformation_surv_model.py")` to load the `train_transformation_surv` model implemented in Python, and passes the processed training data to start model training. After training, the trained model is used to predict the test data `X_test`, and the prediction results on the Python side are converted back to R for analysis. To evaluate the performance of the model, the function constructs a **Confusion Matrix** to compare the categories predicted by the model and the true `y_test` labels. Then, performance metrics (such as accuracy, sensitivity, specificity, etc.) are calculated through `calculate_model_metrics` and converted to data frame format for further analysis and visualization. Finally, the function returns a list containing `metrics_tfs_dataframe` (performance data frame for display) and `metrics_tfs` (raw performance metrics) to support subsequent research and model optimization.

```{r}
transformation_surv_model <- function(train_data, test_data, epochs = 10, num_nodes = c(64L, 64L), dropout = 0, 
                                      batch_size = 256L, lr = 0.001, class_weight = NULL) {
  
  # library(reticulate) # Enables integration with Python, allowing R to call Python functions
  # library(dplyr) # Provides functions for data manipulation and transformation
  # Data Validation: Ensure that training and testing datasets are provided, not empty, and include 
  # the 'event' column.
  if (is.null(train_data) || is.null(test_data)) {
    stop("Error: train_data or test_data is NULL!")
  }
  if (nrow(train_data) == 0 || nrow(test_data) == 0) {
    stop("Error: train_data or test_data is empty!")
  }
  if (!("event" %in% colnames(train_data)) || !("event" %in% colnames(test_data))) {
    stop("Error: 'event' column is missing!")
  }
  
  # Split the training data into training (80%) and validation (20%) subsets
  set.seed(123) # Set seed for reproducibility
  train_idx <- sample(seq_len(nrow(train_data)), size = 0.8 * nrow(train_data))
  val_idx <- setdiff(seq_len(nrow(train_data)), train_idx)
  train_subset <- train_data[train_idx, ]
  val_subset <- train_data[val_idx, ]
  
  # Preprocess: Remove the "id" column from training, validation, and testing datasets if present
  if ("id" %in% names(train_subset)) {
    train_subset <- train_subset %>% select(-id)
  }
  if ("id" %in% names(val_subset)) {
    val_subset <- val_subset %>% select(-id)
  }
  if ("id" %in% names(test_data)) {
    test_data <- test_data %>% select(-id)
  }
  
  # Feature and Label Extraction:
  # Separate features and labels for training, validation, and test datasets
  X_train <- train_subset %>% select(-event)
  y_train <- train_subset$event
  X_val <- val_subset %>% select(-event)
  y_val <- val_subset$event
  X_test <- test_data %>% select(-event)
  y_test <- test_data$event
  
  # Define a helper function to convert features to numeric and handle missing values
  convert_to_numeric <- function(x) {
    if (is.numeric(x)) {
      res <- x
    } 
    else if (is.factor(x)) {
      res <- suppressWarnings(as.numeric(as.character(x)))
    } 
    else if (is.character(x)) {
      res <- suppressWarnings(as.numeric(x))
    } 
    else {
      res <- suppressWarnings(as.numeric(x))
    }
    if (any(is.na(res))) {
      med <- median(res, na.rm = TRUE)
      if (is.na(med)) med <- 0 # Default to 0 if median cannot be computed
      res[is.na(res)] <- med
    }
    return(res)
  }
  
  # Convert all feature columns in training, validation, and test sets to numeric
  X_train <- X_train %>% mutate(across(everything(), convert_to_numeric))
  X_val <- X_val %>% mutate(across(everything(), convert_to_numeric))
  X_test <- X_test %>% mutate(across(everything(), convert_to_numeric))
  
  # Convert the data frames of features to matrices
  X_train <- as.matrix(X_train)
  X_val <- as.matrix(X_val)
  X_test <- as.matrix(X_test)
  
  # Convert labels to binary numeric values (0/1)
  y_train <- as.numeric(y_train) - 1
  y_val <- as.numeric(y_val) - 1
  y_test <- as.numeric(y_test) - 1
  
  # Prepare Data for Python:
  # Create a list containing the training features, labels, validation data, and model parameters, 
  # then convert it to a Python object.
  train_data_py <- r_to_py(list(
    X = X_train,
    y = y_train,
    val_X = X_val,
    val_y = y_val,
    epochs = as.integer(epochs),
    num_nodes = num_nodes,
    dropout = dropout,
    batch_size = as.integer(batch_size),
    lr = lr
  ))
  
  # Prepare the test data in a similar fashion
  test_data_py <- r_to_py(list(
    X = X_test,
    y = y_test
  ))
  
  # Convert class_weight to a Python dictionary if provided
  class_weight_py <- NULL
  if (!is.null(class_weight)) {
    class_weight_py <- r_to_py(as.list(class_weight))
  }
  
  # Load the Python script that implements the Transformation Survival model
  source_python("transformation_surv_model.py")
  
  # Train the Transformation Survival model using the provided training data
  # Pass class_weight_py to handle class imbalance in Python
  trans_model <- train_transformation_surv(train_data_py, class_weight = class_weight_py)
  
  # Use the trained model to predict outcomes on the test dataset
  predictions <- trans_model$predict(test_data_py[["X"]])
  
  # Convert predictions from Python format back to R
  predictions <- py_to_r(predictions)
  
  # Create a confusion matrix comparing predicted outcomes with the actual test labels
  test_conf_matrix <- table(
    Predicted = factor(predictions, levels = c(0, 1)),
    Actual = factor(y_test, levels = c(0, 1))
  )
  
  # Calculate performance metrics using the confusion matrix
  # This function (calculate_model_metrics) is assumed to be defined elsewhere
  metrics_tfs <- calculate_model_metrics(test_conf_matrix, predictions, "TransformationSurv")
  
  # Convert the metrics into a data frame for easy viewing and further analysis
  metrics_tfs_dataframe <- get_dataframe("TransformationSurv", metrics_tfs)
  
  # Return a list containing both the metrics data frame and the raw metrics
  return(list(metrics_tfs_dataframe = metrics_tfs_dataframe, metrics_tfs = metrics_tfs))
}

# Function to train a Transformation Survival model using the given training and test datasets
transformation_surv_model <- function(train_data, test_data, epochs = 10, num_nodes = c(64L, 64L), dropout = 0, 
                                      batch_size = 256L, lr = 0.001, class_weight = NULL) {
  
  # library(reticulate) # Enables integration with Python, allowing R to call Python functions
  # library(dplyr) # Provides functions for data manipulation and transformation
  # Data Validation: Ensure that training and testing datasets are provided, not empty, and include 
  # the 'event' column.
  if (is.null(train_data) || is.null(test_data)) {
    stop("Error: train_data or test_data is NULL!")
  }
  if (nrow(train_data) == 0 || nrow(test_data) == 0) {
    stop("Error: train_data or test_data is empty!")
  }
  if (!("event" %in% colnames(train_data)) || !("event" %in% colnames(test_data))) {
    stop("Error: 'event' column is missing!")
  }
  
  # Split the training data into training (80%) and validation (20%) subsets
  set.seed(123) # Set seed for reproducibility
  train_idx <- sample(seq_len(nrow(train_data)), size = 0.8 * nrow(train_data))
  val_idx <- setdiff(seq_len(nrow(train_data)), train_idx)
  train_subset <- train_data[train_idx, ]
  val_subset <- train_data[val_idx, ]
  
  # Preprocess: Remove the "id" column from training, validation, and testing datasets if present
  if ("id" %in% names(train_subset)) {
    train_subset <- train_subset %>% select(-id)
  }
  if ("id" %in% names(val_subset)) {
    val_subset <- val_subset %>% select(-id)
  }
  if ("id" %in% names(test_data)) {
    test_data <- test_data %>% select(-id)
  }
  
  # Feature and Label Extraction:
  # Separate features and labels for training, validation, and test datasets
  X_train <- train_subset %>% select(-event)
  y_train <- train_subset$event
  X_val <- val_subset %>% select(-event)
  y_val <- val_subset$event
  X_test <- test_data %>% select(-event)
  y_test <- test_data$event
  
  # Define a helper function to convert features to numeric and handle missing values
  convert_to_numeric <- function(x) {
    if (is.numeric(x)) {
      res <- x
    } 
    else if (is.factor(x)) {
      res <- suppressWarnings(as.numeric(as.character(x)))
    } 
    else if (is.character(x)) {
      res <- suppressWarnings(as.numeric(x))
    } 
    else {
      res <- suppressWarnings(as.numeric(x))
    }
    if (any(is.na(res))) {
      med <- median(res, na.rm = TRUE)
      if (is.na(med)) med <- 0 # Default to 0 if median cannot be computed
      res[is.na(res)] <- med
    }
    return(res)
  }
  
  # Convert all feature columns in training, validation, and test sets to numeric
  X_train <- X_train %>% mutate(across(everything(), convert_to_numeric))
  X_val <- X_val %>% mutate(across(everything(), convert_to_numeric))
  X_test <- X_test %>% mutate(across(everything(), convert_to_numeric))
  
  # Convert the data frames of features to matrices
  X_train <- as.matrix(X_train)
  X_val <- as.matrix(X_val)
  X_test <- as.matrix(X_test)
  
  # Convert labels to binary numeric values (0/1)
  y_train <- as.numeric(y_train) - 1
  y_val <- as.numeric(y_val) - 1
  y_test <- as.numeric(y_test) - 1
  
  # Prepare Data for Python:
  # Create a list containing the training features, labels, validation data, and model parameters, 
  # then convert it to a Python object.
  train_data_py <- r_to_py(list(
    X = X_train,
    y = y_train,
    val_X = X_val,
    val_y = y_val,
    epochs = as.integer(epochs),
    num_nodes = num_nodes,
    dropout = dropout,
    batch_size = as.integer(batch_size),
    lr = lr
  ))
  
  # Prepare the test data in a similar fashion
  test_data_py <- r_to_py(list(
    X = X_test,
    y = y_test
  ))
  
  # Convert class_weight to a Python dictionary if provided
  class_weight_py <- NULL
  if (!is.null(class_weight)) {
    class_weight_py <- r_to_py(as.list(class_weight))
  }
  
  # Load the Python script that implements the Transformation Survival model
  source_python("transformation_surv_model.py")
  
  # Train the Transformation Survival model using the provided training data
  # Pass class_weight_py to handle class imbalance in Python
  trans_model <- train_transformation_surv(train_data_py, class_weight = class_weight_py)
  
  # Use the trained model to predict outcomes on the test dataset
  predictions <- trans_model$predict(test_data_py[["X"]])
  
  # Convert predictions from Python format back to R
  predictions <- py_to_r(predictions)
  
  # Create a confusion matrix comparing predicted outcomes with the actual test labels
  test_conf_matrix <- table(
    Predicted = factor(predictions, levels = c(0, 1)),
    Actual = factor(y_test, levels = c(0, 1))
  )
  
  # Calculate performance metrics using the confusion matrix
  # This function (calculate_model_metrics) is assumed to be defined elsewhere
  metrics_tfs <- calculate_model_metrics(test_conf_matrix, predictions, "TransformationSurv")
  
  # Convert the metrics into a data frame for easy viewing and further analysis
  metrics_tfs_dataframe <- get_dataframe("TransformationSurv", metrics_tfs)
  
  # Return a list containing both the metrics data frame and the raw metrics
  return(list(metrics_tfs_dataframe = metrics_tfs_dataframe, metrics_tfs = metrics_tfs))
}
```

### transformation_surv_model.py

The **Transformation-Surv** model implemented in this code is a deep learning-based survival analysis framework that incorporates a **logarithmic transformation layer** to model risk transformations effectively. The model is designed for binary classification in survival analysis and employs **deep neural networks with regularization techniques** to enhance stability and performance. The key component of this architecture is the `transformation_layer`, which applies a **logarithmic transformation** to the input features to simulate risk transformation while ensuring numerical stability by clipping values to prevent invalid logarithmic operations. The `TransformationSurvModel` class defines the neural network structure, which consists of an **input layer**, **two fully connected hidden layers** with `ReLU` activation, **batch normalization layers** to stabilize training, **dropout layers** to prevent overfitting, and a **custom transformation layer** before the final **softmax classification layer**. The model is compiled using the **Adam optimizer** with a learning rate of `0.0005` and **gradient clipping (`clipnorm=1.0`)** to improve training stability. The `fit` method incorporates **early stopping** and **adaptive learning rate reduction** to prevent overfitting and ensure optimal convergence. The `train_transformation_surv` function acts as a wrapper to facilitate data preprocessing, feature conversion, and model training. It ensures that input data is properly formatted, applies **one-hot encoding to labels**, and checks for **validation datasets**. If a validation set is provided, the training process includes validation monitoring and class weighting to address **imbalanced datasets**. Finally, the trained model is returned for further prediction tasks, allowing users to evaluate its performance on new test data.

```{python}
import numpy as np # Import Numpy For Numerical Operations
import tensorflow.keras.backend as K # Import Keras Backend For Tensor Operations
from tensorflow.keras.models import Sequential # Import Sequential Model Constructor
# Added BatchNormalization for stability
from tensorflow.keras.layers import Dense, Dropout, Input, Lambda, BatchNormalization
from tensorflow.keras.optimizers import Adam # Import Adam Optimizer For Model Training
from tensorflow.keras.utils import to_categorical # Import Utility To Convert Labels To One-Hot Encoding
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # Import Callbacks For Training Control

# Define A Transformation Layer Function That Applies A Logarithmic Transformation To The Input
def transformation_layer(x):
    """
    Applies a logarithmic transformation to simulate the transformation of the risk function
    """
    # Clip input to avoid log(0) or log(negative) issues
    clipped_input = K.clip(1.0 + x, K.epsilon(), None)
    return K.log(clipped_input) # Return The Logarithm Of One Plus The Input

# Define A Class For The Transformation-Surv Model Architecture And Training Procedures
class TransformationSurvModel:
    # Initialize The Model With Input Dimension And Number Of Hidden Units
    def __init__(self, input_dim, hidden_units=64):
        """
        Initializes a deep neural network model for binary classification using the Transformation-Surv approach
        This model includes a transformation layer to simulate risk transformation
        
        Parameters:
        - input_dim: Number of input features
        - hidden_units: Number of neurons in hidden layers
        """
        # Build A Sequential Model With An Input Layer, Dense Layers, BatchNormalization, Dropout For
        # Regularization, A Transformation Lambda Layer, And An Output Layer
        self.model = Sequential([
            Input(shape=(input_dim,)), # Define The Input Layer With The Specified Input Dimension
            Dense(hidden_units, activation='relu'), # Add A Dense Layer With ReLU Activation
            BatchNormalization(), # Added BatchNormalization for stability
            Dropout(0.2), # Adjusted dropout rate for stability
            Dense(hidden_units, activation='relu'), # Add A Second Dense Layer With ReLU Activation
            BatchNormalization(), # Added BatchNormalization for stability
            Lambda(transformation_layer), # Apply The Custom Transformation Layer To Simulate Risk Transformation
            Dense(2, activation='softmax') # Add An Output Layer For Binary Classification With Softmax
        ])
        # Compile The Model With Adam Optimizer, Categorical Crossentropy Loss, And Accuracy Metric
        # Set learning rate to 0.0005 and added clipnorm for gradient clipping
        self.model.compile(optimizer=Adam(learning_rate=0.0005, clipnorm=1.0),
                           loss='categorical_crossentropy',
                           metrics=['accuracy'])
    
    # Define A Method To Train The Model Using Provided Data And Callbacks For Early Stopping And 
    # Learning Rate Reduction
    def fit(self, X, y, epochs=10, batch_size=32, validation_data=None, class_weight=None):
        """
        Trains the Transformation-Surv model using early stopping and learning rate reduction
        
        Parameters:
        - X: Training feature matrix
        - y: One-hot encoded labels
        - epochs: Maximum number of epochs
        - batch_size: Batch size
        - validation_data: Optional tuple (val_X, val_y) for validation
        - class_weight: Optional dictionary for imbalanced data, e.g. {0: 1.0, 1: 5.0}
        """
        # Define Callbacks To Monitor Loss And Adjust Training Accordingly
        # If validation data exists, monitor 'val_loss', otherwise 'loss'
        monitor_metric = 'val_loss' if validation_data is not None else 'loss'
        callbacks = [
            EarlyStopping(monitor=monitor_metric, patience=3, restore_best_weights=True),
            ReduceLROnPlateau(monitor=monitor_metric, factor=0.5, patience=2, min_lr=1e-6)
        ]
        
        # Pass class_weight to model.fit to deal with imbalance
        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0,
                       validation_data=validation_data, callbacks=callbacks,
                       class_weight=class_weight)
    
    # Define A Method To Predict Class Labels From Input Data
    def predict(self, X):
        """
        Predicts class labels for the given input
        
        Parameters:
        - X: Input feature matrix
        
        Returns:
        - Predicted class labels (0 or 1)
        """
        # Generate Predictions Using The Model And Select The Class With The Highest Probability For Each Sample
        preds = self.model.predict(X)
        return np.argmax(preds, axis=1)

# Define A Function To Train A Transformation-Surv Model Using Provided Training Data
def train_transformation_surv(train_data, class_weight=None):
    """
    Trains a Transformation-Surv model using the provided training data
    
    Parameters:
    - train_data: A dictionary with keys:
         "X": Features
         "y": Labels (assumed to be 0/1)
         Optional keys "val_X" and "val_y" for validation data
    - class_weight: Optional dictionary to handle class imbalance
    
    Returns:
    - A trained TransformationSurvModel instance
    """
    # Convert The Training Features And Labels To Numpy Arrays
    X = np.array(train_data["X"])
    y = np.array(train_data["y"])
    # Reshape X To Two Dimensions If It Is A One-Dimensional Array
    if (X.ndim == 1):
        X = X.reshape(-1, 1)
    # Raise An Error If There Are No Features In X
    if (X.shape[1] == 0):
        raise ValueError("Invalid input: X has no features")
    # Determine The Number Of Input Features From The Shape Of X
    input_dim = X.shape[1]
    # Convert The Labels To A One-Hot Encoded Format For Binary Classification
    y_onehot = to_categorical(y, num_classes=2)
    
    # Initialize The TransformationSurvModel With The Determined Input Dimension And Fixed Number Of Hidden Units
    model = TransformationSurvModel(input_dim=input_dim, hidden_units=64)
    
    # Check If Validation Data Is Provided In The Training Data Dictionary
    if ("val_X" in train_data and "val_y" in train_data):
        val_X = np.array(train_data["val_X"])
        val_y = np.array(train_data["val_y"])
        if (val_X.ndim == 1):
            val_X = val_X.reshape(-1, 1)
        val_y_onehot = to_categorical(val_y, num_classes=2)
        
        # Fit The Model Using Both Training And Validation Data
        # Pass class_weight for imbalance
        model.fit(X, y_onehot, epochs=10, batch_size=32, 
                  validation_data=(val_X, val_y_onehot), 
                  class_weight=class_weight)
    else:
        # Fit The Model Using Only The Training Data If Validation Data Is Not Provided
        model.fit(X, y_onehot, epochs=10, batch_size=32, class_weight=class_weight)
    
    # Return The Trained TransformationSurvModel Instance
    return model
```
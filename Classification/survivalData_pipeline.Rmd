---
title: "DMLR DeFi Survival Data Pipeline"
author: "Hanzhen Qin(qinh2)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: true
    number_sections: true
    df_print: paged
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
# Check and install required R packages
if (!require("conflicted")) {
  install.packages("conflicted", dependencies = TRUE)
  library(conflicted)
}

# Set default CRAN repository
local({
  r <- getOption("repos")
  r["CRAN"] <- "http://cran.r-project.org"
  options(repos = r)
})

# Define the list of required packages
required_packages <- c(
  "rmarkdown", "tidyverse", "stringr", "ggbiplot", "pheatmap", 
  "caret", "survival", "survminer", "ggplot2", 
  "kableExtra", "rpart", "glmnet", "data.table", "reshape2", "pROC", 
  "pander", "readr", "dplyr", "e1071", "ROSE", "xgboost", "parallel"
)

# Loop through the package list and install missing packages
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}

# Handle function name conflicts
conflict_prefer("slice", "dplyr")
conflict_prefer("filter", "dplyr")

# Set knitr options for R Markdown
knitr::opts_chunk$set(echo = TRUE)

# Rename dplyr functions to avoid conflicts with other packages
select <- dplyr::select
rename <- dplyr::rename
summarize <- dplyr::summarize
group_by <- dplyr::group_by
```

# Survival Data Pipeline

## Pipeline Summary

* Summary of work
  
  * Data preprocessing: `data_processing`, `get_train_test_data`, `smote_data`, 
  `get_classification_cutoff`
  
  * Model Performance Evaluation and Visualization : `calculate_model_metrics`, `get_dataframe`,
  `combine_classification_results`, `accuracy_comparison_plot`, `get_percentage`, 
  `specific_accuracy_statistics`, `combine_accuracy_dataframes`
  
  * Classification model: `logistic_regression`, `decision_tree`, `naive_bayes`, `XG_Boost`, 
  `elastic_net`, `logistic_regression_op`, `decision_tree_op`, `naive_bayes_op`, `elastic_net_op`
  

## Data Preprocessing

### def get_train_test_data

The `get_train_test_data` function is designed to get the training and test data based on the `indexEvent` and `outcomeEvent` parameters, but the specific logic has not yet been implemented. The function references the external script `dataLoader.r` through `source()` to load the latest processed survival data, and returns the train and test data sets directly by default.

```{r}
get_train_test_data <- function(indexEvent, outcomeEvent) {
  source("~/KDD_DeFi_Survival_Dataset_And_Benchmark/dataLoader.R")
}
```

### def data_processing

The `data_processing` function is designed to transform a survival analysis dataset into a format suitable for classification tasks by performing a series of preprocessing steps. It begins by filtering out invalid records where `timeDiff` is less than or equal to 0, ensuring only valid observations are retained. Then, it removes records where `timeDiff` falls within a specified threshold (`set_timeDiff`) but no event occurred (`status == 0`). A new binary column, `event`, is created to indicate whether an event occurred within the threshold ("yes" or "no"). The function drops unnecessary columns as defined in a predefined list, ensuring only relevant features remain, and removes columns entirely populated with missing values. To handle missing data, numeric columns are filled with `-999` to mark missing values clearly without distorting numericul distributions, and categorical columns are filled with `"missing"` to create a distinct category for absent data. Finally, character columns are converted to factors, making them ready for classification models.

```{r}
data_processing <- function(survivalData, set_timeDiff) {
  # filter out invalid records where `timeDiff` is <= 0 early
  survivalData <- survivalData %>% filter(timeDiff > 0)
  
  # filter out records based on the `set_timeDiff` threshold and `status`
  survivalData <- survivalData %>% filter(!(timeDiff / 86400 <= set_timeDiff & status == 0))
  
  # create a new binary column `event` based on `timeDiff`
  survivalDataForClassification <- survivalData %>%
    mutate(event = case_when(
      timeDiff / 86400 <= set_timeDiff ~ "yes",
      timeDiff / 86400 > set_timeDiff ~ "no"
    ))
  
  # define features to drop
  # featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
  #                     "timeDiff", "deployment", "version", "indexID", "user", "status", 
  #                     "quarter", "liquidator", "userFlashloanAvgAmount", "userReserveMode",
  #                     "reserve", "pool", "timestamp", "type", "datetime", "quarter_start_date",
  #                     "userCoinTypeMode", "coinType", "userIsNew", "userDepositSum", 
  #                     "userDepositSumUSD", "userDepositAvgAmountUSD", "userDepositSumETH",
  #                     "userDepositAvgAmountETH", "userWithdrawSum", "userWithdrawSumUSD",
  #                     "userWithdrawAvgAmountUSD", "userWithdrawSumETH", 
  #                     "userWithdrawAvgAmountETH", "userBorrowSum", "userBorrowSumUSD", 
  #                     "userBorrowAvgAmountUSD", "userBorrowSumETH", 
  #                     "userBorrowAvgAmountETH", "userRepaySum", "userRepaySumUSD", 
  #                     "userRepayAvgAmountUSD", "userRepaySumETH", "userRepayAvgAmountETH", 
  #                     "userFlashloanSum", "userLiquidationSum", "userLiquidationSumUSD",
  #                     "userLiquidationAvgAmountUSD", "userLiquidationSumETH", 
  #                     "userLiquidationAvgAmountETH", "userFlashloanCount", "priceInUSD")
  
  featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
                      "timeDiff", "status", "deployment", "version", "indexID", "user", 
                      "liquidator", "pool", "timestamp", "type", "datetime", "quarter_start_date")
  
  # remove only columns that actually exist in the dataset
  featuresToDrop <- intersect(featuresToDrop, colnames(survivalDataForClassification))
  
  survivalDataForClassification <- survivalDataForClassification %>%
    # drop unnecessary columns
    select(-any_of(featuresToDrop)) %>%
    # remove columns with only NA values
    select(where(~ !all(is.na(.)))) %>%
    # replace NA in numeric columns with -999
    mutate(across(where(is.numeric), ~ replace_na(., -999))) %>%
    # replace NA in character columns with "missing"
    mutate(across(where(is.character), ~ replace_na(., "missing"))) %>%
    # convert character columns to factors
    mutate(across(where(is.character), as.factor))
  
  # return the processed dataset
  return(survivalDataForClassification)
}
```

### def get_classification_cutoff (@Author: Aaron Green)

The `get_classification_cutoff` function retrieves the `ConvergedRMST_5` value from a predefined dataset based on the specified `indexEvent` and `outcomeEvent`. The dataset contains information about various index events (e.g., "Borrow", "Deposit") and their corresponding outcome events (e.g., "Full Repay", "Withdraw"), along with convergence metrics like `ConvergedTau` and `ConvergedRMST` at different time horizons. The function filters the dataset by matching the input events with the respective `IndexEvent` and `OutcomeEvent` columns and returns the relevant `ConvergedRMST_5` value, which represents a specific metric of convergence over a 5-unit time period. Return the optimal cutoff value for the `data_processing` function.

```{r}
get_classification_cutoff <- function(indexEvent, outcomeEvent){
  # library(stringr)
  # Create the dataframe
  data <- data.frame(
    IndexEvent = c("Borrow", "Borrow", "Borrow", "Borrow", "Borrow", "Borrow", 
                   "Deposit", "Deposit", "Deposit", "Deposit", "Deposit",
                   "Repay", "Repay", "Repay", "Repay", "Repay",
                   "Withdraw", "Withdraw", "Withdraw", "Withdraw", "Withdraw"),
    OutcomeEvent = c("Account Liquidated", "Deposit", "Full Repay", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Repay"),
    ConvergedTau_1 = c(91, 99, 95, 101, 69, 99, 
                       90, 100, 101, 99, 93, 
                       93, 95, 100, 101, 100, 
                       94, 101, 99, 101, 100),
    ConvergedRMST_1 = c(69.36, 90.98, 74.12, 100.88, 28.71, 92.59, 
                        68.64, 91.58, 100.95, 89.70, 64.84, 
                        72.56, 63.23, 93.72, 100.86, 93.93,
                        75.10, 96.01, 82.13, 100.88, 93.82),
    ConvergedTau_5 = c(20, 21, 20, 21, 17, 21, 
                       20, 21, 21, 21, 20, 
                       20, 20, 21, 21, 21,
                       20, 21, 21, 21, 21),
    ConvergedRMST_5 = c(17.43, 19.88, 17.15, 20.99, 10.24, 20.21, 
                        17.52, 19.73, 20.99, 19.69, 15.67, 
                        17.43, 14.99, 20.13, 20.99, 20.15,
                        17.72, 20.26, 18.14, 20.98, 20.04),
    ConvergedTau_10 = c(11, 11, 11, 11, 10, 11, 
                        11, 11, 11, 11, 11, 
                        11, 11, 11, 11, 11,
                        11, 11, 11, 11, 11),
    ConvergedRMST_10 = c(9.94, 10.51, 9.72, 11.00, 6.66, 10.68, 
                         10.00, 10.43, 11.00, 10.44, 8.95, 
                         9.90, 8.64, 10.62, 11.00, 10.62,
                         10.05, 10.68, 9.68, 10.99, 10.57)
  )
  
  
  return(data %>% filter(IndexEvent == str_to_title(indexEvent), OutcomeEvent == str_to_title(outcomeEvent)) 
         %>% pull(ConvergedRMST_5))
}
```

### def smote_data

The `smote_data` function leverages the `ROSE` package to address class imbalance in datasets by dynamically generating a balanced dataset based on oversampling and undersampling techniques. The function is designed to be flexible, allowing the user to specify the target variable (`target_var`) and an optional random seed (`seed`) for reproducibility. It validates the input dataset to ensure the target variable exists and dynamically constructs the formula for the `ROSE` function, making it adaptable to different datasets and classification tasks. By generating a balanced dataset where the minority and majority classes are better represented, this function helps mitigate bias in machine learning models and improves their classification accuracy. 

```{r}
smote_data <- function(train_data, target_var = "event", seed = 123) {
  # library(ROSE)
  # check if the input data contains the target variable
  if (!target_var %in% colnames(train_data)) {
    stop(paste("Target variable", target_var, "not found in the dataset"))
  }
  
  # set the random seed (if provided)
  if (!is.null(seed)) {
    set.seed(seed)
  }
  
  # dynamic formula creation to adapt to different target variables
  formula <- as.formula(paste(target_var, "~ ."))
  
  # applying ROSE Balance Data
  train_data_balanced <- ROSE(formula, data = train_data, seed = seed)$data
  
  # return the balanced dataset
  return(train_data_balanced)
}
```

## Model Performance Evaluation and Visualization

### def calculate_model_metrics

The `calculate_model_metrics` function evaluates a classification model's performance based on a given confusion matrix by computing key metrics, including **class accuracy (specificity)**, **negative-1 accuracy (sensitivity/recall)**, **balanced accuracy**, **precision**, and the **F1-score**. It extracts True Negatives (TN), False Positives (FP), False Negatives (FN), and True Positives (TP) from the matrix and calculates specificity as \( TN / (TN + FP) \) and sensitivity as \( TP / (TP + FN) \), then derives balanced accuracy as their average. Precision is determined as \( TP / (TP + FP) \), and the F1-score is computed as the harmonic mean of precision and recall. The function prints the balanced accuracy and F1-score in percentage format before returning them in a list.

```{r}
calculate_model_metrics <- function(confusion_matrix, binary_predictions, model_name) {
  TN <- confusion_matrix[1, 1] # True Negatives
  FP <- confusion_matrix[1, 2] # False Positives
  FN <- confusion_matrix[2, 1] # False Negatives
  TP <- confusion_matrix[2, 2] # True Positives
  
  # positive Class accuracy (Specificity): TN / (TN + FP)
  class_accuracy <- TN / (TN + FP)
  
  # negative 1 accuracy (Sensitivity/Recall): TP / (TP + FN)
  negative_1_accuracy <- TP / (TP + FN)
  
  # balanced accuracy: Average of Sensitivity and Specificity
  balanced_accuracy <- (class_accuracy + negative_1_accuracy) / 2
  
  # precision
  precision <- TP / (TP + FP)
  
  # f1 score
  f1_score <- 2 * (precision * negative_1_accuracy) / (precision + negative_1_accuracy)
  
  # print out all the accuracy records
  print(paste(model_name, "model prediction accuracy:"))
  cat("Balanced accuracy:", sprintf("%.2f%%", balanced_accuracy * 100), "\n")
  cat("F1 score:", sprintf("%.2f%%", f1_score * 100), "\n")
  
  return (list(balanced_accuracy = balanced_accuracy, 
               f1_score = f1_score))
}
```

### def get_dataframe

The `get_dataframe` function creates a formatted data frame containing accuracy metrics for a given model. It takes two inputs: `model_name`, a string representing the model's name, and `metrics`, a list containing the model's accuracy metrics. The function constructs a data frame with three columns: "Model," "Balanced_Accuracy," and "F1_Score." The accuracy values are converted to percentages with two decimal places using `sprintf("%.2f%%", value * 100)`. Finally, the formatted data frame is returned.

```{r}
get_dataframe <- function(model_name, metrics) {
  metrics_dataframe <- data.frame(
    Model = model_name, 
    Balanced_Accuracy = sprintf("%.2f%%", metrics$balanced_accuracy * 100), 
    F1_Score = sprintf("%.2f%%", metrics$f1_score * 100)
  )
  return (metrics_dataframe)
}
```

### def combine_classification_results

The `combine_classification_results` function is designed to unify performance metrics from multiple classification models into a single, cohesive dataframe. It accepts two parameters: a list of dataframes (`accuracy_dataframe_list`), each containing accuracy metrics for a different model, and a string (`data_combination`) describing the dataset combination (e.g., "Withdraw + Deposit"). The function uses `lapply` to iterate over each dataframe in the list, adding a `Data_Combination` column that records the dataset combination description for that model's metrics. Once each dataframe is labeled, `do.call(rbind, accuracy_dataframe_list)` concatenates the dataframes by rows, resulting in a single combined dataframe with all models’ metrics and an additional column indicating the dataset combination.

```{r}
combine_classification_results <- function(accuracy_dataframe_list, data_combination) {
  # apply the data combination description to each dataframe in the list
  accuracy_dataframe_list <- lapply(accuracy_dataframe_list, function(df) {
    # add a new column `Data_Combination` to store the combination description
    # this allows each dataframe to retain information about the specific data combination it
    # represents
    df$Data_Combination <- data_combination
    # return the modified dataframe with the new column added
    return(df)
  })
  
  # combine all the modified dataframes into one large dataframe
  # `do.call` applies `rbind` to all dataframes in the list, effectively stacking them by rows
  combined_dataframe <- do.call(rbind, accuracy_dataframe_list)
  
  # return the combined dataframe
  return(combined_dataframe)
}
```

### def get_percentage

The `get_percentage` function takes a dataset (`survivalDataForClassification`), along with two event labels (`indexEvent` and `outcomeEvent`), and calculates the percentage distribution of different event types within the dataset. It groups the data by the event variable, counts occurrences for each event type, and then calculates the total and corresponding percentage for each event. The function then creates a bar plot to visually display the percentage of each event type, labeling the bars with the percentage values and showing the y-axis as a percentage. The plot is titled according to the provided `indexEvent` and `outcomeEvent` labels.

```{r}
get_percentage <- function(survivalDataForClassification, indexEvent, outcomeEvent) {
  # indexEvent and outcomeEvent is a string type
  pctPerEvent <- survivalDataForClassification %>%
  group_by(event) %>%
  dplyr::summarize(numPerEvent = n()) %>%
  mutate(total = sum(numPerEvent)) %>%
  mutate(percentage = numPerEvent / total) %>%
  dplyr::select(event, percentage)
  # create a bar plot for event percentages
  # stat = "identity": percentages used directly to draw the bar chart
  print(ggplot(pctPerEvent, aes(x = event, y = percentage, fill = event)) +
    geom_bar(stat = "identity") +
    scale_y_continuous(labels = scales::percent_format()) +  # show y-axis in percentage
    labs(title = "Percentage of Events: 'Yes' event vs 'No' event",
         x = paste(indexEvent, "and", outcomeEvent),
         y = "Percentage") +
    geom_text(aes(label = scales::percent(percentage)), 
              vjust = -0.5, size = 3.5) +  # show percentages on top of bars
    theme_minimal())
}
```

### def accuracy_comparison_plot

The `accuracy_comparison_plot` function visualizes the accuracy performance of different models by creating a faceted bar plot. It takes `metrics_list` as input, where each element consists of a model’s accuracy metrics and its corresponding name. The function first initializes an empty data frame and iterates through `metrics_list`, extracting `balanced_accuracy` and `f1_score` for each model, and appending them to the data frame. The data is then transformed into a long format using `reshape2::melt()` to facilitate plotting. A `ggplot2` bar chart is generated, where models are represented on the x-axis and accuracy values on the y-axis. The plot is faceted by metric type, ensuring a separate visualization for `balanced_accuracy` and `f1_score`. Labels displaying percentage values are added on top of each bar for clarity. The final visualization is styled using a minimal theme, with x-axis text rotated for better readability.

```{r}
accuracy_comparison_plot <- function(metrics_list) {
  # initialize an empty data frame to store the metrics for all models
  accuracy_table <- data.frame()
  
  # loop over each element in metrics_list (each element is a list containing metrics and model name)
  for (metrics in metrics_list) {
    # Extract metrics and model name from each "tuple"
    model_metrics <- metrics[[1]]
    model_name <- metrics[[2]]
    
    # create a temporary dataframe for this model
    temp_df <- data.frame(
      Model = model_name, 
      BalancedA = model_metrics$balanced_accuracy, 
      F1_score = model_metrics$f1_score
    )
    
    # append the temporary dataframe to the main accuracy_table
    accuracy_table <- rbind(accuracy_table, temp_df)
  }
  
  # melt the dataframe into long format for plotting
  accuracy_results_melted <- reshape2::melt(accuracy_table, id.vars = "Model")
  
  # generate the plot with faceted bars
  ggplot(accuracy_results_melted, aes(x = Model, y = value, fill = Model)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ variable, scales = "free_y") +  # Facet by each metric
    labs(title = "Comparison of Accuracy Metrics Across Models",
         x = "Model",
         y = "Value") +
    # add percentage labels on top of each bar
    geom_text(aes(label = scales::percent(value, accuracy = 0.1)),
              position = position_dodge(width = 0.9),
              vjust = 0.5, size = 2.0) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

### def specific_accuracy_statistics

The `specific_accuracy_statistics` function calculates and organizes accuracy-related metrics for different models based on a given accuracy type. It takes three arguments: `event_pair`, which represents a specific event combination, `accuracy_type`, which specifies the type of accuracy metric to extract (either "balanced_accuracy" or "f1_score"), and `metrics_list`, a list containing accuracy data and corresponding model names. The function initializes a result list, placing `accuracy_type` in the top-left corner and `event_pair` as the first column. It then iterates through `metrics_list`, extracting and rounding the selected accuracy metric for each model. If an invalid `accuracy_type` is provided, it raises an error. Finally, the results are converted into a DataFrame and returned with row names set to `NULL`.

```{r}
specific_accuracy_statistics <- function(event_pair, accuracy_type, metrics_list) {
  # initialize the result list
  results <- list()
  
  # add accuracy_type as the left front corner and event_pair is the first column of the data
  results[[accuracy_type]] <- event_pair
  
  # traverse metrics_list and extract the specified accuracy type for each model
  for (metric_item in metrics_list) {
    # get accuracy data
    accuracy_data <- metric_item[[1]]
    # get the model name
    model_name <- metric_item[[2]]
    if (accuracy_type == "balanced_accuracy") {
      results[[model_name]] <- round(accuracy_data$balanced_accuracy * 100, 1)
    }
    else if (accuracy_type == "f1_score") {
      results[[model_name]] <- round(accuracy_data$f1_score * 100, 1)
    }
    else {
      # An error message is displayed if the specified accuracy_type does not exist.
      stop(paste("Invalid accuracy type:", accuracy_type))
    }
  }
  
  # convert the result to a DataFrame and set row.names = NULL
  df <- as.data.frame(results, row.names = NULL)
  return(df)
}
```

### def combine_accuracy_dataframes

The `combine_accuracy_dataframes` function takes a list of data frames as input and combines them into a single data frame by stacking the rows. It first checks if the input is a valid list and throws an error if not. Then, using `do.call` and `rbind`, it merges all the data frames in the list row-wise. The function is designed to handle multiple data frames efficiently and returns a consolidated data frame for further analysis.

```{r}
combine_accuracy_dataframes <- function(df_list) {
  # check if the input is a list
  if (!is.list(df_list)) {
    stop("Input must be a list of data.frames.")
  }
  
  # Use do.call and rbind to combine all data.frames in a list.
  combined_df <- do.call(rbind, df_list)
  
  # returns the merged data.frame
  return(combined_df)
}
```

## Classification Model Development and Optimization

### def logistic_regression

This **logistic regression model** applies **Lasso regularization (L1 penalty)** for **binary classification** using the `glmnet` package. The model follows a structured workflow, starting with **data preprocessing**, where numeric features in the `train_set`, `validation_set`, and `test_data` are **standardized** (mean = 0, standard deviation = 1) to ensure stable optimization. The categorical variables are handled automatically by converting the dataset into **matrix format** using `model.matrix()`, which excludes the intercept and creates a design matrix suitable for `glmnet`. To **train the logistic regression model**, **Lasso regularization** (`alpha = 1`) is applied, which helps in **feature selection by shrinking less important coefficients to zero**, improving interpretability and reducing overfitting. The model first undergoes **cross-validation (`cv.glmnet()`)**, which automatically determines a range of `lambda` values (the regularization parameter) and selects the `lambda.min` that minimizes the cross-validation error. This approach helps **balance model complexity and generalization**, preventing overfitting while ensuring predictive performance. After selecting the **best lambda**, the model predicts **event probabilities** on both the `validation_set` and `test_data`, applying a **0.5 threshold** to classify instances as `"yes"` or `"no"`. Performance evaluation is conducted using **confusion matrices** and key classification metrics computed through the `calculate_model_metrics()` function.

```{r}
logistic_regression <- function(train_data, test_data) {
  # library(glmnet)  # load glmnet package for logistic regression with regularization
  # library(data.table)  # load data.table for efficient data handling
  # ensure train_data and test_data are in the data.table format for fast operations
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  set.seed(123)  # ensure reproducibility
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset, and scale them to have mean = 0 and standard deviation = 1
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Scale numeric columns in train set
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # Scale numeric columns in validation set
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # Scale numeric columns in test set
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert the training, validation, and testing datasets to matrix format as required by the glmnet package
  # model matrix function excludes the intercept (-1) and converts data for glmnet
  x_train <- model.matrix(event ~ . - 1, data = train_set)  
  y_train <- train_set$event  # extract the target variable from the training data
  
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)  
  x_test <- model.matrix(event ~ . - 1, data = test_data)  
  
  # Apply logistic regression with Lasso regularization (alpha = 1 means Lasso)
  # 'family = binomial' specifies logistic regression for binary classification
  logistic_regression_classifier <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
  
  # Use validation set to select the best lambda value
  lambda_values <- logistic_regression_classifier$lambda  # Get available lambda values
  cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)  # Cross-validation
  best_lambda <- cv_model$lambda.min  # Select the best lambda based on cross-validation
  
  # Predict the probability of the event (outcome) on the validation set
  predict_probabilities_val <- predict(logistic_regression_classifier, s = best_lambda, newx = x_validation, type = "response")
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation set performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, "Logistic Regression (Validation)")
  
  # Predict the probability of the event (outcome) on the test set using the best lambda
  predict_probabilities_test <- predict(logistic_regression_classifier, s = best_lambda, newx = x_test, type = "response")
  binary_prediction_test <- ifelse(predict_probabilities_test > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  test_conf_matrix <- table(Predicted = binary_prediction_test, Actual = test_data$event)
  
  # Evaluate model performance by calculating metrics such as accuracy, precision, recall, etc.
  metrics_lr <- calculate_model_metrics(test_conf_matrix, predict_probabilities_test, "Logistic Regression")
  
  # Create a dataframe with the desired structure
  metrics_lr_dataframe = get_dataframe("Logistic Regression", metrics_lr)
  return (list(metrics_lr_dataframe = metrics_lr_dataframe, metrics_lr = metrics_lr))
}
```

### def decision_tree

This **decision tree model** is designed for **binary classification**, using the `rpart` package to construct and optimize a tree-based classifier. The model starts with **data partitioning**, where the training data is split into **80% training set and 20% validation set** to allow hyperparameter tuning. The decision tree is then trained on the `train_set` using the **CART (Classification and Regression Trees) algorithm**, with the **"class"** method to handle categorical target variables.Key hyperparameters are set through `rpart.control()`, including: **`cp = 0.01`**: The complexity parameter, which controls **pruning** by preventing overly complex trees. **`maxdepth = 30`**: The maximum depth of the tree, balancing complexity and generalization. **`minsplit = 20`**: The minimum number of observations required to split a node, ensuring stability in decision-making. After training, the model **predicts on the validation set**, and its performance is evaluated using a **confusion matrix** and classification metrics (`calculate_model_metrics()`). The final decision tree is then applied to the **test dataset**, producing **predicted labels**, which are compared against actual outcomes to compute key performance metrics.

```{r}
decision_tree <- function(train_data, test_data) {
  # library(rpart)
  # library(caret) # Load caret for data partitioning
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Train the decision tree model with hyperparameter tuning
  decision_tree_classifier <- rpart(
    event ~ .,
    data = train_set,
    method = "class",
    control = rpart.control(
      # Complexity parameter for pruning
      cp = 0.01,
      # Maximum depth of the tree
      maxdepth = 30,
      # Minimum number of observations needed to split a node
      minsplit = 20
    )
  )
  # Predict on the validation dataset
  predict_probabilities_val <- predict(decision_tree_classifier, validation_set, type = "class")
  validation_conf_matrix <- table(Predicted = predict_probabilities_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Decision Tree (Validation)")
  
  # Predict on the testing dataset
  predict_probabilities_dt <- predict(decision_tree_classifier, test_data, type = "class")
  test_conf_matrix <- table(Predicted = predict_probabilities_dt, Actual = test_data$event)
  
  # Evaluate model performance
  metrics_dt <- calculate_model_metrics(test_conf_matrix, predict_probabilities_dt, "Decision Tree")
  
  # Create a dataframe with the desired structure
  metrics_dt_dataframe = get_dataframe("Decision Tree", metrics_dt)
  return (list(metrics_dt_dataframe = metrics_dt_dataframe, metrics_dt = metrics_dt))
}
```

### def naive_bayes

This **Naive Bayes model** is designed for **binary classification**, leveraging the `e1071` package to implement a probabilistic classifier based on **Bayes' theorem** with the assumption of **feature independence**. The model begins with **data preprocessing**, where the target variable `event` is explicitly converted into a **factor** to ensure compatibility with Naive Bayes. The dataset is then **split into an 80% training set and a 20% validation set**, allowing for hyperparameter tuning and performance evaluation. Feature matrices for **validation and test sets** are prepared by removing the target column to ensure that the model only relies on predictors for classification. During training, the **Naive Bayes classifier** is built using the `naiveBayes()` function, which computes **prior probabilities** for each class and estimates conditional probabilities for each feature. Predictions are then made on the **validation set**, with both **class labels** and **probability scores** being generated. The model's performance is evaluated using a **confusion matrix** and various classification metrics, calculated through the `calculate_model_metrics()` function.

```{r}
naive_bayes <- function(train_data, test_data) {
  # library(e1071)
  # library(caret) # Load caret for data partitioning
  
  target_column = "event"
  
  # Convert the target column to a factor if it's not already
  train_data[[target_column]] <- as.factor(train_data[[target_column]])
  test_labels <- as.factor(test_data[[target_column]])
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data[[target_column]], p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Remove the target column from the validation and test sets for prediction
  validation_features <- validation_set %>% select(-all_of(target_column))
  test_features <- test_data %>% select(-all_of(target_column))
  
  # Train Naive Bayes model on train_set
  nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set)
  
  # Make predictions on the validation set
  validation_predictions <- predict(nb_model, validation_features)
  
  # Get prediction probabilities for validation set
  validation_probabilities <- predict(nb_model, validation_features, type = "raw")
  
  # Evaluate model performance with a confusion matrix for validation set
  validation_conf_matrix <- table(Predicted = validation_predictions, Actual = validation_set[[target_column]])
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, validation_probabilities, 
                                                "Naive Bayes (Validation)")
  
  # Make predictions on the test set
  predictions <- predict(nb_model, test_features)
  
  # Get prediction probabilities for test set
  prediction_probabilities <- predict(nb_model, test_features, type = "raw")
  
  # Ensure both predicted and actual labels are factors with the same levels
  predictions <- factor(predictions, levels = levels(test_labels))
  
  # Evaluate model performance with a confusion matrix for test set
  conf_matrix <- table(Predicted = predictions, Actual = test_labels)
  
  metrics_nb <- calculate_model_metrics(conf_matrix, prediction_probabilities, "Naive Bayes")
  
  # Create a dataframe with the desired structure
  metrics_nb_dataframe = get_dataframe("Naive Bayes", metrics_nb)
  
  # Each classification model needs to return these two variables
  return (list(metrics_nb_dataframe = metrics_nb_dataframe, metrics_nb = metrics_nb))
}
```

### def XG_Boost

This **XGBoost model** is a powerful gradient boosting framework designed for **binary classification**, leveraging **parallel computation** and **tree-based learning** to achieve high predictive performance. The model starts with **data preprocessing**, where numeric features in the training, validation, and test sets are **standardized** to ensure stable optimization. The target variable is converted into **binary labels (0/1)** to align with the **logistic objective function** used in XGBoost. The dataset is then transformed into **DMatrix format**, a highly optimized data structure that accelerates computation and improves memory efficiency. The model employs **hyperparameters** such as `max_depth = 6`, `eta = 0.1`, `subsample = 0.8`, and `colsample_bytree = 0.8` to **balance complexity and generalization**. A **learning rate (`eta`)** of 0.1 is chosen to ensure stable convergence, while `max_depth` controls the depth of each decision tree, preventing overfitting. Additionally, `subsample` and `colsample_bytree` introduce **randomness in data selection**, enhancing model robustness. The model is trained using **200 boosting rounds**, with **early stopping** (`early_stopping_rounds = 10`) applied to prevent unnecessary computation when performance no longer improves on the validation set. After training, the model **predicts event probabilities** on both the validation and test sets. Predictions are converted into binary class labels using a **0.5 probability threshold**, and classification performance is assessed using **confusion matrices** and key evaluation metrics through `calculate_model_metrics()`.

```{r}
XG_Boost <- function(train_data, test_data) {
  # Convert train_data and test_data to data.table format
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features and scale them
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert data to matrix format required by XGBoost
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Convert event labels to 0/1
  y_train <- as.numeric(train_set$event == "yes")
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  y_validation <- as.numeric(validation_set$event == "yes")
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Convert to DMatrix format, which is optimized for XGBoost
  dtrain <- xgb.DMatrix(data = x_train, label = y_train)
  dvalidation <- xgb.DMatrix(data = x_validation, label = y_validation)
  dtest <- xgb.DMatrix(data = x_test)
  
  # Detect available CPU cores for parallel computation
  num_cores <- detectCores()
  
  # Define XGBoost hyperparameters
  params <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = 6,
    eta = 0.1,
    subsample = 0.8,
    colsample_bytree = 0.8,
    nthread = num_cores
  )
  
  # Train XGBoost model with early stopping using validation set
  xgb_model <- xgb.train(params = params,
                         data = dtrain,
                         nrounds = 200,
                         early_stopping_rounds = 10,
                         watchlist = list(train = dtrain, validation = dvalidation),
                         verbose = 0)
  
  # Predict probabilities on the validation dataset
  predict_probabilities_val <- predict(xgb_model, dvalidation)
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(factor(binary_prediction_val, levels = c("yes", "no")),
                                  factor(validation_set$event, levels = c("yes", "no")))
  
  # Evaluate validation set performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "XGBoost (Validation)")
  
  # Predict probabilities on the test dataset
  predict_probabilities_xgb <- predict(xgb_model, dtest)
  # Convert predicted probabilities into binary class labels (yes/no) using a threshold of 0.5
  binary_prediction_xgb <- ifelse(predict_probabilities_xgb > 0.5, "yes", "no")
  
  # Ensure both predicted and actual labels are factors with the same levels
  binary_prediction_xgb <- factor(binary_prediction_xgb, levels = c("yes", "no"))
  test_data$event <- factor(test_data$event, levels = c("yes", "no"))
  
  # Ensure confusion matrix includes all levels
  confusion_matrix_xgb <- table(factor(binary_prediction_xgb, levels = c("yes", "no")), 
                                factor(test_data$event, levels = c("yes", "no")))
  
  # Ensure matrix is complete to avoid subscript out of bounds error
  if (!all(c("yes", "no") %in% rownames(confusion_matrix_xgb))) {
    confusion_matrix_xgb <- matrix(0, nrow = 2, ncol = 2, dimnames = list(c("yes", "no"), c("yes", "no")))
  }
  
  # Evaluate model performance using accuracy, precision, recall, and F1-score
  metrics_xgb <- calculate_model_metrics(confusion_matrix_xgb, predict_probabilities_xgb, "XGBoost")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_xgb_dataframe <- get_dataframe("XGBoost", metrics_xgb)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_xgb_dataframe = metrics_xgb_dataframe, metrics_xgb = metrics_xgb))
}
```

### def elastic_net

This **Elastic Net model** is a **regularized logistic regression classifier** that combines **Lasso (L1) and Ridge (L2) penalties**, offering a balance between **feature selection** and **coefficient shrinkage**. The model starts with **data preprocessing**, where numeric features are standardized to ensure stable optimization, and categorical features are transformed into a matrix format using `model.matrix()`, making them compatible with `glmnet`. The dataset is then split into **80% training set and 20% validation set**, allowing for hyperparameter tuning to find the optimal regularization strength. Elastic Net is trained with an **alpha value of 0.5**, meaning it applies **equal weight to both L1 and L2 regularization**, ensuring that it not only selects important features (like Lasso) but also maintains **feature correlations** (like Ridge). The model then undergoes **cross-validation (`cv.glmnet()`)**, which evaluates multiple values of `lambda`, the regularization parameter. The **best lambda (`lambda.min`)** is selected based on the lowest cross-validation error, ensuring an optimal trade-off between **bias and variance**. Once the **best lambda** is determined, the model makes **probabilistic predictions** on both the validation and test sets. These probabilities are **converted into binary classifications** using a **0.5 decision threshold**, and performance is assessed using a **confusion matrix and key classification metrics**, calculated through `calculate_model_metrics()`.

```{r}
elastic_net <- function(train_data, test_data) {
  # Load required libraries
  # library(glmnet) # Required for Elastic Net (Lasso + Ridge regularization)
  # library(data.table) # For efficient data handling using data.table
  
  # Convert train_data and test_data to data.table format for optimized processing
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset for standardization
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert the dataset into a matrix format, as required by glmnet
  # Feature matrix for training
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Target variable
  y_train <- train_set$event
  # Feature matrix for validation
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  # Feature matrix for testing
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Train the Elastic Net model with a combination of Lasso (L1) and Ridge (L2) regularization
  # alpha = 0.5 sets an equal mix of Lasso and Ridge penalties
  elastic_net_model <- glmnet(x_train, y_train, family = "binomial", alpha = 0.5)
  
  # Use cross-validation to determine the best lambda value
  cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0.5)
  best_lambda <- cv_model$lambda.min
  
  # Predict event probabilities for the validation dataset
  predict_probabilities_val <- predict(elastic_net_model, s = best_lambda, newx = x_validation, type = "response")
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Elastic Net (Validation)")
  
  # Predict event probabilities for the test dataset using the best lambda
  predict_probabilities_en <- predict(elastic_net_model, s = best_lambda, newx = x_test, type = "response")
  binary_prediction_en <- ifelse(predict_probabilities_en > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  confusion_matrix_en <- table(Predicted = binary_prediction_en, Actual = test_data$event)
  
  # Evaluate model performance using key classification metrics (accuracy, precision, recall, F1-score)
  metrics_en <- calculate_model_metrics(confusion_matrix_en, predict_probabilities_en, "Elastic Net")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_en_dataframe <- get_dataframe("Elastic Net", metrics_en)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_en_dataframe = metrics_en_dataframe, metrics_en = metrics_en))
}
```

### def logistic_regression_op

This **logistic regression model** implements **Lasso regularization (L1 penalty)** to enhance feature selection and prevent overfitting in binary classification tasks. The process begins with **data preprocessing**, where numeric features in the training, validation, and test sets are **standardized** (mean = 0, standard deviation = 1) to improve optimization stability. The data is then converted into **matrix format** using `model.matrix()`, ensuring compatibility with the `glmnet` package. To determine the **optimal regularization parameter (`lambda`)**, the model first **performs cross-validation (`cv.glmnet()`)** on the training set, generating a range of candidate `lambda` values. These values are then evaluated using the **validation set**, where for each `lambda`, the model predicts event probabilities and computes the **AUC (Area Under the Curve)**. The `lambda` yielding the highest **AUC** is selected as the best parameter, ensuring that the model maximizes predictive performance. Once the **best lambda** is determined, the model is retrained on the full training set with this optimal parameter. Predictions are made on both the **validation and test sets**, with event probabilities converted into **binary classifications** using a **0.5 threshold**. Model performance is assessed through **confusion matrices** and various classification metrics, calculated via `calculate_model_metrics()`.

```{r}
logistic_regression_op <- function(train_data, test_data) {
  # Load required libraries
  # library(glmnet) # Logistic regression with regularization
  # library(data.table) # Efficient data handling
  # library(pROC) # Compute AUC for validation
  
  # Convert train_data and test_data to data.table format
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features and scale them
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Standardize numeric features
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert datasets to matrix format as required by glmnet
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Convert event labels to 0/1
  y_train <- as.numeric(train_set$event == "yes")
  
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  # Convert event labels to 0/1
  y_validation <- as.numeric(validation_set$event == "yes")
  
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Apply logistic regression with Lasso regularization (alpha = 1)
  logistic_regression_classifier <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
  
  # Perform cross-validation to get a range of lambda values
  cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
  # Extract lambda values from cross-validation
  lambda_candidates <- cv_model$lambda
  
  # Select best lambda based on AUC from validation set
  best_lambda <- NULL
  # Initialize best AUC score
  best_auc <- -Inf
  
  for (lambda in lambda_candidates) {
    # Predict probabilities on validation set
    predict_probabilities_val <- predict(logistic_regression_classifier, s = lambda, newx = x_validation, 
                                         type = "response")
    # Convert matrix to numeric vector
    predict_probabilities_val <- as.vector(predict_probabilities_val)
    
    # Compute AUC for validation set
    roc_obj <- roc(y_validation, predict_probabilities_val)
    auc_val <- auc(roc_obj)
    
    # Update best lambda if current AUC is better
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_lambda <- lambda
    }
  }
  
  # Train final model with the best lambda
  final_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = best_lambda)
  
  # Predict on validation dataset using the best lambda
  predict_probabilities_val <- predict(final_model, newx = x_validation, type = "response")
  predict_probabilities_val <- as.vector(predict_probabilities_val)
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation set performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Logistic Regression (Validation)")
  
  # Predict on test dataset using the best lambda
  predict_probabilities_test <- predict(final_model, newx = x_test, type = "response")
  predict_probabilities_test <- as.vector(predict_probabilities_test)
  binary_prediction_test <- ifelse(predict_probabilities_test > 0.5, "yes", "no")
  
  # Create confusion matrix for the test set
  test_conf_matrix <- table(Predicted = binary_prediction_test, Actual = test_data$event)
  
  # Evaluate model performance on the test set
  metrics_lr <- calculate_model_metrics(test_conf_matrix, predict_probabilities_test, "Logistic Regression")
  
  # Store calculated metrics in a structured dataframe
  metrics_lr_dataframe <- get_dataframe("Logistic Regression", metrics_lr)
  
  return (list(metrics_lr_dataframe = metrics_lr_dataframe, metrics_lr = metrics_lr))
}
```

### def decision_tree_op

This **Decision Tree model** is a classification algorithm designed to iteratively split the data based on feature values, forming a tree-like structure for decision-making. The process begins with **data partitioning**, where the dataset is split into **80% training set and 20% validation set**, ensuring a separate dataset for hyperparameter tuning. The model undergoes **hyperparameter optimization** by selecting the best **complexity parameter (cp)**, which controls **pruning** and helps prevent overfitting. A range of `cp` values is tested, and for each candidate value, a **decision tree is trained on the training set**, followed by **prediction on the validation set**. The model's **AUC (Area Under the Curve)** is computed for each `cp` value, and the one that **yields the highest AUC** is selected as the **optimal pruning parameter**. Once the **best `cp`** is determined, the final **decision tree** is trained using this optimal parameter and is applied to both the **validation and test sets**. Predictions are generated, and classification performance is evaluated using a **confusion matrix and key metrics** computed through `calculate_model_metrics()`.

```{r}
decision_tree_op <- function(train_data, test_data) {
  # Load required libraries
  # library(rpart)
  # library(caret) # Load caret for data partitioning
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Define candidate complexity parameters (cp) for pruning
  # Range of possible cp values
  cp_candidates <- seq(0.0001, 0.05, by = 0.002)
  best_cp <- NULL
  # Initialize the best AUC score
  best_auc <- -Inf
  
  # Loop over different cp values to find the best one using the validation set
  for (cp in cp_candidates) {
    # Train the decision tree model with the current cp value
    decision_tree_model <- rpart(
      event ~ .,
      data = train_set,
      method = "class",
      control = rpart.control(
        cp = cp,        # Adjusting cp for pruning
        maxdepth = 30,  # Keeping max depth fixed
        minsplit = 20   # Keeping minsplit fixed
      )
    )
    
    # Predict probabilities on validation set
    # Get probability for "yes"
    predict_probabilities_val <- predict(decision_tree_model, validation_set, type = "prob")[, 2]
    
    # Compute AUC for validation set
    auc_val <- auc(roc(validation_set$event, predict_probabilities_val))
    
    # Update the best cp if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_cp <- cp
    }
  }
  
  # Train the final model with the best cp value
  decision_tree_classifier <- rpart(
    event ~ .,
    data = train_set,
    method = "class",
    control = rpart.control(
      cp = best_cp,  # Using the best cp found from validation set
      maxdepth = 30,
      minsplit = 20
    )
  )
  
  # Predict on the validation dataset using the selected cp
  predict_probabilities_val <- predict(decision_tree_classifier, validation_set, type = "class")
  validation_conf_matrix <- table(Predicted = predict_probabilities_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Decision Tree (Validation)")
  
  # Predict on the testing dataset using the final model
  predict_probabilities_dt <- predict(decision_tree_classifier, test_data, type = "class")
  test_conf_matrix <- table(Predicted = predict_probabilities_dt, Actual = test_data$event)
  
  # Evaluate model performance
  metrics_dt <- calculate_model_metrics(test_conf_matrix, predict_probabilities_dt, "Decision Tree")
  
  # Create a dataframe with the desired structure
  metrics_dt_dataframe = get_dataframe("Decision Tree", metrics_dt)
  
  return (list(metrics_dt_dataframe = metrics_dt_dataframe, metrics_dt = metrics_dt))
}
```

### def naive_bayes_op

This **Naïve Bayes model** is a probabilistic classifier based on **Bayes’ theorem**, assuming **feature independence** and leveraging **Laplace smoothing** to enhance performance. The model begins with **data preprocessing**, where the target variable is converted into a **factor**, ensuring compatibility with the `naiveBayes()` function. The dataset is then split into **80% training set and 20% validation set**, allowing for hyperparameter tuning. To optimize classification performance, the model tests multiple **Laplace smoothing values** ranging from 0 to 1, which help handle cases where certain feature-class combinations do not appear in the training data. For each candidate Laplace value, a **Naïve Bayes classifier** is trained and evaluated on the validation set, with **AUC (Area Under the Curve)** computed as the selection criterion. The Laplace value that yields the **highest AUC** is chosen as the optimal parameter, ensuring robust probability estimation. Once the best Laplace parameter is determined, the final **Naïve Bayes model** is trained on the full training set and applied to **both the validation and test datasets**. Predictions are made in **both categorical and probabilistic forms**, with **probabilities for the positive class ("yes")** used to evaluate model performance. The results are assessed using a **confusion matrix and classification metrics**, computed through `calculate_model_metrics()`.

```{r}
naive_bayes_op <- function(train_data, test_data) {
  # Load required libraries
  # library(e1071)
  # library(caret) # Load caret for data partitioning
  
  target_column = "event"
  
  # Convert the target column to a factor if it's not already
  train_data[[target_column]] <- as.factor(train_data[[target_column]])
  test_labels <- as.factor(test_data[[target_column]])
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data[[target_column]], p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Remove the target column from the validation and test sets for prediction
  validation_features <- validation_set %>% select(-all_of(target_column))
  test_features <- test_data %>% select(-all_of(target_column))
  
  # Define candidate Laplace smoothing values
  # Range of laplace values to test
  laplace_candidates <- seq(0, 1, by = 0.1)
  best_laplace <- NULL
  # Initialize the best AUC score
  best_auc <- -Inf
  
  # Hyperparameter tuning: Find the best laplace value using validation set
  for (laplace in laplace_candidates) {
    # Train Naïve Bayes model with current laplace value
    nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set, laplace = laplace)
    
    # Get probability predictions on the validation set
    # Probability for "yes"
    validation_probabilities <- predict(nb_model, validation_features, type = "raw")[, 2]
    
    # Compute AUC for validation set
    auc_val <- auc(roc(validation_set[[target_column]], validation_probabilities))
    
    # Update best laplace if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_laplace <- laplace
    }
  }
  
  # Train the final Naïve Bayes model with the best laplace value
  nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set, laplace = best_laplace)
  
  # Make predictions on the validation set
  validation_predictions <- predict(nb_model, validation_features)
  
  # Get prediction probabilities for validation set
  validation_probabilities <- predict(nb_model, validation_features, type = "raw")
  
  # Evaluate model performance with a confusion matrix for validation set
  validation_conf_matrix <- table(Predicted = validation_predictions, Actual = validation_set[[target_column]])
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, validation_probabilities, 
                                                "Naive Bayes (Validation)")
  
  # Make predictions on the test set using the final model
  predictions <- predict(nb_model, test_features)
  
  # Get prediction probabilities for test set
  prediction_probabilities <- predict(nb_model, test_features, type = "raw")
  
  # Ensure both predicted and actual labels are factors with the same levels
  predictions <- factor(predictions, levels = levels(test_labels))
  
  # Evaluate model performance with a confusion matrix for test set
  conf_matrix <- table(Predicted = predictions, Actual = test_labels)
  
  metrics_nb <- calculate_model_metrics(conf_matrix, prediction_probabilities, "Naive Bayes")
  
  # Create a dataframe with the desired structure
  metrics_nb_dataframe = get_dataframe("Naive Bayes", metrics_nb)
  
  # Each classification model needs to return these two variables
  return (list(metrics_nb_dataframe = metrics_nb_dataframe, metrics_nb = metrics_nb))
}
```

### def elastic_net_op

This **Elastic Net model** is an advanced **regularized logistic regression classifier** that combines both **Lasso (L1) and Ridge (L2) penalties**, optimizing for **feature selection and coefficient stability**. The model begins with **data preprocessing**, where numeric features are standardized for improved optimization, and categorical features are transformed into a matrix format using `model.matrix()` to ensure compatibility with `glmnet`. The dataset is split into **80% training and 20% validation**, allowing for hyperparameter tuning. Unlike standard models that rely on a predefined **alpha** value, this implementation systematically searches for the **best combination of alpha (L1-L2 mixing parameter) and lambda (regularization strength)** using the validation set. The model iterates through **alpha values ranging from 0 to 1**, with each step performing **cross-validation (`cv.glmnet()`)** to find the corresponding **optimal lambda**. The combination that achieves the **highest AUC (Area Under the Curve) score** on the validation set is selected as the best set of parameters. After determining the optimal hyperparameters, the final model is retrained on the full training set and used to **predict probabilities for both the validation and test sets**. Predictions are thresholded at **0.5** to produce binary classifications, which are evaluated using a **confusion matrix and key classification metrics** via `calculate_model_metrics()`.

```{r}
elastic_net_op <- function(train_data, test_data) {
  # Load required libraries
  # library(glmnet) # Required for Elastic Net (Lasso + Ridge regularization)
  # library(data.table) # For efficient data handling using data.table
  # library(pROC) # Compute AUC for validation
  
  # Convert train_data and test_data to data.table format for optimized processing
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  # Ensure reproducibility
  set.seed(123)
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset for standardization
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert the dataset into a matrix format, as required by glmnet
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Convert event labels to 0/1
  y_train <- as.numeric(train_set$event == "yes")
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  # Convert event labels to 0/1
  y_validation <- as.numeric(validation_set$event == "yes")
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Define candidate alpha values for Elastic Net optimization
  alpha_candidates <- seq(0, 1, by = 0.1)  # Range from 0 (Ridge) to 1 (Lasso)
  best_alpha <- NULL
  best_lambda <- NULL
  # Initialize best AUC score
  best_auc <- -Inf
  
  # Iterate through different alpha values to find the best one using validation set
  for (alpha in alpha_candidates) {
    # Perform cross-validation to find the best lambda for the current alpha
    cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = alpha)
    # Get best lambda from cross-validation
    lambda_min <- cv_model$lambda.min
    
    # Predict on validation set
    predict_probabilities_val <- predict(cv_model$glmnet.fit, s = lambda_min, newx = x_validation, 
                                         type = "response")
    # Convert matrix to numeric vector
    predict_probabilities_val <- as.vector(predict_probabilities_val)
    
    # Compute AUC for validation set
    roc_obj <- roc(y_validation, predict_probabilities_val)
    auc_val <- auc(roc_obj)
    
    # Update best alpha and lambda if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_alpha <- alpha
      best_lambda <- lambda_min
    }
  }
  
  # Train the final Elastic Net model with the best alpha and lambda values
  elastic_net_model <- glmnet(x_train, y_train, family = "binomial", alpha = best_alpha, lambda = best_lambda)
  
  # Predict on the validation dataset using the selected best parameters
  predict_probabilities_val <- predict(elastic_net_model, newx = x_validation, type = "response")
  # Convert matrix to numeric vector
  predict_probabilities_val <- as.vector(predict_probabilities_val)
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Elastic Net (Validation)")
  
  # Predict event probabilities for the test dataset using the best parameters
  predict_probabilities_en <- predict(elastic_net_model, newx = x_test, type = "response")
  # Convert matrix to numeric vector
  predict_probabilities_en <- as.vector(predict_probabilities_en)
  binary_prediction_en <- ifelse(predict_probabilities_en > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  confusion_matrix_en <- table(Predicted = binary_prediction_en, Actual = test_data$event)
  
  # Evaluate model performance using key classification metrics (accuracy, precision, recall, F1-score)
  metrics_en <- calculate_model_metrics(confusion_matrix_en, predict_probabilities_en, "Elastic Net")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_en_dataframe <- get_dataframe("Elastic Net", metrics_en)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_en_dataframe = metrics_en_dataframe, metrics_en = metrics_en))
}
```
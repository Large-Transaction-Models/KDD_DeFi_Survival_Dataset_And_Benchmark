---
title: "DeepSurvForSurvival"
output: html_document
date: "2025-01-22"
---

```{r setup, include=FALSE}
# 环境配置
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
local({r <- getOption("repos"); r["CRAN"] <- "http://cran.r-project.org"; options(repos=r)})

# 包安装检查
required_packages <- c(
  "tidyverse", "survival", "survminer", "ggplot2",
  "survivalmodels", "reticulate", "lubridate"
)

install_missing <- function(pkg) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}

sapply(required_packages, install_missing)
```

```{r}
normalize_data <- function(train, test, exclude_cols = c("timeDiff", "status")) {
  # Identify numeric predictor columns (exclude outcome variables)
  numeric_cols <- train %>%
    select(where(is.numeric)) %>%
    colnames() %>%
    setdiff(exclude_cols)
  
  if (length(numeric_cols) == 0) {
    warning("No numeric predictor columns to normalize!")
    return(list(train = train, test = test))
  }
  
  # Compute means and standard deviations for predictor columns
  train_means <- sapply(train[, numeric_cols, drop = FALSE], mean, na.rm = TRUE)
  train_sds   <- sapply(train[, numeric_cols, drop = FALSE], sd, na.rm = TRUE)
  
  # Avoid division by zero: replace any 0 SD with 1
  #[train_sds == 0] <- 1
  train_sds[train_sds == 0] <- 1e-6  # Avoid true zero division
  
  # Convert predictor columns to matrices for safe arithmetic operations
  train_mat <- as.matrix(train[, numeric_cols, drop = FALSE])
  test_mat  <- as.matrix(test[, numeric_cols, drop = FALSE])
  
  # Normalize: subtract mean and divide by standard deviation
  train_mat <- sweep(train_mat, 2, train_means, FUN = "-")
  train_mat <- sweep(train_mat, 2, train_sds,   FUN = "/")
  test_mat  <- sweep(test_mat,  2, train_means, FUN = "-")
  test_mat  <- sweep(test_mat,  2, train_sds,   FUN = "/")
  
  # Replace any NA or infinite values with 0 in the matrices
  train_mat[is.na(train_mat)]       <- 0
  train_mat[is.infinite(train_mat)] <- 0
  test_mat[is.na(test_mat)]         <- 0
  test_mat[is.infinite(test_mat)]   <- 0
  
  # Put the normalized predictor columns back into the original data frames
  train[, numeric_cols] <- train_mat
  test[, numeric_cols]  <- test_mat
  
  list(train = train, test = test)
}

```



```{r}
deep_surv_pipeline <- function(
  train_data, 
  test_data,
  time_col = "timeDiff",
  status_col = "status",
  epochs = 50,          # Increased epochs
  num_nodes = c(17),
  #num_nodes = c(32,16), 
  dropout = 0.401,
  activation = "relu",
  optimizer = "sgd",
  l2_reg = 4.425,        # L2 regularization
  lr_decay = 3.173e-4,
  momentum = 0.936,
  batch_size = 64,
  lr = 0.0003194
) {
  if (is.null(train_data) || is.null(test_data)) {
    stop("Error: train_data or test_data is NULL!")
  }
  
  if (nrow(train_data) == 0 || nrow(test_data) == 0) {
    stop("Error: train_data or test_data is empty!")
  }

  required_cols <- c(time_col, status_col)
  if (!all(required_cols %in% colnames(train_data)) || !all(required_cols %in% colnames(test_data))) {
    stop(paste("Error: Required columns", paste(required_cols, collapse = ", "), "are missing in train_data or test_data!"))
  }

  # Create the survival object for training (not used later in this snippet, but required by the model)
  train_y <- Surv(
    time = train_data[[time_col]],
    event = train_data[[status_col]]
  )
  
  formula_str <- paste0("Surv(", time_col, ", ", status_col, ") ~ .")
  
  model <- survivalmodels::deepsurv(
    formula = as.formula(formula_str),
    data = train_data,
    num_nodes = num_nodes,
    dropout = dropout,
    activation = activation,
    epochs = epochs,
    batch_size = batch_size,
    early_stopping = TRUE,
    patience = 15,
    verbose = TRUE,
    #min_delta = 0.1  # Require meaningful improvement
  )
  
  predict_wrapper <- function(model, newdata) {
    tryCatch({
      predict(model, newdata = newdata, type = "risk")
    }, error = function(e) {
      print("Prediction error:")
      print(e)
      return(rep(NA, nrow(newdata)))
    })
  }
  
  risk_scores <- predict_wrapper(model, test_data)
  
  if (any(is.na(risk_scores))) {
    stop("Error: Risk scores contain NA values!")
  }
  
  # Filter test_data for non-missing time and status values
  valid_rows <- !is.na(test_data[[time_col]]) & !is.na(test_data[[status_col]])
  if (sum(valid_rows) > 0) {
    test_data <- test_data[valid_rows, ]
    risk_scores <- risk_scores[valid_rows]
    
    if (length(risk_scores) != nrow(test_data)) {
      stop("Error: Length of risk_scores does not match number of rows in test_data!")
    }
    
    # Debugging: Print summaries to verify non-missing observations and events
    print(paste("Valid test observations:", nrow(test_data)))
    print(summary(test_data[[time_col]]))
    print(table(test_data[[status_col]]))
    
    # Check that there is at least one event before computing concordance
    if (sum(test_data[[status_col]] == 1, na.rm = TRUE) == 0) {
      warning("No events in test_data. Concordance index cannot be computed.")
      cindex <- NA
    } else {
      cindex <- survival::concordance(
        Surv(test_data[[time_col]], test_data[[status_col]]) ~ risk_scores
      )$concordance
      print(paste("C-index:", cindex))
    }
  } else {
    stop("Error: No valid observations in test_data for concordance calculation!")
  }
  
  list(
    model = model,
    cindex = cindex,
    risk_scores = risk_scores
  )
}


```



```{r}
# Data Loading
load_data <- function(indexEvent = "borrow", outcomeEvent = "repay") {
  print(paste("Loading data for indexEvent:", indexEvent, "and outcomeEvent:", outcomeEvent))

  assign("indexEvent", indexEvent, envir = .GlobalEnv)
  assign("outcomeEvent", outcomeEvent, envir = .GlobalEnv)

  source("~/DMLR_DeFi_Survival_Dataset_And_Benchmark/DeFi_source/dataLoader.R")
  source("~/DMLR_DeFi_Survival_Dataset_And_Benchmark/StudentData/preprocessing.R")

  n_data <- preprocessing(train, test)
  train <- as.data.frame(n_data[[1]])
  test <- as.data.frame(n_data[[2]])
  #train$timeDiff <- log1p(train$timeDiff)  # Log-transform
  #test$timeDiff <- log1p(test$timeDiff)

  print("Preprocessing train and test datasets...")

  if (is.null(n_data) || length(n_data) < 2) {
    stop("Error: preprocessing() returned NULL or incomplete data!")
  }

  return(list(train = train, test = test))
}
```

```{r}
# Modified Execution Pipeline with Cross-Validation
deepsurv_results <- local({
  datasets <- load_data()
  
  # Normalize data
  normalized_data <- normalize_data(datasets$train, datasets$test)
  train <- normalized_data$train
  test <- normalized_data$test

  # Create smaller dataset for testing
  set.seed(123)
  mini_train <- train %>% sample_n(80000)
  mini_test <- test %>% sample_n(20000)
  
  # Add stratified K-fold cross-validation
  n_folds <- 5
  fold_ids <- caret::createFolds(
    y = mini_train$status,
    k = n_folds,
    list = FALSE
  )
  
  cv_results <- list()
  
  for (fold in 1:n_folds) {
    cat("\n=== Processing fold", fold, "/", n_folds, "===\n")
    
    # Split into train/validation
    val_idx <- which(fold_ids == fold)
    train_fold <- mini_train[-val_idx, ]
    val_fold <- mini_train[val_idx, ]
    
    # Run pipeline with validation monitoring
    fold_result <- deep_surv_pipeline(
      train_data = train_fold,
      test_data = val_fold,  # Using validation fold as test for CV
      epochs = 10,
      num_nodes = c(17),
      #num_nodes = c(32,16),
      dropout = 0.0,
      lr = 0.5,
      optimizer = "sgd",
      l2_reg = 4.425,        # L2 regularization
      batch_size = 32,
      momentum = 0.936
    )
    
    cv_results[[fold]] <- fold_result$cindex
  }
  
  # Aggregate CV results
  mean_cindex <- mean(unlist(cv_results), na.rm = TRUE)
  cat("\n=== Cross-Validation Results ===\n")
  cat("Mean C-index across folds:", mean_cindex, "\n")
  
  # Final training on full mini_train with best params
  final_model <- deep_surv_pipeline(
    train_data = mini_train,
    test_data = mini_test,
    epochs = 10,
    num_nodes = c(17),
    #num_nodes = c(32,16),
    dropout = 0,
    lr = 0.5,
    optimizer = "sgd",
    l2_reg = 4.425,        # L2 regularization
    batch_size = 32,
    momentum = 0.936
  )
  
  # Save results
  output_dir <- "/data/DMLR_DeFi_Survival_Dataset_And_Benchmark/DeepLearningModels"
  saveRDS(final_model, file.path(output_dir, "deepsurv_final_model.rds"))
  
  list(
    cv_results = cv_results,
    final_model = final_model
  )
})

```


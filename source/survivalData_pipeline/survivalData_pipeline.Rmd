---
title: "DMLR DeFi Survival Data Pipeline"
author: "Hanzhen Qin(qinh2)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: true
    number_sections: true
    df_print: paged
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
# Check and install required R packages
if (!require("conflicted")) {
  install.packages("conflicted", dependencies = TRUE)
  library(conflicted)
}

# Set default CRAN repository
local({
  r <- getOption("repos")
  r["CRAN"] <- "http://cran.r-project.org"
  options(repos = r)
})

# Define the list of required packages
required_packages <- c(
  "rmarkdown", "tidyverse", "stringr", "ggbiplot", "pheatmap", 
  "caret", "survival", "survminer", "ggplot2", 
  "kableExtra", "rpart", "glmnet", "data.table", "reshape2", 
  "pander", "readr", "dplyr", "e1071", "ROSE", "xgboost", "gbm", "parallel"
)

# Loop through the package list and install missing packages
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}

# Handle function name conflicts
conflict_prefer("slice", "dplyr")
conflict_prefer("filter", "dplyr")

# Set knitr options for R Markdown
knitr::opts_chunk$set(echo = TRUE)

# Rename dplyr functions to avoid conflicts with other packages
select <- dplyr::select
rename <- dplyr::rename
summarize <- dplyr::summarize
group_by <- dplyr::group_by
```

# Survival Data Pipeline

## Pipeline Summary

* Summary of work
  
  * Data preprocessing: `data_processing`, `get_train_test_data`, `smote_data`, 
  `get_classification_cutoff`
  
  * Model Performance Evaluation and Visualization : `calculate_model_metrics`, `get_dataframe`,
  `combine_classification_results`, `accuracy_comparison_plot`, `get_percentage`, 
  `specific_accuracy_statistics`, `combine_accuracy_dataframes`
  
  * Classification model: `logistic_regression`, `decision_tree`, `Naive_bayes`, `XG_Boost`, `GBM`, 
  `elastic_net`, `XG_Boost_optimization (NOT USED)`, `GBM_optimization (NOT USED)`
  

## Data Preprocessing

### def get_train_test_data

The `get_train_test_data` function is designed to get the training and test data based on the `indexEvent` and `outcomeEvent` parameters, but the specific logic has not yet been implemented. The function references the external script `dataLoader.r` through `source()` to load the latest processed survival data, and returns the train and test data sets directly by default.

```{r}
get_train_test_data <- function(indexEvent, outcomeEvent) {
  source("~/DMLR_DeFi_Survival_Dataset_And_Benchmark/source/dataLoader.R")
}
```

### def data_processing

The `data_processing` function is designed to transform a survival analysis dataset into a format suitable for classification tasks by performing a series of preprocessing steps. It begins by filtering out invalid records where `timeDiff` is less than or equal to 0, ensuring only valid observations are retained. Then, it removes records where `timeDiff` falls within a specified threshold (`set_timeDiff`) but no event occurred (`status == 0`). A new binary column, `event`, is created to indicate whether an event occurred within the threshold ("yes" or "no"). The function drops unnecessary columns as defined in a predefined list, ensuring only relevant features remain, and removes columns entirely populated with missing values. To handle missing data, numeric columns are filled with `-999` to mark missing values clearly without distorting numerical distributions, and categorical columns are filled with `"missing"` to create a distinct category for absent data. Finally, character columns are converted to factors, making them ready for classification models.

```{r}
data_processing <- function(survivalData, set_timeDiff) {
  # filter out invalid records where `timeDiff` is <= 0 early
  survivalData <- survivalData %>% filter(timeDiff > 0)
  
  # filter out records based on the `set_timeDiff` threshold and `status`
  survivalData <- survivalData %>% filter(!(timeDiff / 86400 <= set_timeDiff & status == 0))
  
  # create a new binary column `event` based on `timeDiff`
  survivalDataForClassification <- survivalData %>%
    mutate(event = case_when(
      timeDiff / 86400 <= set_timeDiff ~ "yes",
      timeDiff / 86400 > set_timeDiff ~ "no"
    ))
  
  # define features to drop
  # featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
  #                     "timeDiff", "deployment", "version", "indexID", "user", "status", 
  #                     "quarter", "liquidator", "userFlashloanAvgAmount", "userReserveMode",
  #                     "reserve", "pool", "timestamp", "type", "datetime", "quarter_start_date",
  #                     "userCoinTypeMode", "coinType", "userIsNew", "userDepositSum", 
  #                     "userDepositSumUSD", "userDepositAvgAmountUSD", "userDepositSumETH",
  #                     "userDepositAvgAmountETH", "userWithdrawSum", "userWithdrawSumUSD",
  #                     "userWithdrawAvgAmountUSD", "userWithdrawSumETH", 
  #                     "userWithdrawAvgAmountETH", "userBorrowSum", "userBorrowSumUSD", 
  #                     "userBorrowAvgAmountUSD", "userBorrowSumETH", 
  #                     "userBorrowAvgAmountETH", "userRepaySum", "userRepaySumUSD", 
  #                     "userRepayAvgAmountUSD", "userRepaySumETH", "userRepayAvgAmountETH", 
  #                     "userFlashloanSum", "userLiquidationSum", "userLiquidationSumUSD",
  #                     "userLiquidationAvgAmountUSD", "userLiquidationSumETH", 
  #                     "userLiquidationAvgAmountETH", "userFlashloanCount", "priceInUSD")
  
  featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
                      "timeDiff", "status", "deployment", "version", "indexID", "user", 
                      "liquidator", "pool", "timestamp", "type", "datetime", "quarter_start_date")
  
  # remove only columns that actually exist in the dataset
  featuresToDrop <- intersect(featuresToDrop, colnames(survivalDataForClassification))
  
  survivalDataForClassification <- survivalDataForClassification %>%
    # drop unnecessary columns
    select(-any_of(featuresToDrop)) %>%
    # remove columns with only NA values
    select(where(~ !all(is.na(.)))) %>%
    # replace NA in numeric columns with -999
    mutate(across(where(is.numeric), ~ replace_na(., -999))) %>%
    # replace NA in character columns with "missing"
    mutate(across(where(is.character), ~ replace_na(., "missing"))) %>%
    # convert character columns to factors
    mutate(across(where(is.character), as.factor))
  
  # return the processed dataset
  return(survivalDataForClassification)
}
```

### def get_classification_cutoff (@Author: Aaron Green)

The `get_classification_cutoff` function retrieves the `ConvergedRMST_5` value from a predefined dataset based on the specified `indexEvent` and `outcomeEvent`. The dataset contains information about various index events (e.g., "Borrow", "Deposit") and their corresponding outcome events (e.g., "Full Repay", "Withdraw"), along with convergence metrics like `ConvergedTau` and `ConvergedRMST` at different time horizons. The function filters the dataset by matching the input events with the respective `IndexEvent` and `OutcomeEvent` columns and returns the relevant `ConvergedRMST_5` value, which represents a specific metric of convergence over a 5-unit time period. Return the optimal cutoff value for the `data_processing` function.

```{r}
get_classification_cutoff <- function(indexEvent, outcomeEvent){
  # library(stringr)
  # Create the dataframe
  data <- data.frame(
    IndexEvent = c("Borrow", "Borrow", "Borrow", "Borrow", "Borrow", "Borrow", 
                   "Deposit", "Deposit", "Deposit", "Deposit", "Deposit",
                   "Repay", "Repay", "Repay", "Repay", "Repay",
                   "Withdraw", "Withdraw", "Withdraw", "Withdraw", "Withdraw"),
    OutcomeEvent = c("Account Liquidated", "Deposit", "Full Repay", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Repay"),
    ConvergedTau_1 = c(91, 99, 95, 101, 69, 99, 
                       90, 100, 101, 99, 93, 
                       93, 95, 100, 101, 100, 
                       94, 101, 99, 101, 100),
    ConvergedRMST_1 = c(69.36, 90.98, 74.12, 100.88, 28.71, 92.59, 
                        68.64, 91.58, 100.95, 89.70, 64.84, 
                        72.56, 63.23, 93.72, 100.86, 93.93,
                        75.10, 96.01, 82.13, 100.88, 93.82),
    ConvergedTau_5 = c(20, 21, 20, 21, 17, 21, 
                       20, 21, 21, 21, 20, 
                       20, 20, 21, 21, 21,
                       20, 21, 21, 21, 21),
    ConvergedRMST_5 = c(17.43, 19.88, 17.15, 20.99, 10.24, 20.21, 
                        17.52, 19.73, 20.99, 19.69, 15.67, 
                        17.43, 14.99, 20.13, 20.99, 20.15,
                        17.72, 20.26, 18.14, 20.98, 20.04),
    ConvergedTau_10 = c(11, 11, 11, 11, 10, 11, 
                        11, 11, 11, 11, 11, 
                        11, 11, 11, 11, 11,
                        11, 11, 11, 11, 11),
    ConvergedRMST_10 = c(9.94, 10.51, 9.72, 11.00, 6.66, 10.68, 
                         10.00, 10.43, 11.00, 10.44, 8.95, 
                         9.90, 8.64, 10.62, 11.00, 10.62,
                         10.05, 10.68, 9.68, 10.99, 10.57)
  )
  
  
  return(data %>% filter(IndexEvent == str_to_title(indexEvent), OutcomeEvent == str_to_title(outcomeEvent)) 
         %>% pull(ConvergedRMST_5))
}
```

### def smote_data

The `smote_data` function leverages the `ROSE` package to address class imbalance in datasets by dynamically generating a balanced dataset based on oversampling and undersampling techniques. The function is designed to be flexible, allowing the user to specify the target variable (`target_var`) and an optional random seed (`seed`) for reproducibility. It validates the input dataset to ensure the target variable exists and dynamically constructs the formula for the `ROSE` function, making it adaptable to different datasets and classification tasks. By generating a balanced dataset where the minority and majority classes are better represented, this function helps mitigate bias in machine learning models and improves their classification accuracy. 

```{r}
smote_data <- function(train_data, target_var = "event", seed = 123) {
  # library(ROSE)
  # check if the input data contains the target variable
  if (!target_var %in% colnames(train_data)) {
    stop(paste("Target variable", target_var, "not found in the dataset"))
  }
  
  # set the random seed (if provided)
  if (!is.null(seed)) {
    set.seed(seed)
  }
  
  # dynamic formula creation to adapt to different target variables
  formula <- as.formula(paste(target_var, "~ ."))
  
  # applying ROSE Balance Data
  train_data_balanced <- ROSE(formula, data = train_data, seed = seed)$data
  
  # return the balanced dataset
  return(train_data_balanced)
}
```

## Model Performance Evaluation and Visualization

### def calculate_model_metrics

The `calculate_model_metrics` function in R is designed to evaluate the performance of a binary classification model using key metrics derived from a confusion matrix. It calculates several important metrics: class accuracy (specificity), negative class accuracy (sensitivity/recall), balanced accuracy (the average of sensitivity and specificity), overall accuracy, precision, and F1 score. The function takes as input the confusion matrix, binary predictions, and the model's name, then computes these metrics and prints them out in a formatted manner. Finally, it returns a list containing all the calculated metrics for further use in performance analysis.

```{r}
calculate_model_metrics <- function(confusion_matrix, binary_predictions, model_name) {
  TN <- confusion_matrix[1, 1] # True Negatives
  FP <- confusion_matrix[1, 2] # False Positives
  FN <- confusion_matrix[2, 1] # False Negatives
  TP <- confusion_matrix[2, 2] # True Positives
  
  # positive Class accuracy (Specificity): TN / (TN + FP)
  class_accuracy <- TN / (TN + FP)
  
  # negative 1 accuracy (Sensitivity/Recall): TP / (TP + FN)
  negative_1_accuracy <- TP / (TP + FN)
  
  # balanced accuracy: Average of Sensitivity and Specificity
  balanced_accuracy <- (class_accuracy + negative_1_accuracy) / 2
  
  # overall accuracy: (TP + TN) / (TP + TN + FP + FN)
  overall_accuracy <- (TP + TN) / (TP + TN + FP + FN)
  
  # precision
  precision <- TP / (TP + FP)
  
  # f1 score
  f1_score <- 2 * (precision * negative_1_accuracy) / (precision + negative_1_accuracy)
  
  # print out all the accuracy records
  print(paste(model_name, "model prediction accuracy:"))
  cat("Class accuracy (Specificity):", sprintf("%.0f%%", class_accuracy * 100), "\n")
  cat("Negative 1 accuracy (Sensitivity/Recall):", sprintf("%.0f%%", negative_1_accuracy * 100), "\n")
  cat("Balanced accuracy:", sprintf("%.0f%%", balanced_accuracy * 100), "\n")
  cat("Overall accuracy:", sprintf("%.0f%%", overall_accuracy * 100), "\n")
  cat("Precision:", sprintf("%.0f%%", precision * 100), "\n")
  cat("F1 score:", sprintf("%.0f%%", f1_score * 100), "\n")
  
  return (list(class_accuracy = class_accuracy,
              negative_1_accuracy = negative_1_accuracy,
              balanced_accuracy = balanced_accuracy,
              overall_accuracy = overall_accuracy,
              precision = precision,
              f1_score = f1_score))
}
```

### def get_dataframe

The `get_dataframe` creates and returns a dataframe `metrics_dataframe` that stores various performance metrics for a specific model. The function accepts a model name (`model_name`) and a metrics object (`metrics`) as parameters, and formats the various metrics in `metrics` (such as classification accuracy, negative class accuracy, balanced accuracy, overall accuracy, precision, and F1 score) as percentages, retains them to integers, and then aggregates them into a dataframe. The final returned `metrics_dataframe` contains key performance information for each model.

```{r}
get_dataframe <- function(model_name, metrics) {
  metrics_dataframe <- data.frame(
    Model = model_name,
    Class_Accuracy = sprintf("%.0f%%", metrics$class_accuracy * 100),
    Negative_1_Accuracy = sprintf("%.0f%%", metrics$negative_1_accuracy * 100),
    Balanced_Accuracy = sprintf("%.0f%%", metrics$balanced_accuracy * 100),
    Overall_Accuracy = sprintf("%.0f%%", metrics$overall_accuracy * 100),
    Precision = sprintf("%.0f%%", metrics$precision * 100),
    F1_Score = sprintf("%.0f%%", metrics$f1_score * 100)
  )
  return (metrics_dataframe)
}
```

### def combine_classification_results

The `combine_classification_results` function is designed to unify performance metrics from multiple classification models into a single, cohesive dataframe. It accepts two parameters: a list of dataframes (`accuracy_dataframe_list`), each containing accuracy metrics for a different model, and a string (`data_combination`) describing the dataset combination (e.g., "Withdraw + Deposit"). The function uses `lapply` to iterate over each dataframe in the list, adding a `Data_Combination` column that records the dataset combination description for that model's metrics. Once each dataframe is labeled, `do.call(rbind, accuracy_dataframe_list)` concatenates the dataframes by rows, resulting in a single combined dataframe with all models’ metrics and an additional column indicating the dataset combination.

```{r}
combine_classification_results <- function(accuracy_dataframe_list, data_combination) {
  # apply the data combination description to each dataframe in the list
  accuracy_dataframe_list <- lapply(accuracy_dataframe_list, function(df) {
    # add a new column `Data_Combination` to store the combination description
    # this allows each dataframe to retain information about the specific data combination it
    # represents
    df$Data_Combination <- data_combination
    # return the modified dataframe with the new column added
    return(df)
  })
  
  # combine all the modified dataframes into one large dataframe
  # `do.call` applies `rbind` to all dataframes in the list, effectively stacking them by rows
  combined_dataframe <- do.call(rbind, accuracy_dataframe_list)
  
  # return the combined dataframe
  return(combined_dataframe)
}
```

### def get_percentage

The `get_percentage` function takes a dataset (`survivalDataForClassification`), along with two event labels (`indexEvent` and `outcomeEvent`), and calculates the percentage distribution of different event types within the dataset. It groups the data by the event variable, counts occurrences for each event type, and then calculates the total and corresponding percentage for each event. The function then creates a bar plot to visually display the percentage of each event type, labeling the bars with the percentage values and showing the y-axis as a percentage. The plot is titled according to the provided `indexEvent` and `outcomeEvent` labels.

```{r}
get_percentage <- function(survivalDataForClassification, indexEvent, outcomeEvent) {
  # indexEvent and outcomeEvent is a string type
  pctPerEvent <- survivalDataForClassification %>%
  group_by(event) %>%
  dplyr::summarize(numPerEvent = n()) %>%
  mutate(total = sum(numPerEvent)) %>%
  mutate(percentage = numPerEvent / total) %>%
  dplyr::select(event, percentage)
  # create a bar plot for event percentages
  # stat = "identity": percentages used directly to draw the bar chart
  print(ggplot(pctPerEvent, aes(x = event, y = percentage, fill = event)) +
    geom_bar(stat = "identity") +
    scale_y_continuous(labels = scales::percent_format()) +  # show y-axis in percentage
    labs(title = "Percentage of Events: 'Yes' event vs 'No' event",
         x = paste(indexEvent, "and", outcomeEvent),
         y = "Percentage") +
    geom_text(aes(label = scales::percent(percentage)), 
              vjust = -0.5, size = 3.5) +  # show percentages on top of bars
    theme_minimal())
}
```

### def accuracy_comparison_plot

The `accuracy_comparison_plot` function generates a faceted bar chart comparing the accuracy metrics across multiple classification models. It accepts a list of lists (`metrics_list`), where each sublist contains a model's metrics and its name, enabling flexible scaling to accommodate any number of models. The function iterates through each model’s metrics, constructing a consolidated dataframe that includes metrics like class accuracy, negative class accuracy, balanced accuracy, overall accuracy, precision, and F1 score for each model. Using `ggplot2`, it melts this data for visualization and creates a bar plot with facets for each metric, complete with percentage labels on the bars. 

```{r}
accuracy_comparison_plot <- function(metrics_list) {
  # initialize an empty data frame to store the metrics for all models
  accuracy_table <- data.frame()
  
  # loop over each element in metrics_list (each element is a list containing metrics and model name)
  for (metrics in metrics_list) {
    # Extract metrics and model name from each "tuple"
    model_metrics <- metrics[[1]]
    model_name <- metrics[[2]]
    
    # create a temporary dataframe for this model
    temp_df <- data.frame(
      Model = model_name,
      ClassA = model_metrics$class_accuracy,
      Negative_1A = model_metrics$negative_1_accuracy,
      BalancedA = model_metrics$balanced_accuracy,
      OverallA = model_metrics$overall_accuracy,
      Precision = model_metrics$precision,
      F1_score = model_metrics$f1_score
    )
    
    # append the temporary dataframe to the main accuracy_table
    accuracy_table <- rbind(accuracy_table, temp_df)
  }
  
  # melt the dataframe into long format for plotting
  accuracy_results_melted <- reshape2::melt(accuracy_table, id.vars = "Model")
  
  # generate the plot with faceted bars
  ggplot(accuracy_results_melted, aes(x = Model, y = value, fill = Model)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ variable, scales = "free_y") +  # Facet by each metric
    labs(title = "Comparison of Accuracy Metrics Across Models",
         x = "Model",
         y = "Value") +
    # add percentage labels on top of each bar
    geom_text(aes(label = scales::percent(value, accuracy = 0.1)),
              position = position_dodge(width = 0.9),
              vjust = 0.5, size = 2.0) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

### def specific_accuracy_statistics

The `specific_accuracy_statistics` function generates a summary DataFrame containing specified accuracy metrics for various classification models. It takes three parameters: `event_pair` (a string describing the event or dataset, which is added as the first column), `accuracy_type` (a string specifying the accuracy metric to extract, such as "class_accuracy" or "f1_score"), and `metrics_list` (a list of lists where each sub-list contains accuracy data and a model name). The function traverses `metrics_list` and dynamically extracts the specified accuracy metric, converting it to a percentage (multiplied by 100) and rounding to one decimal place for each model. If the requested accuracy type is invalid, it throws an error. Finally, it organizes the extracted data into a DataFrame, with `accuracy_type` in the first column and each model's results as additional columns, and returns this structured DataFrame.

```{r}
specific_accuracy_statistics <- function(event_pair, accuracy_type, metrics_list) {
  # initialize the result list
  results <- list()
  
  # add accuracy_type as the left front corner and event_pair is the first column of the data
  results[[accuracy_type]] <- event_pair
  
  # traverse metrics_list and extract the specified accuracy type for each model
  for (metric_item in metrics_list) {
    # get accuracy data
    accuracy_data <- metric_item[[1]]
    # get the model name
    model_name <- metric_item[[2]]
    if (accuracy_type == "class_accuracy") {
      results[[model_name]] <- round(accuracy_data$class_accuracy * 100, 1)
    }
    else if (accuracy_type == "negative_1_accuracy") {
      results[[model_name]] <- round(accuracy_data$negative_1_accuracy * 100, 1)
    }
    else if (accuracy_type == "balanced_accuracy") {
      results[[model_name]] <- round(accuracy_data$balanced_accuracy * 100, 1)
    }
    else if (accuracy_type == "overall_accuracy") {
      results[[model_name]] <- round(accuracy_data$overall_accuracy * 100, 1)
    }
    else if (accuracy_type == "precision") {
      results[[model_name]] <- round(accuracy_data$precision * 100, 1)
    }
    else if (accuracy_type == "f1_score") {
      results[[model_name]] <- round(accuracy_data$f1_score * 100, 1)
    }
    else {
      # An error message is displayed if the specified accuracy_type does not exist.
      stop(paste("Invalid accuracy type:", accuracy_type))
    }
  }
  
  # convert the result to a DataFrame and set row.names = NULL
  df <- as.data.frame(results, row.names = NULL)
  return(df)
}
```

### def combine_accuracy_dataframes

The `combine_accuracy_dataframes` function takes a list of data frames as input and combines them into a single data frame by stacking the rows. It first checks if the input is a valid list and throws an error if not. Then, using `do.call` and `rbind`, it merges all the data frames in the list row-wise. The function is designed to handle multiple data frames efficiently and returns a consolidated data frame for further analysis.

```{r}
combine_accuracy_dataframes <- function(df_list) {
  # check if the input is a list
  if (!is.list(df_list)) {
    stop("Input must be a list of data.frames.")
  }
  
  # Use do.call and rbind to combine all data.frames in a list.
  combined_df <- do.call(rbind, df_list)
  
  # returns the merged data.frame
  return(combined_df)
}
```

## Classification Model Development and Optimization

### def logistic_regression

This function implements a logistic regression classifier using Lasso regularization for binary classification. It first loads the required packages (`glmnet` for logistic regression with regularization and `data.table` for efficient data handling). The function scales the numeric features in both training and testing datasets and converts them to matrix format for compatibility with the `glmnet` package. Logistic regression with Lasso regularization is then applied, and predictions are made on the test set based on a fixed regularization parameter (`lambda = 0.01`). The function converts predicted probabilities into binary class labels, creates a confusion matrix to compare predictions with actual outcomes, and evaluates the model's performance through various metrics like accuracy, precision, recall, and F1 score. These metrics are summarized into a dataframe for easy visualization and returned.

```{r}
logistic_regression <- function(train_data, test_data) {
  # library(glmnet)  # load glmnet package for logistic regression with regularization
  # library(data.table)  # load data.table for efficient data handling
  # ensure train_data and test_data are in the data.table format for fast operations
  setDT(train_data)
  setDT(test_data)
  # identify numeric features in the dataset, and scale them to have mean = 0 and standard
  # deviation = 1
  numeric_features <- names(train_data)[sapply(train_data, is.numeric)]
  # scale numeric columns in train set
  train_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # scale numeric columns in test set
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # convert the training and testing datasets to matrix format as required by the glmnet package
  # model matrix function excludes the intercept (-1) and converts data for glmnet
  x_train <- model.matrix(event ~ . - 1, data = train_data)  
  y_train <- train_data$event  # extract the target variable from the training data
  # convert test set features to matrix format
  x_test <- model.matrix(event ~ . - 1, data = test_data)  
  # apply logistic regression with Lasso regularization (alpha = 1 means Lasso)
  # 'family = binomial' specifies logistic regression for binary classification
  logistic_regression_classifier <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
  # predict the probability of the event (outcome) on the test set
  # use a fixed regularization parameter lambda = 0.01 for prediction
  predict_probabilities_lr <- 
    predict(logistic_regression_classifier, s = 0.01, newx = x_test, type = "response")
  # convert the predicted probabilities into binary class labels (yes or no)
  binary_prediction_lr <- ifelse(predict_probabilities_lr > 0.5, "yes", "no")
  # create a confusion matrix to compare predicted vs. actual outcomes in the test set
  confusion_matrix_lr <- table(Predicted = binary_prediction_lr, Actual = test_data$event)
  # evaluate model performance by calculating metrics such as accuracy, precision, recall, etc.
  metrics_lr <- 
    calculate_model_metrics(confusion_matrix_lr, predict_probabilities_lr, "Logistic regression")
  # create a dataframe with the desired structure
  metrics_lr_dataframe = get_dataframe("Logistic Regression", metrics_lr)
  return (list(metrics_lr_dataframe = metrics_lr_dataframe, metrics_lr = metrics_lr))
}
```

### def decision_tree

This function implements a decision tree classifier for binary classification using the `rpart` package. It trains a decision tree model on the training dataset with hyperparameter tuning, including a complexity parameter for pruning, a maximum tree depth of 30, and a minimum split requirement of 20 observations per node. After training, the function predicts class labels on the testing dataset and creates a confusion matrix to evaluate the model's performance. Various metrics, such as class accuracy, balanced accuracy, precision, and F1 score, are calculated using the `calculate_model_metrics` function and presented in a structured dataframe. The metrics dataframe is printed and returned as part of the function output.

```{r}
decision_tree <- function(train_data, test_data) {
  # library(rpart)
  # train the decision tree model with hyperparameter tuning
  decision_tree_classifier <- rpart(
    event ~ .,
    data = train_data,
    method = "class",
    control = rpart.control(
      # complexity parameter for pruning
      cp = 0.01,
      # maximum depth of the tree
      maxdepth = 30,
      # minimum number of observations needed to split a node
      minsplit = 20
    )
  )
  # predict on the testing dataset
  predict_probabilities_dt <- predict(decision_tree_classifier, test_data, type = "class")
  # confusion matrix and metrics
  confusion_matrix_dt <- table(Predicted = predict_probabilities_dt, Actual = test_data$event)
  metrics_dt <- calculate_model_metrics(confusion_matrix_dt, predict_probabilities_dt, 
                                        "Decision tree")
  # create a dataframe with the desired structure
  metrics_dt_dataframe = get_dataframe("Decision Tree", metrics_dt)
  return (list(metrics_dt_dataframe = metrics_dt_dataframe, metrics_dt = metrics_dt))
}
```

### def Naive_bayes

(Naive Bayes, relying on the assumption of feature independence, struggles to capture the intricate relationships and dependencies between features that are common in survival data. This simplification leads to inaccurate probability estimations and poor classification performance, especially in scenarios with overlapping or correlated features. Additionally, the model's inability to adjust effectively to imbalanced datasets results in low balanced accuracy and F1 scores, as it tends to overemphasize the majority class while failing to adequately recognize minority class instances. These limitations make Naive Bayes unsuitable for predicting survival data outcomes.)

The `Naive_bayes` function implements a Naive Bayes classifier for binary classification tasks. It begins by converting the target column (`event`) in the training and testing datasets to factors to ensure proper model functionality. The function trains a Naive Bayes model on the training dataset using all features except the target column. Predictions and prediction probabilities are then generated for the test set, ensuring consistency in factor levels between predicted and actual labels. A confusion matrix is created to evaluate the model’s performance, and key metrics such as accuracy, precision, recall, and F1 score are calculated using the `calculate_model_metrics` function. The metrics are formatted into a structured dataframe.

```{r}
Naive_bayes <- function(train_data, test_data) {
  # library(e1071)
  target_column = "event"
  # Convert the target column to a factor if it's not already
  train_data[[target_column]] <- as.factor(train_data[[target_column]])
  test_labels <- as.factor(test_data[[target_column]])
  
  # Remove the target column from the test set for prediction
  test_features <- test_data %>%
    select(-all_of(target_column))
  
  # Train Naive Bayes model
  nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_data)
  
  # Make predictions on the test set
  predictions <- predict(nb_model, test_features)
  
  # Get prediction probabilities
  prediction_probabilities <- predict(nb_model, test_features, type = "raw")
  
  # Ensure both predicted and actual labels are factors with the same levels
  predictions <- factor(predictions, levels = levels(test_labels))
  
  # Evaluate model performance with a confusion matrix
  conf_matrix <- table(Predicted = predictions, Actual = test_labels)
  
  metrics <- calculate_model_metrics(conf_matrix, prediction_probabilities, 
                                     "Naive Bayes")
  # create a dataframe with the desired structure
  metrics_dataframe = get_dataframe("Naive Bayes", metrics)
  # each classification models need to return these two variables
  return (list(metrics_dataframe = metrics_dataframe, metrics = metrics))
}
```

### def XG_Boost

The `XG_Boost` function implements an **XGBoost classifier** to predict binary outcomes using **gradient boosting decision trees**. The function first **preprocesses the dataset** by converting it into `data.table` format for faster operations, standardizing numeric features, and encoding event labels as **0 (no) and 1 (yes)**. It then **trains an XGBoost model with 100 boosting iterations**, using logistic regression as the objective function for binary classification. The trained model generates **probability-based predictions**, which are converted into **binary classifications** using a **threshold of 0.5**. Finally, the function evaluates the model’s performance using **a confusion matrix and key classification metrics** (accuracy, precision, recall, and F1-score) and returns the results in a structured dataframe.

```{r}
XG_Boost <- function(train_data, test_data) {
  # Load required libraries
  # library(xgboost)   # XGBoost for gradient boosting
  # library(data.table)  # Efficient data handling with data.table
  
  # Convert train_data and test_data to data.table format for optimized processing
  setDT(train_data)
  setDT(test_data)
  
  # Identify numeric features in the dataset for standardization
  numeric_features <- names(train_data)[sapply(train_data, is.numeric)]

  # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
  train_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert data to matrix format, as required by XGBoost
  x_train <- model.matrix(event ~ . - 1, data = train_data)  # Feature matrix for training
  y_train <- as.numeric(train_data$event == "yes")  # Convert event labels to binary format (0 = no, 1 = yes)
  x_test <- model.matrix(event ~ . - 1, data = test_data)  # Feature matrix for testing
  
  # Train an XGBoost model with default hyperparameters
  # - nrounds = 100: Number of boosting iterations
  # - objective = "binary:logistic": Binary classification using logistic regression
  # - verbose = 0: Suppress log output to keep console clean
  xgb_model <- xgboost(data = x_train, 
                       label = y_train, 
                       nrounds = 100, 
                       objective = "binary:logistic", 
                       verbose = 0)

  # Generate probability predictions for the test dataset
  predict_probabilities_xgb <- predict(xgb_model, x_test)
  
  # Convert probability predictions into binary class labels (yes/no) using a threshold of 0.5
  binary_prediction_xgb <- ifelse(predict_probabilities_xgb > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  confusion_matrix_xgb <- table(Predicted = binary_prediction_xgb, Actual = test_data$event)
  
  # Evaluate model performance using key classification metrics (accuracy, precision, recall, F1-score)
  metrics_xgb <- calculate_model_metrics(confusion_matrix_xgb, predict_probabilities_xgb, "XGBoost")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_xgb_dataframe <- get_dataframe("XGBoost", metrics_xgb)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_xgb_dataframe = metrics_xgb_dataframe, metrics_xgb = metrics_xgb))
}
```

### def GBM

The `GBM` function implements a **Gradient Boosting Machine (GBM)** model to predict binary outcomes using decision trees. It begins by **preprocessing the data**, converting it to `data.table` format for efficient handling, standardizing numeric features, and encoding the event variable as **0 (no) or 1 (yes)**. The function then trains a GBM model with **100 boosting iterations and a tree depth of 3**, which balances model complexity and overfitting risks. After training, it predicts **probabilities** on the test dataset, converts them into **binary classifications**, and evaluates the model’s performance using a confusion matrix and key classification metrics.

```{r}
GBM <- function(train_data, test_data) {
  # Load required libraries
  # library(gbm)         # GBM package for gradient boosting
  # library(data.table)  # Efficient data handling with data.table
  
  # Convert train_data and test_data to data.table format for optimized processing
  setDT(train_data)
  setDT(test_data)

  # Identify numeric features in the dataset for standardization
  numeric_features <- names(train_data)[sapply(train_data, is.numeric)]

  # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
  train_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]

  # Convert the target variable (event) into a numeric format for binary classification
  # "yes" → 1 (positive event), "no" → 0 (negative event)
  train_data[, event := ifelse(event == "yes", 1, 0)]
  test_data[, event := ifelse(event == "yes", 1, 0)]

  # Train the GBM model using gradient boosting with default hyperparameters
  # - event ~ .: Predict the event variable using all available features.
  # - data = train_data: Use the preprocessed training dataset for model training.
  # - distribution = "bernoulli": Use Bernoulli distribution for binary classification (0/1 outcome).
  # - n.trees = 100: Number of boosting iterations (reduced from 500 to speed up training).
  # - interaction.depth = 3: Maximum depth of each tree (limits complexity to prevent overfitting).
  gbm_model <- gbm(event ~ ., 
                    data = train_data, 
                    distribution = "bernoulli", 
                    n.trees = 100, 
                    interaction.depth = 3)

  # Generate probability predictions on the test dataset using the trained model
  predict_probabilities_gbm <- predict(gbm_model, test_data, n.trees = 100, type = "response")

  # Convert probability predictions into binary class labels (yes/no) using a threshold of 0.5
  binary_prediction_gbm <- ifelse(predict_probabilities_gbm > 0.5, "yes", "no")

  # Create a confusion matrix to compare predictions vs. actual outcomes
  confusion_matrix_gbm <- table(Predicted = binary_prediction_gbm, Actual = test_data$event)

  # Evaluate model performance using key metrics: accuracy, precision, recall, and F1-score
  metrics_gbm <- calculate_model_metrics(confusion_matrix_gbm, predict_probabilities_gbm, "GBM")

  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_gbm_dataframe <- get_dataframe("GBM", metrics_gbm)

  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_gbm_dataframe = metrics_gbm_dataframe, metrics_gbm = metrics_gbm))
}
```

### def elastic_net

The `elastic_net` function implements an **Elastic Net Classifier**, **regularized logistic regression model** that combines **Lasso (L1) and Ridge (L2) penalties** for improved generalization. The function first preprocesses the dataset by standardizing numeric features and converting the data into a matrix format, as required by the `glmnet` package. The Elastic Net model is then trained with **α = 0.5**, meaning it applies an equal mix of Lasso and Ridge regularization. The function predicts event probabilities for the test dataset, converts them into binary classifications, and evaluates performance using accuracy, precision, recall, and F1-score.

```{r}
elastic_net <- function(train_data, test_data) {
    # Load required libraries
    # library(glmnet) # Required for Elastic Net (Lasso + Ridge regularization)
    # library(data.table) # For efficient data handling using data.table
    
    # Convert train_data and test_data to data.table format for optimized processing
    setDT(train_data)
    setDT(test_data)
    
    # Identify numeric features in the dataset for standardization
    numeric_features <- names(train_data)[sapply(train_data, is.numeric)]

    # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
    train_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
    test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
    
    # Convert the dataset into a matrix format, as required by glmnet
    x_train <- model.matrix(event ~ . - 1, data = train_data) # Feature matrix for training
    y_train <- train_data$event # Target variable
    x_test <- model.matrix(event ~ . - 1, data = test_data) # Feature matrix for testing
    
    # Train the Elastic Net model with a combination of Lasso (L1) and Ridge (L2) regularization
    # alpha = 0.5 sets an equal mix of Lasso and Ridge penalties
    elastic_net_model <- glmnet(x_train, y_train, family = "binomial", alpha = 0.5)

    # Predict event probabilities for the test dataset
    # s = 0.01 sets a specific regularization strength for prediction
    predict_probabilities_en <- predict(elastic_net_model, s = 0.01, newx = x_test, type = "response")

    # Convert probability predictions into binary class labels (yes/no) using a threshold of 0.5
    binary_prediction_en <- ifelse(predict_probabilities_en > 0.5, "yes", "no")
    
    # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
    confusion_matrix_en <- table(Predicted = binary_prediction_en, Actual = test_data$event)
    
    # Evaluate model performance using key classification metrics (accuracy, precision, recall, F1-score)
    metrics_en <- calculate_model_metrics(confusion_matrix_en, predict_probabilities_en, "Elastic Net")
    
    # Store the calculated metrics in a structured dataframe for easy comparison
    metrics_en_dataframe <- get_dataframe("Elastic Net", metrics_en)
    
    # Return both the detailed metrics list and the formatted dataframe
    return (list(metrics_en_dataframe = metrics_en_dataframe, metrics_en = metrics_en))
    }
```

### def XG_Boost_optimization (NOT USED)

The `XG_Boost_optimization` function implements an **XGBoost classifier for binary classification**, leveraging gradient boosting decision trees. The function begins by preprocessing the dataset, standardizing numeric features, and converting categorical labels into binary format (0/1). It then trains an optimized XGBoost model using parallel processing, **tree depth of 6**, **learning rate of 0.1**, and an **80%** feature and row subsampling strategy to enhance generalization. The training process includes early stopping, which halts training if performance does not improve for **10 rounds**. After training, the function makes probability-based predictions, converts them into binary classifications, and evaluates the model using accuracy, precision, recall, and F1-score.

```{r}
XG_Boost_optimization <- function(train_data, test_data) {
  # Load required libraries
  # library(xgboost)  # XGBoost for gradient boosting
  # library(data.table)  # Efficient data handling
  
  # Convert train_data and test_data to data.table format
  setDT(train_data)
  setDT(test_data)
  
  # Identify numeric features and scale them
  numeric_features <- names(train_data)[sapply(train_data, is.numeric)]
  train_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert data to matrix format required by XGBoost
  x_train <- model.matrix(event ~ . - 1, data = train_data)
  y_train <- as.numeric(train_data$event == "yes")  # Convert event labels to 0/1
  x_test <- model.matrix(event ~ . - 1, data = test_data)

  # Convert to DMatrix format, which is optimized for XGBoost
  dtrain <- xgb.DMatrix(data = x_train, label = y_train)
  dtest <- xgb.DMatrix(data = x_test)
  
  # Detect available CPU cores for parallel computation
  num_cores <- detectCores()

  # Define XGBoost hyperparameters
  # XGBoost Hyperparameter Configuration:
  # - objective = "binary:logistic": Defines a binary classification task with logistic regression loss.
  # - eval_metric = "logloss": Uses log-loss as the evaluation metric to measure prediction accuracy.
  # - max_depth = 6: Sets the maximum depth of each tree (higher values increase model complexity).
  # - eta = 0.1: Defines the learning rate (lower values prevent overfitting but require more trees).
  # - subsample = 0.8: Randomly selects 80% of the data per boosting iteration to improve generalization.
  # - colsample_bytree = 0.8: Uses 80% of features for each tree to reduce overfitting.
  # - nthread = num_cores: Utilizes all available CPU cores to speed up training.
  params <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = 6,
    eta = 0.1,
    subsample = 0.8,
    colsample_bytree = 0.8,
    nthread = num_cores
  )

  # Train XGBoost model with early stopping
  # XGBoost Model Training Strategy:
  # - xgb.train(): Trains the model with the defined hyperparameters.
  # - nrounds = 200: Runs 200 boosting iterations to enhance learning.
  # - early_stopping_rounds = 10: Stops training if log-loss does not improve for 10 consecutive rounds.
  # - watchlist = list(train = dtrain): Monitors training performance to optimize stopping criteria.
  # - verbose = 0: Suppresses training logs for a cleaner output.
  xgb_model <- xgb.train(params = params,
                         data = dtrain,
                         nrounds = 200,
                         early_stopping_rounds = 10,
                         watchlist = list(train = dtrain),
                         verbose = 0)


  # Predict probabilities on the test dataset
  predict_probabilities_xgb <- predict(xgb_model, dtest)
  
  # Convert predicted probabilities into binary class labels (yes/no) using a threshold of 0.5
  binary_prediction_xgb <- ifelse(predict_probabilities_xgb > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes
  confusion_matrix_xgb <- table(Predicted = binary_prediction_xgb, Actual = test_data$event)
  
  # Evaluate model performance using accuracy, precision, recall, and F1-score
  metrics_xgb <- calculate_model_metrics(confusion_matrix_xgb, predict_probabilities_xgb, "XGBoost")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_xgb_dataframe <- get_dataframe("XGBoost", metrics_xgb)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_xgb_dataframe = metrics_xgb_dataframe, metrics_xgb = metrics_xgb))
}
```

### def GBM_optimization (NOT USED)

The `GBM_optimization` function trains a **Gradient Boosting Machine (GBM)** model to predict binary outcomes using an optimized set of hyperparameters. It first preprocesses the data by standardizing numeric features and converting categorical event labels into binary (0/1). The function then detects available CPU cores to enable parallel computation, improving training efficiency. The GBM model is trained with **300 trees, a maximum depth of 3, a learning rate of 0.01, and 80% data sampling per iteration** to balance speed and accuracy while reducing overfitting. Cross-validation (`cv.folds = 3`) is used to automatically determine the optimal number of trees. Finally, the function generates predictions on the test dataset, evaluates performance using key metrics such as accuracy, precision, recall, and F1-score, and returns a structured results dataframe.

```{r}
GBM_optimization <- function(train_data, test_data) {
  # Load required libraries
  # library(gbm)         # Load GBM package for gradient boosting modeling
  # library(data.table)  # Load data.table for efficient data handling
  # library(parallel)    # Load parallel to utilize multi-core processing

  # Convert train_data and test_data to data.table format for fast operations
  setDT(train_data)
  setDT(test_data)

  # Identify numeric features in the dataset for scaling
  numeric_features <- names(train_data)[sapply(train_data, is.numeric)]

  # Standardize numeric columns (mean = 0, standard deviation = 1)
  train_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]

  # Convert the target variable (event) into numeric format (Binary: 0 = "no", 1 = "yes")
  train_data[, event := ifelse(event == "yes", 1, 0)]
  test_data[, event := ifelse(event == "yes", 1, 0)]

  # Detect available CPU cores for parallel processing (to speed up GBM training)
  num_cores <- detectCores()

  # Train GBM model with optimized hyperparameters for better speed and performance
  # GBM Model Configuration:
  # - event ~ .: Predict the event variable using all available features.
  # - data = train_data: Use the preprocessed training dataset for model training.
  # - distribution = "bernoulli": Use Bernoulli distribution for binary classification (0/1 outcome).
  # - n.trees = 300: Number of boosting iterations (reduced from 500 to speed up training).
  # - interaction.depth = 3: Maximum depth of each tree (limits complexity to prevent overfitting).
  # - shrinkage = 0.01: Learning rate: lower values reduce overfitting but require more trees.
  # - bag.fraction = 0.8: Fraction of training data used in each iteration (adds randomness).
  # - train.fraction = 0.8: Use 80% of the dataset for training, leaving 20% for validation.
  # - cv.folds = 3: 3-fold cross-validation to optimize the number of trees.
  # - n.cores = num_cores: Enable parallel computation using all available CPU cores.
  gbm_model <- gbm(event ~ .,  
                 data = train_data, 
                 distribution = "bernoulli",  
                 n.trees = 300,  
                 interaction.depth = 3,  
                 shrinkage = 0.01,  
                 bag.fraction = 0.8,  
                 train.fraction = 0.8,  
                 cv.folds = 3,  
                 n.cores = num_cores)

  # Automatically determine the optimal number of trees using cross-validation
  best_trees <- gbm.perf(gbm_model, method = "cv", plot.it = FALSE) # Avoid plotting to speed up processing

  # Generate probability predictions on the test dataset using the optimal number of trees
  predict_probabilities_gbm <- predict(gbm_model, test_data, n.trees = best_trees, type = "response")

  # Convert probability predictions into binary class labels (yes/no) using a threshold of 0.5
  binary_prediction_gbm <- ifelse(predict_probabilities_gbm > 0.5, "yes", "no")

  # Create a confusion matrix to compare model predictions against actual test outcomes
  confusion_matrix_gbm <- table(Predicted = binary_prediction_gbm, Actual = test_data$event)

  # Calculate model performance metrics (accuracy, precision, recall, F1-score, etc.)
  metrics_gbm <- calculate_model_metrics(confusion_matrix_gbm, predict_probabilities_gbm, "GBM")

  # Store the calculated metrics in a structured dataframe for easier comparison with other models
  metrics_gbm_dataframe <- get_dataframe("GBM", metrics_gbm)

  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_gbm_dataframe = metrics_gbm_dataframe, metrics_gbm = metrics_gbm))
}
```
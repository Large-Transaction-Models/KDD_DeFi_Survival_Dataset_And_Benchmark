---
title: "DMLR DeFi Survival Data Pipeline"
author: "Hanzhen Qin(qinh2)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: true
    number_sections: true
    df_print: paged
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
# Check and install required R packages
if (!require("conflicted")) {
  install.packages("conflicted", dependencies = TRUE)
  library(conflicted)
}

# Set default CRAN repository
local({
  r <- getOption("repos")
  r["CRAN"] <- "http://cran.r-project.org"
  options(repos = r)
})

# Define the list of required packages
required_packages <- c(
  "rmarkdown", "tidyverse", "stringr", "ggbiplot", "pheatmap", 
  "caret", "survival", "survminer", "ggplot2", 
  "kableExtra", "rpart", "glmnet", "data.table", "reshape2", 
  "pander", "readr", "dplyr", "e1071", "ROSE", "xgboost", "parallel"
)

# Loop through the package list and install missing packages
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}

# Handle function name conflicts
conflict_prefer("slice", "dplyr")
conflict_prefer("filter", "dplyr")

# Set knitr options for R Markdown
knitr::opts_chunk$set(echo = TRUE)

# Rename dplyr functions to avoid conflicts with other packages
select <- dplyr::select
rename <- dplyr::rename
summarize <- dplyr::summarize
group_by <- dplyr::group_by
```

# Survival Data Pipeline

## Pipeline Summary

* Summary of work
  
  * Data preprocessing: `data_processing`, `get_train_test_data`, `smote_data`, 
  `get_classification_cutoff`
  
  * Model Performance Evaluation and Visualization : `calculate_model_metrics`, `get_dataframe`,
  `combine_classification_results`, `accuracy_comparison_plot`, `get_percentage`, 
  `specific_accuracy_statistics`, `combine_accuracy_dataframes`
  
  * Classification model: `logistic_regression`, `decision_tree`, `naive_bayes`, `XG_Boost`, 
  `elastic_net`
  

## Data Preprocessing

### def get_train_test_data

The `get_train_test_data` function is designed to get the training and test data based on the `indexEvent` and `outcomeEvent` parameters, but the specific logic has not yet been implemented. The function references the external script `dataLoader.r` through `source()` to load the latest processed survival data, and returns the train and test data sets directly by default.

```{r}
get_train_test_data <- function(indexEvent, outcomeEvent) {
  source("~/DMLR_DeFi_Survival_Dataset_And_Benchmark/source/dataLoader.R")
}
```

### def data_processing

The `data_processing` function is designed to transform a survival analysis dataset into a format suitable for classification tasks by performing a series of preprocessing steps. It begins by filtering out invalid records where `timeDiff` is less than or equal to 0, ensuring only valid observations are retained. Then, it removes records where `timeDiff` falls within a specified threshold (`set_timeDiff`) but no event occurred (`status == 0`). A new binary column, `event`, is created to indicate whether an event occurred within the threshold ("yes" or "no"). The function drops unnecessary columns as defined in a predefined list, ensuring only relevant features remain, and removes columns entirely populated with missing values. To handle missing data, numeric columns are filled with `-999` to mark missing values clearly without distorting numerical distributions, and categorical columns are filled with `"missing"` to create a distinct category for absent data. Finally, character columns are converted to factors, making them ready for classification models.

```{r}
data_processing <- function(survivalData, set_timeDiff) {
  # filter out invalid records where `timeDiff` is <= 0 early
  survivalData <- survivalData %>% filter(timeDiff > 0)
  
  # filter out records based on the `set_timeDiff` threshold and `status`
  survivalData <- survivalData %>% filter(!(timeDiff / 86400 <= set_timeDiff & status == 0))
  
  # create a new binary column `event` based on `timeDiff`
  survivalDataForClassification <- survivalData %>%
    mutate(event = case_when(
      timeDiff / 86400 <= set_timeDiff ~ "yes",
      timeDiff / 86400 > set_timeDiff ~ "no"
    ))
  
  # define features to drop
  # featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
  #                     "timeDiff", "deployment", "version", "indexID", "user", "status", 
  #                     "quarter", "liquidator", "userFlashloanAvgAmount", "userReserveMode",
  #                     "reserve", "pool", "timestamp", "type", "datetime", "quarter_start_date",
  #                     "userCoinTypeMode", "coinType", "userIsNew", "userDepositSum", 
  #                     "userDepositSumUSD", "userDepositAvgAmountUSD", "userDepositSumETH",
  #                     "userDepositAvgAmountETH", "userWithdrawSum", "userWithdrawSumUSD",
  #                     "userWithdrawAvgAmountUSD", "userWithdrawSumETH", 
  #                     "userWithdrawAvgAmountETH", "userBorrowSum", "userBorrowSumUSD", 
  #                     "userBorrowAvgAmountUSD", "userBorrowSumETH", 
  #                     "userBorrowAvgAmountETH", "userRepaySum", "userRepaySumUSD", 
  #                     "userRepayAvgAmountUSD", "userRepaySumETH", "userRepayAvgAmountETH", 
  #                     "userFlashloanSum", "userLiquidationSum", "userLiquidationSumUSD",
  #                     "userLiquidationAvgAmountUSD", "userLiquidationSumETH", 
  #                     "userLiquidationAvgAmountETH", "userFlashloanCount", "priceInUSD")
  
  featuresToDrop <- c("indexTime", "outcomeTime", "id", "Index Event", "Outcome Event",
                      "timeDiff", "status", "deployment", "version", "indexID", "user", 
                      "liquidator", "pool", "timestamp", "type", "datetime", "quarter_start_date")
  
  # remove only columns that actually exist in the dataset
  featuresToDrop <- intersect(featuresToDrop, colnames(survivalDataForClassification))
  
  survivalDataForClassification <- survivalDataForClassification %>%
    # drop unnecessary columns
    select(-any_of(featuresToDrop)) %>%
    # remove columns with only NA values
    select(where(~ !all(is.na(.)))) %>%
    # replace NA in numeric columns with -999
    mutate(across(where(is.numeric), ~ replace_na(., -999))) %>%
    # replace NA in character columns with "missing"
    mutate(across(where(is.character), ~ replace_na(., "missing"))) %>%
    # convert character columns to factors
    mutate(across(where(is.character), as.factor))
  
  # return the processed dataset
  return(survivalDataForClassification)
}
```

### def get_classification_cutoff (@Author: Aaron Green)

The `get_classification_cutoff` function retrieves the `ConvergedRMST_5` value from a predefined dataset based on the specified `indexEvent` and `outcomeEvent`. The dataset contains information about various index events (e.g., "Borrow", "Deposit") and their corresponding outcome events (e.g., "Full Repay", "Withdraw"), along with convergence metrics like `ConvergedTau` and `ConvergedRMST` at different time horizons. The function filters the dataset by matching the input events with the respective `IndexEvent` and `OutcomeEvent` columns and returns the relevant `ConvergedRMST_5` value, which represents a specific metric of convergence over a 5-unit time period. Return the optimal cutoff value for the `data_processing` function.

```{r}
get_classification_cutoff <- function(indexEvent, outcomeEvent){
  # library(stringr)
  # Create the dataframe
  data <- data.frame(
    IndexEvent = c("Borrow", "Borrow", "Borrow", "Borrow", "Borrow", "Borrow", 
                   "Deposit", "Deposit", "Deposit", "Deposit", "Deposit",
                   "Repay", "Repay", "Repay", "Repay", "Repay",
                   "Withdraw", "Withdraw", "Withdraw", "Withdraw", "Withdraw"),
    OutcomeEvent = c("Account Liquidated", "Deposit", "Full Repay", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Liquidation Performed", "Repay", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Withdraw",
                     "Account Liquidated", "Borrow", "Deposit", "Liquidation Performed", "Repay"),
    ConvergedTau_1 = c(91, 99, 95, 101, 69, 99, 
                       90, 100, 101, 99, 93, 
                       93, 95, 100, 101, 100, 
                       94, 101, 99, 101, 100),
    ConvergedRMST_1 = c(69.36, 90.98, 74.12, 100.88, 28.71, 92.59, 
                        68.64, 91.58, 100.95, 89.70, 64.84, 
                        72.56, 63.23, 93.72, 100.86, 93.93,
                        75.10, 96.01, 82.13, 100.88, 93.82),
    ConvergedTau_5 = c(20, 21, 20, 21, 17, 21, 
                       20, 21, 21, 21, 20, 
                       20, 20, 21, 21, 21,
                       20, 21, 21, 21, 21),
    ConvergedRMST_5 = c(17.43, 19.88, 17.15, 20.99, 10.24, 20.21, 
                        17.52, 19.73, 20.99, 19.69, 15.67, 
                        17.43, 14.99, 20.13, 20.99, 20.15,
                        17.72, 20.26, 18.14, 20.98, 20.04),
    ConvergedTau_10 = c(11, 11, 11, 11, 10, 11, 
                        11, 11, 11, 11, 11, 
                        11, 11, 11, 11, 11,
                        11, 11, 11, 11, 11),
    ConvergedRMST_10 = c(9.94, 10.51, 9.72, 11.00, 6.66, 10.68, 
                         10.00, 10.43, 11.00, 10.44, 8.95, 
                         9.90, 8.64, 10.62, 11.00, 10.62,
                         10.05, 10.68, 9.68, 10.99, 10.57)
  )
  
  
  return(data %>% filter(IndexEvent == str_to_title(indexEvent), OutcomeEvent == str_to_title(outcomeEvent)) 
         %>% pull(ConvergedRMST_5))
}
```

### def smote_data

The `smote_data` function leverages the `ROSE` package to address class imbalance in datasets by dynamically generating a balanced dataset based on oversampling and undersampling techniques. The function is designed to be flexible, allowing the user to specify the target variable (`target_var`) and an optional random seed (`seed`) for reproducibility. It validates the input dataset to ensure the target variable exists and dynamically constructs the formula for the `ROSE` function, making it adaptable to different datasets and classification tasks. By generating a balanced dataset where the minority and majority classes are better represented, this function helps mitigate bias in machine learning models and improves their classification accuracy. 

```{r}
smote_data <- function(train_data, target_var = "event", seed = 123) {
  # library(ROSE)
  # check if the input data contains the target variable
  if (!target_var %in% colnames(train_data)) {
    stop(paste("Target variable", target_var, "not found in the dataset"))
  }
  
  # set the random seed (if provided)
  if (!is.null(seed)) {
    set.seed(seed)
  }
  
  # dynamic formula creation to adapt to different target variables
  formula <- as.formula(paste(target_var, "~ ."))
  
  # applying ROSE Balance Data
  train_data_balanced <- ROSE(formula, data = train_data, seed = seed)$data
  
  # return the balanced dataset
  return(train_data_balanced)
}
```

## Model Performance Evaluation and Visualization

### def calculate_model_metrics

The `calculate_model_metrics` function in R is designed to evaluate the performance of a binary classification model using key metrics derived from a confusion matrix. It calculates several important metrics: class accuracy (specificity), negative class accuracy (sensitivity/recall), balanced accuracy (the average of sensitivity and specificity), overall accuracy, precision, and F1 score. The function takes as input the confusion matrix, binary predictions, and the model's name, then computes these metrics and prints them out in a formatted manner. Finally, it returns a list containing all the calculated metrics for further use in performance analysis.

```{r}
calculate_model_metrics <- function(confusion_matrix, binary_predictions, model_name) {
  TN <- confusion_matrix[1, 1] # True Negatives
  FP <- confusion_matrix[1, 2] # False Positives
  FN <- confusion_matrix[2, 1] # False Negatives
  TP <- confusion_matrix[2, 2] # True Positives
  
  # positive Class accuracy (Specificity): TN / (TN + FP)
  class_accuracy <- TN / (TN + FP)
  
  # negative 1 accuracy (Sensitivity/Recall): TP / (TP + FN)
  negative_1_accuracy <- TP / (TP + FN)
  
  # balanced accuracy: Average of Sensitivity and Specificity
  balanced_accuracy <- (class_accuracy + negative_1_accuracy) / 2
  
  # overall accuracy: (TP + TN) / (TP + TN + FP + FN)
  overall_accuracy <- (TP + TN) / (TP + TN + FP + FN)
  
  # precision
  precision <- TP / (TP + FP)
  
  # f1 score
  f1_score <- 2 * (precision * negative_1_accuracy) / (precision + negative_1_accuracy)
  
  # print out all the accuracy records
  print(paste(model_name, "model prediction accuracy:"))
  cat("Class accuracy (Specificity):", sprintf("%.0f%%", class_accuracy * 100), "\n")
  cat("Negative 1 accuracy (Sensitivity/Recall):", sprintf("%.0f%%", negative_1_accuracy * 100), "\n")
  cat("Balanced accuracy:", sprintf("%.0f%%", balanced_accuracy * 100), "\n")
  cat("Overall accuracy:", sprintf("%.0f%%", overall_accuracy * 100), "\n")
  cat("Precision:", sprintf("%.0f%%", precision * 100), "\n")
  cat("F1 score:", sprintf("%.0f%%", f1_score * 100), "\n")
  
  return (list(class_accuracy = class_accuracy,
              negative_1_accuracy = negative_1_accuracy,
              balanced_accuracy = balanced_accuracy,
              overall_accuracy = overall_accuracy,
              precision = precision,
              f1_score = f1_score))
}
```

### def get_dataframe

The `get_dataframe` creates and returns a dataframe `metrics_dataframe` that stores various performance metrics for a specific model. The function accepts a model name (`model_name`) and a metrics object (`metrics`) as parameters, and formats the various metrics in `metrics` (such as classification accuracy, negative class accuracy, balanced accuracy, overall accuracy, precision, and F1 score) as percentages, retains them to integers, and then aggregates them into a dataframe. The final returned `metrics_dataframe` contains key performance information for each model.

```{r}
get_dataframe <- function(model_name, metrics) {
  metrics_dataframe <- data.frame(
    Model = model_name,
    Class_Accuracy = sprintf("%.0f%%", metrics$class_accuracy * 100),
    Negative_1_Accuracy = sprintf("%.0f%%", metrics$negative_1_accuracy * 100),
    Balanced_Accuracy = sprintf("%.0f%%", metrics$balanced_accuracy * 100),
    Overall_Accuracy = sprintf("%.0f%%", metrics$overall_accuracy * 100),
    Precision = sprintf("%.0f%%", metrics$precision * 100),
    F1_Score = sprintf("%.0f%%", metrics$f1_score * 100)
  )
  return (metrics_dataframe)
}
```

### def combine_classification_results

The `combine_classification_results` function is designed to unify performance metrics from multiple classification models into a single, cohesive dataframe. It accepts two parameters: a list of dataframes (`accuracy_dataframe_list`), each containing accuracy metrics for a different model, and a string (`data_combination`) describing the dataset combination (e.g., "Withdraw + Deposit"). The function uses `lapply` to iterate over each dataframe in the list, adding a `Data_Combination` column that records the dataset combination description for that model's metrics. Once each dataframe is labeled, `do.call(rbind, accuracy_dataframe_list)` concatenates the dataframes by rows, resulting in a single combined dataframe with all models’ metrics and an additional column indicating the dataset combination.

```{r}
combine_classification_results <- function(accuracy_dataframe_list, data_combination) {
  # apply the data combination description to each dataframe in the list
  accuracy_dataframe_list <- lapply(accuracy_dataframe_list, function(df) {
    # add a new column `Data_Combination` to store the combination description
    # this allows each dataframe to retain information about the specific data combination it
    # represents
    df$Data_Combination <- data_combination
    # return the modified dataframe with the new column added
    return(df)
  })
  
  # combine all the modified dataframes into one large dataframe
  # `do.call` applies `rbind` to all dataframes in the list, effectively stacking them by rows
  combined_dataframe <- do.call(rbind, accuracy_dataframe_list)
  
  # return the combined dataframe
  return(combined_dataframe)
}
```

### def get_percentage

The `get_percentage` function takes a dataset (`survivalDataForClassification`), along with two event labels (`indexEvent` and `outcomeEvent`), and calculates the percentage distribution of different event types within the dataset. It groups the data by the event variable, counts occurrences for each event type, and then calculates the total and corresponding percentage for each event. The function then creates a bar plot to visually display the percentage of each event type, labeling the bars with the percentage values and showing the y-axis as a percentage. The plot is titled according to the provided `indexEvent` and `outcomeEvent` labels.

```{r}
get_percentage <- function(survivalDataForClassification, indexEvent, outcomeEvent) {
  # indexEvent and outcomeEvent is a string type
  pctPerEvent <- survivalDataForClassification %>%
  group_by(event) %>%
  dplyr::summarize(numPerEvent = n()) %>%
  mutate(total = sum(numPerEvent)) %>%
  mutate(percentage = numPerEvent / total) %>%
  dplyr::select(event, percentage)
  # create a bar plot for event percentages
  # stat = "identity": percentages used directly to draw the bar chart
  print(ggplot(pctPerEvent, aes(x = event, y = percentage, fill = event)) +
    geom_bar(stat = "identity") +
    scale_y_continuous(labels = scales::percent_format()) +  # show y-axis in percentage
    labs(title = "Percentage of Events: 'Yes' event vs 'No' event",
         x = paste(indexEvent, "and", outcomeEvent),
         y = "Percentage") +
    geom_text(aes(label = scales::percent(percentage)), 
              vjust = -0.5, size = 3.5) +  # show percentages on top of bars
    theme_minimal())
}
```

### def accuracy_comparison_plot

The `accuracy_comparison_plot` function generates a faceted bar chart comparing the accuracy metrics across multiple classification models. It accepts a list of lists (`metrics_list`), where each sublist contains a model's metrics and its name, enabling flexible scaling to accommodate any number of models. The function iterates through each model’s metrics, constructing a consolidated dataframe that includes metrics like class accuracy, negative class accuracy, balanced accuracy, overall accuracy, precision, and F1 score for each model. Using `ggplot2`, it melts this data for visualization and creates a bar plot with facets for each metric, complete with percentage labels on the bars. 

```{r}
accuracy_comparison_plot <- function(metrics_list) {
  # initialize an empty data frame to store the metrics for all models
  accuracy_table <- data.frame()
  
  # loop over each element in metrics_list (each element is a list containing metrics and model name)
  for (metrics in metrics_list) {
    # Extract metrics and model name from each "tuple"
    model_metrics <- metrics[[1]]
    model_name <- metrics[[2]]
    
    # create a temporary dataframe for this model
    temp_df <- data.frame(
      Model = model_name,
      ClassA = model_metrics$class_accuracy,
      Negative_1A = model_metrics$negative_1_accuracy,
      BalancedA = model_metrics$balanced_accuracy,
      OverallA = model_metrics$overall_accuracy,
      Precision = model_metrics$precision,
      F1_score = model_metrics$f1_score
    )
    
    # append the temporary dataframe to the main accuracy_table
    accuracy_table <- rbind(accuracy_table, temp_df)
  }
  
  # melt the dataframe into long format for plotting
  accuracy_results_melted <- reshape2::melt(accuracy_table, id.vars = "Model")
  
  # generate the plot with faceted bars
  ggplot(accuracy_results_melted, aes(x = Model, y = value, fill = Model)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ variable, scales = "free_y") +  # Facet by each metric
    labs(title = "Comparison of Accuracy Metrics Across Models",
         x = "Model",
         y = "Value") +
    # add percentage labels on top of each bar
    geom_text(aes(label = scales::percent(value, accuracy = 0.1)),
              position = position_dodge(width = 0.9),
              vjust = 0.5, size = 2.0) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

### def specific_accuracy_statistics

The `specific_accuracy_statistics` function generates a summary DataFrame containing specified accuracy metrics for various classification models. It takes three parameters: `event_pair` (a string describing the event or dataset, which is added as the first column), `accuracy_type` (a string specifying the accuracy metric to extract, such as "class_accuracy" or "f1_score"), and `metrics_list` (a list of lists where each sub-list contains accuracy data and a model name). The function traverses `metrics_list` and dynamically extracts the specified accuracy metric, converting it to a percentage (multiplied by 100) and rounding to one decimal place for each model. If the requested accuracy type is invalid, it throws an error. Finally, it organizes the extracted data into a DataFrame, with `accuracy_type` in the first column and each model's results as additional columns, and returns this structured DataFrame.

```{r}
specific_accuracy_statistics <- function(event_pair, accuracy_type, metrics_list) {
  # initialize the result list
  results <- list()
  
  # add accuracy_type as the left front corner and event_pair is the first column of the data
  results[[accuracy_type]] <- event_pair
  
  # traverse metrics_list and extract the specified accuracy type for each model
  for (metric_item in metrics_list) {
    # get accuracy data
    accuracy_data <- metric_item[[1]]
    # get the model name
    model_name <- metric_item[[2]]
    if (accuracy_type == "class_accuracy") {
      results[[model_name]] <- round(accuracy_data$class_accuracy * 100, 1)
    }
    else if (accuracy_type == "negative_1_accuracy") {
      results[[model_name]] <- round(accuracy_data$negative_1_accuracy * 100, 1)
    }
    else if (accuracy_type == "balanced_accuracy") {
      results[[model_name]] <- round(accuracy_data$balanced_accuracy * 100, 1)
    }
    else if (accuracy_type == "overall_accuracy") {
      results[[model_name]] <- round(accuracy_data$overall_accuracy * 100, 1)
    }
    else if (accuracy_type == "precision") {
      results[[model_name]] <- round(accuracy_data$precision * 100, 1)
    }
    else if (accuracy_type == "f1_score") {
      results[[model_name]] <- round(accuracy_data$f1_score * 100, 1)
    }
    else {
      # An error message is displayed if the specified accuracy_type does not exist.
      stop(paste("Invalid accuracy type:", accuracy_type))
    }
  }
  
  # convert the result to a DataFrame and set row.names = NULL
  df <- as.data.frame(results, row.names = NULL)
  return(df)
}
```

### def combine_accuracy_dataframes

The `combine_accuracy_dataframes` function takes a list of data frames as input and combines them into a single data frame by stacking the rows. It first checks if the input is a valid list and throws an error if not. Then, using `do.call` and `rbind`, it merges all the data frames in the list row-wise. The function is designed to handle multiple data frames efficiently and returns a consolidated data frame for further analysis.

```{r}
combine_accuracy_dataframes <- function(df_list) {
  # check if the input is a list
  if (!is.list(df_list)) {
    stop("Input must be a list of data.frames.")
  }
  
  # Use do.call and rbind to combine all data.frames in a list.
  combined_df <- do.call(rbind, df_list)
  
  # returns the merged data.frame
  return(combined_df)
}
```

## Classification Model Development and Optimization

### def logistic_regression

This `logistic_regression` function implements a logistic regression model based on **Lasso regularization** for binary classification tasks. The model first preprocesses the data, including dividing `train_data` into 80% `train_set` and 20% `validation_set`, where the validation set is actively used for **hyperparameter tuning** rather than just evaluation. For numerical features, all datasets are standardized to have a **mean of 0** and a **standard deviation of 1**, improving model stability and optimization efficiency. In terms of feature transformation, `model.matrix` is used to generate a design matrix **without an intercept**, ensuring compatibility with the `glmnet` function. `glmnet` applies **logistic regression (`family = "binomial"`) with Lasso regularization (`alpha = 1`)**, which automatically selects the most important features while eliminating redundant ones, thereby enhancing the model's generalization ability. To determine the optimal regularization parameter **`lambda`**, the function first performs **cross-validation using `cv.glmnet`** to generate a range of candidate `lambda` values. Unlike traditional methods that directly select `lambda.min` from cross-validation, this implementation **evaluates multiple `lambda` values on the validation set** and selects the one that achieves the highest **AUC (Area Under the Curve) score**. This ensures that the chosen `lambda` not only minimizes training error but also performs optimally on unseen validation data, **preventing overfitting and improving generalization**. Once the best `lambda` is determined, it is applied to **predict on both `validation_set` and `test_data`**, where probabilities are converted into binary classification results based on a **0.5 threshold**. Finally, the function calculates performance metrics such as **accuracy, recall, and F1 score** using `calculate_model_metrics`, providing a comprehensive evaluation of model performance.

```{r}
logistic_regression <- function(train_data, test_data) {
  # Ensure train_data and test_data are in data.table format for fast operations
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  set.seed(123)  # Ensure reproducibility
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset and scale them
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Scale numeric columns in train set
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # Scale numeric columns in validation set
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  # Scale numeric columns in test set
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert datasets to matrix format as required by the glmnet package
  x_train <- model.matrix(event ~ . - 1, data = train_set)  
  y_train <- train_set$event  
  
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)  
  y_validation <- validation_set$event  # Extract target variable for validation set
  
  x_test <- model.matrix(event ~ . - 1, data = test_data)  
  
  # Apply logistic regression with Lasso regularization (alpha = 1 means Lasso)
  logistic_regression_classifier <- glmnet(x_train, y_train, family = "binomial", alpha = 1)
  
  # Perform cross-validation to get a range of lambda values
  cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
  # Retrieve all lambda values from cross-validation
  lambda_candidates <- cv_model$lambda
  
  # Use validation set to select the best lambda based on AUC score
  best_lambda <- NULL
  best_auc <- -Inf  # Initialize the best AUC score
  
  for (lambda in lambda_candidates) {
    predict_probabilities_val <- predict(cv_model$glmnet.fit, s = lambda, newx = x_validation, type = "response")
    
    # Compute AUC for validation set
    auc_val <- auc(roc(y_validation, predict_probabilities_val))
    
    # Update best lambda if current AUC is better
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_lambda <- lambda
    }
  }
  
  # Predict on the validation set using the selected best lambda
  predict_probabilities_val <- predict(logistic_regression_classifier, s = best_lambda, newx = x_validation, 
                                       type = "response")
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation set performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Logistic Regression (Validation)")
  
  # Predict on the test set using the best lambda
  predict_probabilities_test <- predict(logistic_regression_classifier, s = best_lambda, newx = x_test, 
                                        type = "response")
  binary_prediction_test <- ifelse(predict_probabilities_test > 0.5, "yes", "no")
  
  # Create a confusion matrix for the test set
  test_conf_matrix <- table(Predicted = binary_prediction_test, Actual = test_data$event)
  
  # Evaluate model performance on the test set
  metrics_lr <- calculate_model_metrics(test_conf_matrix, predict_probabilities_test, "Logistic Regression")
  
  # Create a dataframe with the desired structure
  metrics_lr_dataframe = get_dataframe("Logistic Regression", metrics_lr)
  
  return (list(metrics_lr_dataframe = metrics_lr_dataframe, metrics_lr = metrics_lr))
}
```

### def decision_tree

This **decision tree model** is optimized to ensure that **hyperparameter selection is driven by validation data**, rather than relying on fixed values. The model first preprocesses the data by splitting `train_data` into **80% `train_set` and 20% `validation_set`**, ensuring that validation data plays an active role in hyperparameter tuning. The model is trained using **recursive partitioning (`rpart`)**, where it learns decision rules to classify observations based on feature splits. Instead of using a fixed complexity parameter (`cp`), which controls pruning and prevents overfitting, the model **evaluates multiple `cp` values** ranging from **0.0001 to 0.05**. For each candidate `cp`, a **decision tree is trained on `train_set`**, and its performance is assessed on `validation_set` using the **AUC (Area Under the Curve) score**. The `cp` value that achieves the highest **AUC on the validation set** is selected as the final hyperparameter. This approach prevents **underfitting (if `cp` is too large, resulting in an overly simple tree)** and **overfitting (if `cp` is too small, allowing excessive complexity)**. Once the best `cp` is determined, the final decision tree is trained using this parameter and applied to the **test dataset**, where its classification performance is evaluated using accuracy, recall, and F1 score.

```{r}
decision_tree <- function(train_data, test_data) {
  # Load required libraries
  # library(rpart)
  # library(caret) # Load caret for data partitioning
  
  # Split train_data into train_set (80%) and validation_set (20%)
  set.seed(123) # Ensure reproducibility
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Define candidate complexity parameters (cp) for pruning
  cp_candidates <- seq(0.0001, 0.05, by = 0.002)  # Range of possible cp values
  best_cp <- NULL
  best_auc <- -Inf  # Initialize the best AUC score
  
  # Loop over different cp values to find the best one using the validation set
  for (cp in cp_candidates) {
    # Train the decision tree model with the current cp value
    decision_tree_model <- rpart(
      event ~ .,
      data = train_set,
      method = "class",
      control = rpart.control(
        cp = cp,        # Adjusting cp for pruning
        maxdepth = 30,  # Keeping max depth fixed
        minsplit = 20   # Keeping minsplit fixed
      )
    )
    
    # Predict probabilities on validation set
    # Get probability for "yes"
    predict_probabilities_val <- predict(decision_tree_model, validation_set, type = "prob")[, 2]
    
    # Compute AUC for validation set
    auc_val <- auc(roc(validation_set$event, predict_probabilities_val))
    
    # Update the best cp if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_cp <- cp
    }
  }
  
  # Train the final model with the best cp value
  decision_tree_classifier <- rpart(
    event ~ .,
    data = train_set,
    method = "class",
    control = rpart.control(
      cp = best_cp,  # Using the best cp found from validation set
      maxdepth = 30,
      minsplit = 20
    )
  )
  
  # Predict on the validation dataset using the selected cp
  predict_probabilities_val <- predict(decision_tree_classifier, validation_set, type = "class")
  validation_conf_matrix <- table(Predicted = predict_probabilities_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Decision Tree (Validation)")
  
  # Predict on the testing dataset using the final model
  predict_probabilities_dt <- predict(decision_tree_classifier, test_data, type = "class")
  test_conf_matrix <- table(Predicted = predict_probabilities_dt, Actual = test_data$event)
  
  # Evaluate model performance
  metrics_dt <- calculate_model_metrics(test_conf_matrix, predict_probabilities_dt, "Decision Tree")
  
  # Create a dataframe with the desired structure
  metrics_dt_dataframe = get_dataframe("Decision Tree", metrics_dt)
  
  return (list(metrics_dt_dataframe = metrics_dt_dataframe, metrics_dt = metrics_dt))
}
```

### def naive_bayes

This optimized **Naive Bayes model** enhances the traditional approach by incorporating **validation-driven hyperparameter selection**, ensuring better generalization to unseen data. Initially, the dataset is split into **80% training (`train_set`) and 20% validation (`validation_set`)**, allowing the validation set to actively influence hyperparameter tuning. Instead of using a fixed **Laplace smoothing parameter (`laplace`)**, which prevents zero-probability issues in categorical features, the model evaluates multiple `laplace` values **ranging from 0 to 1**. For each candidate `laplace`, a **Naïve Bayes model** is trained on `train_set`, and its performance is measured on `validation_set` using the **AUC (Area Under the Curve) score**. The `laplace` value that maximizes AUC is selected as the optimal parameter, ensuring that the model balances between **avoiding overfitting (when `laplace` is too low) and excessive smoothing (when `laplace` is too high)**. Once the best `laplace` is determined, the final Naïve Bayes model is retrained using this value and applied to **test data** for final evaluation. The model’s performance is assessed using key classification metrics, including **accuracy, recall, and F1 score**.

```{r}
naive_bayes <- function(train_data, test_data) {
  # Load required libraries
  # library(e1071)
  # library(caret) # Load caret for data partitioning
  
  target_column = "event"
  
  # Convert the target column to a factor if it's not already
  train_data[[target_column]] <- as.factor(train_data[[target_column]])
  test_labels <- as.factor(test_data[[target_column]])
  
  # Split train_data into train_set (80%) and validation_set (20%)
  set.seed(123)  # Ensure reproducibility
  trainIndex <- createDataPartition(train_data[[target_column]], p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Remove the target column from the validation and test sets for prediction
  validation_features <- validation_set %>% select(-all_of(target_column))
  test_features <- test_data %>% select(-all_of(target_column))
  
  # Define candidate Laplace smoothing values
  laplace_candidates <- seq(0, 1, by = 0.1)  # Range of laplace values to test
  best_laplace <- NULL
  best_auc <- -Inf  # Initialize the best AUC score
  
  # Hyperparameter tuning: Find the best laplace value using validation set
  for (laplace in laplace_candidates) {
    # Train Naïve Bayes model with current laplace value
    nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set, laplace = laplace)
    
    # Get probability predictions on the validation set
    # Probability for "yes"
    validation_probabilities <- predict(nb_model, validation_features, type = "raw")[, 2]
    
    # Compute AUC for validation set
    auc_val <- auc(roc(validation_set[[target_column]], validation_probabilities))
    
    # Update best laplace if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_laplace <- laplace
    }
  }
  
  # Train the final Naïve Bayes model with the best laplace value
  nb_model <- naiveBayes(as.formula(paste(target_column, "~ .")), data = train_set, laplace = best_laplace)
  
  # Make predictions on the validation set
  validation_predictions <- predict(nb_model, validation_features)
  
  # Get prediction probabilities for validation set
  validation_probabilities <- predict(nb_model, validation_features, type = "raw")
  
  # Evaluate model performance with a confusion matrix for validation set
  validation_conf_matrix <- table(Predicted = validation_predictions, Actual = validation_set[[target_column]])
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, validation_probabilities, 
                                                "Naive Bayes (Validation)")
  
  # Make predictions on the test set using the final model
  predictions <- predict(nb_model, test_features)
  
  # Get prediction probabilities for test set
  prediction_probabilities <- predict(nb_model, test_features, type = "raw")
  
  # Ensure both predicted and actual labels are factors with the same levels
  predictions <- factor(predictions, levels = levels(test_labels))
  
  # Evaluate model performance with a confusion matrix for test set
  conf_matrix <- table(Predicted = predictions, Actual = test_labels)
  
  metrics_nb <- calculate_model_metrics(conf_matrix, prediction_probabilities, "Naive Bayes")
  
  # Create a dataframe with the desired structure
  metrics_nb_dataframe = get_dataframe("Naive Bayes", metrics_nb)
  
  # Each classification model needs to return these two variables
  return (list(metrics_nb_dataframe = metrics_nb_dataframe, metrics_nb = metrics_nb))
}
```

### def elastic_net

This optimized **Elastic Net model** enhances traditional logistic regression by incorporating both **Lasso (L1) and Ridge (L2) regularization**, allowing for automatic feature selection while mitigating multicollinearity. The model first preprocesses the data by standardizing numerical features to have **a mean of 0 and a standard deviation of 1**, ensuring stable convergence. Instead of using a fixed **`alpha`** (which controls the balance between L1 and L2 penalties) and relying solely on **cross-validation for `lambda` selection**, this implementation iterates over multiple **`alpha` values (ranging from 0 to 1)**. For each **`alpha`**, a **cross-validated `glmnet` model** is trained to determine the best **`lambda`**, and its performance is evaluated on the **validation set using the AUC (Area Under the Curve) score**. The combination of **`alpha` and `lambda` that yields the highest AUC** is selected as the final hyperparameter set. The optimized **Elastic Net model** is then retrained using the best `alpha` and `lambda` values and applied to **test data**, where its performance is assessed using standard classification metrics such as **accuracy, precision, recall, and F1-score**.

```{r}
elastic_net <- function(train_data, test_data) {
  # Load required libraries
  # library(glmnet) # Required for Elastic Net (Lasso + Ridge regularization)
  # library(data.table) # For efficient data handling using data.table
  
  # Convert train_data and test_data to data.table format for optimized processing
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  set.seed(123)  # Ensure reproducibility
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features in the dataset for standardization
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  
  # Standardize numeric columns (mean = 0, standard deviation = 1) to improve model performance
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert the dataset into a matrix format, as required by glmnet
  # Feature matrix for training
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  # Target variable
  y_train <- train_set$event
  # Feature matrix for validation
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  y_validation <- validation_set$event
  # Feature matrix for testing
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Define candidate alpha values for Elastic Net optimization
  # Range from 0 (Ridge) to 1 (Lasso)
  alpha_candidates <- seq(0, 1, by = 0.1)
  best_alpha <- NULL
  best_lambda <- NULL
  best_auc <- -Inf  # Initialize best AUC score
  
  # Iterate through different alpha values to find the best one using validation set
  for (alpha in alpha_candidates) {
    # Perform cross-validation to find the best lambda for the current alpha
    cv_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = alpha)
    # Get best lambda from cross-validation
    lambda_min <- cv_model$lambda.min
    
    # Predict on validation set
    predict_probabilities_val <- predict(cv_model$glmnet.fit, s = lambda_min, newx = x_validation, 
                                         type = "response")
    
    # Compute AUC for validation set
    auc_val <- auc(roc(y_validation, predict_probabilities_val))
    
    # Update best alpha and lambda if current AUC is higher
    if (auc_val > best_auc) {
      best_auc <- auc_val
      best_alpha <- alpha
      best_lambda <- lambda_min
    }
  }
  
  # Train the final Elastic Net model with the best alpha and lambda values
  elastic_net_model <- glmnet(x_train, y_train, family = "binomial", alpha = best_alpha, lambda = best_lambda)
  
  # Predict on the validation dataset using the selected best parameters
  predict_probabilities_val <- predict(elastic_net_model, newx = x_validation, type = "response")
  binary_prediction_val <- ifelse(predict_probabilities_val > 0.5, "yes", "no")
  validation_conf_matrix <- table(Predicted = binary_prediction_val, Actual = validation_set$event)
  
  # Evaluate validation performance
  validation_metrics <- calculate_model_metrics(validation_conf_matrix, predict_probabilities_val, 
                                                "Elastic Net (Validation)")
  
  # Predict event probabilities for the test dataset using the best parameters
  predict_probabilities_en <- predict(elastic_net_model, newx = x_test, type = "response")
  binary_prediction_en <- ifelse(predict_probabilities_en > 0.5, "yes", "no")
  
  # Create a confusion matrix to compare predicted vs. actual outcomes in the test set
  confusion_matrix_en <- table(Predicted = binary_prediction_en, Actual = test_data$event)
  
  # Evaluate model performance using key classification metrics (accuracy, precision, recall, F1-score)
  metrics_en <- calculate_model_metrics(confusion_matrix_en, predict_probabilities_en, "Elastic Net")
  
  # Store the calculated metrics in a structured dataframe for easy comparison
  metrics_en_dataframe <- get_dataframe("Elastic Net", metrics_en)
  
  # Return both the detailed metrics list and the formatted dataframe
  return (list(metrics_en_dataframe = metrics_en_dataframe, metrics_en = metrics_en))
}
```

### def XG_Boost

This optimized **XGBoost model** significantly improves efficiency while maintaining high predictive performance by incorporating **random search, cross-validation, parallel computation, and early stopping**. The model begins by **preprocessing the data**, including **standardizing numerical features** and converting categorical variables into a **matrix format compatible with XGBoost**. Unlike traditional **grid search**, which exhaustively tests all hyperparameter combinations, this implementation employs **random search**, selecting only **five hyperparameter sets** from a predefined range. Instead of relying solely on a **validation set** for hyperparameter tuning, it utilizes **`xgb.cv()` with 5-fold cross-validation**, ensuring a **more robust and stable** hyperparameter selection process. The **best combination of `max_depth`, `eta`, `subsample`, and `colsample_bytree`** is determined based on the highest **AUC score** from cross-validation. Additionally, the function enables **parallel processing** by automatically detecting and utilizing all available CPU cores, significantly reducing training time. To further optimize performance and prevent overfitting, **early stopping** is applied, halting training if no improvement is observed after **10 consecutive rounds**. Once the best parameters are identified, the final **XGBoost model** is trained and evaluated on the **test dataset**, where key classification metrics such as **accuracy, recall, precision, and F1-score** are computed.

```{r}
XG_Boost <- function(train_data, test_data) {
  # Convert train_data and test_data to data.table format
  setDT(train_data)
  setDT(test_data)
  
  # Split train_data into train_set (80%) and validation_set (20%)
  set.seed(123)  # Ensure reproducibility
  trainIndex <- createDataPartition(train_data$event, p = 0.8, list = FALSE)
  train_set <- train_data[trainIndex, ]
  validation_set <- train_data[-trainIndex, ]
  
  # Identify numeric features and scale them
  numeric_features <- names(train_set)[sapply(train_set, is.numeric)]
  train_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  validation_set[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  test_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]
  
  # Convert data to matrix format required by XGBoost
  x_train <- model.matrix(event ~ . - 1, data = train_set)
  y_train <- as.numeric(train_set$event == "yes")  # Convert event labels to 0/1
  x_validation <- model.matrix(event ~ . - 1, data = validation_set)
  y_validation <- as.numeric(validation_set$event == "yes")
  x_test <- model.matrix(event ~ . - 1, data = test_data)
  
  # Convert to DMatrix format, which is optimized for XGBoost
  dtrain <- xgb.DMatrix(data = x_train, label = y_train)
  dvalidation <- xgb.DMatrix(data = x_validation, label = y_validation)
  dtest <- xgb.DMatrix(data = x_test)
  
  # Detect available CPU cores for parallel computation
  num_cores <- detectCores()
  
  # Define a small set of candidate hyperparameters for random search
  param_grid <- expand.grid(
    max_depth = c(4, 6),  
    eta = c(0.05, 0.1),  
    subsample = c(0.8),   
    colsample_bytree = c(0.8)
  ) %>% sample_n(5)  # Randomly sample 5 hyperparameter sets
  
  best_params <- NULL
  best_auc <- -Inf  # Initialize best AUC score
  
  # Hyperparameter tuning using cross-validation
  for (i in 1:nrow(param_grid)) {
    params <- list(
      objective = "binary:logistic",
      eval_metric = "auc",
      max_depth = param_grid$max_depth[i],
      eta = param_grid$eta[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i],
      nthread = num_cores
    )
    
    # Perform cross-validation
    cv_model <- xgb.cv(
      params = params,
      data = dtrain,
      nrounds = 200,
      nfold = 5,  # 5-Fold Cross-Validation
      early_stopping_rounds = 10,
      metrics = "auc",
      verbose = 0
    )
    
    # Get best AUC from CV
    best_cv_auc <- max(cv_model$evaluation_log$test_auc_mean)
    
    # Update best parameters if AUC improves
    if (best_cv_auc > best_auc) {
      best_auc <- best_cv_auc
      best_params <- params
    }
  }
  
  # Train final model with the best hyperparameters
  xgb_model <- xgb.train(
    params = best_params,
    data = dtrain,
    nrounds = 200,
    early_stopping_rounds = 10,
    watchlist = list(train = dtrain, validation = dvalidation),
    verbose = 0
  )
  
  # Predict on test set
  predict_probabilities_xgb <- predict(xgb_model, dtest)
  binary_prediction_xgb <- ifelse(predict_probabilities_xgb > 0.5, "yes", "no")
  
  # Ensure both predicted and actual labels are factors with the same levels
  binary_prediction_xgb <- factor(binary_prediction_xgb, levels = c("yes", "no"))
  test_data$event <- factor(test_data$event, levels = c("yes", "no"))
  
  # Generate confusion matrix
  confusion_matrix_xgb <- table(binary_prediction_xgb, test_data$event)
  
  # Evaluate model performance
  metrics_xgb <- calculate_model_metrics(confusion_matrix_xgb, predict_probabilities_xgb, "XGBoost")
  
  # Store metrics in a structured dataframe
  metrics_xgb_dataframe <- get_dataframe("XGBoost", metrics_xgb)
  
  # Return detailed metrics and formatted dataframe
  return (list(metrics_xgb_dataframe = metrics_xgb_dataframe, metrics_xgb = metrics_xgb))
}
```